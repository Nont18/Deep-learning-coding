{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp \n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holist = mp.solutions.holistic \n",
    "mp_draw = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(img, model):\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img.flags.writeable = False                 \n",
    "    result = model.process(img)                 # Make prediction\n",
    "    img.flags.writeable = True                   \n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR) \n",
    "    return img, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks(img, result):\n",
    "    mp_draw.draw_landmarks(img, result.face_landmarks, mp_holist.FACEMESH_CONTOURS) # Draw face connections\n",
    "    mp_draw.draw_landmarks(img, result.pose_landmarks, mp_holist.POSE_CONNECTIONS) # Draw pose connections\n",
    "    mp_draw.draw_landmarks(img, result.left_hand_landmarks, mp_holist.HAND_CONNECTIONS) # Draw left hand connections\n",
    "    mp_draw.draw_landmarks(img, result.right_hand_landmarks, mp_holist.HAND_CONNECTIONS) # Draw right hand connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_styled_landmarks(img, result):\n",
    "    mp_draw.draw_landmarks(img, result.face_landmarks, mp_holist.FACEMESH_CONTOURS, \n",
    "                             mp_draw.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1), # color the joint \n",
    "                             mp_draw.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1) #color the connection\n",
    "                             ) \n",
    "    # mp_draw.draw_landmarks(img, result.face_landmarks, mp_holist.FACEMESH_CONTOURS, \n",
    "    #                          mp_draw.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1), # color the joint \n",
    "    #                          mp_draw.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1) #color the connection\n",
    "    #                          ) \n",
    "    \n",
    "    mp_draw.draw_landmarks(img, result.pose_landmarks, mp_holist.POSE_CONNECTIONS,\n",
    "                             mp_draw.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4), \n",
    "                             mp_draw.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    mp_draw.draw_landmarks(img, result.left_hand_landmarks, mp_holist.HAND_CONNECTIONS, \n",
    "                             mp_draw.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4), \n",
    "                             mp_draw.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    mp_draw.draw_landmarks(img, result.right_hand_landmarks, mp_holist.HAND_CONNECTIONS, \n",
    "                             mp_draw.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4), \n",
    "                             mp_draw.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                             ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "frozenset({(0, 1),\n",
       "           (0, 4),\n",
       "           (1, 2),\n",
       "           (2, 3),\n",
       "           (3, 7),\n",
       "           (4, 5),\n",
       "           (5, 6),\n",
       "           (6, 8),\n",
       "           (9, 10),\n",
       "           (11, 12),\n",
       "           (11, 13),\n",
       "           (11, 23),\n",
       "           (12, 14),\n",
       "           (12, 24),\n",
       "           (13, 15),\n",
       "           (14, 16),\n",
       "           (15, 17),\n",
       "           (15, 19),\n",
       "           (15, 21),\n",
       "           (16, 18),\n",
       "           (16, 20),\n",
       "           (16, 22),\n",
       "           (17, 19),\n",
       "           (18, 20),\n",
       "           (23, 24),\n",
       "           (23, 25),\n",
       "           (24, 26),\n",
       "           (25, 27),\n",
       "           (26, 28),\n",
       "           (27, 29),\n",
       "           (27, 31),\n",
       "           (28, 30),\n",
       "           (28, 32),\n",
       "           (29, 31),\n",
       "           (30, 32)})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mp_holist.POSE_CONNECTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(results):\n",
    "    pose=np.array([[res.x,res.y,res.z,res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    left_hnd=np.array([[res.x,res.y,res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    right_hnd=np.array([[res.x,res.y,res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    face=np.array([[res.x,res.y,res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "    return np.concatenate([pose,left_hnd,right_hnd,face])\n",
    "# concatenating for the model to detect the sign language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "video_dir = \"C:/Users/araya/Desktop/keypoints/video_extract\"\n",
    "video_list = []\n",
    "video_list = os.listdir(video_dir)\n",
    "\n",
    "len(video_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['กฎกระทรวง.mp4',\n",
       " 'กฎหมายรัฐธรรมนูญ.mp4',\n",
       " 'กรมอนามัย.mp4',\n",
       " 'กรรม.mp4',\n",
       " 'กรรมสิทธิ์.mp4',\n",
       " 'กระโดด.mp4',\n",
       " 'กล้วยบวชชี.mp4',\n",
       " 'กล้วยเชื่อม.mp4',\n",
       " 'กังวล.mp4',\n",
       " 'กีฬา.mp4',\n",
       " 'น้อง.mp4',\n",
       " 'เขิน.mp4',\n",
       " 'เขื่อนดิน.mp4',\n",
       " 'เขื่อนสิริกิติ์.mp4',\n",
       " 'เข้าใจผิด.mp4',\n",
       " 'เคย.mp4',\n",
       " 'เครียด.mp4',\n",
       " 'เครื่องปั่นดิน.mp4',\n",
       " 'เครื่องหมายการค้า.mp4',\n",
       " 'เจอ.mp4',\n",
       " 'เจ้าหนี้.mp4',\n",
       " 'เช่าซื้อ.mp4',\n",
       " 'เช่าทรัพย์.mp4',\n",
       " 'เซอร์เบีย.mp4',\n",
       " 'เซเนกัล.mp4',\n",
       " 'เซ็ง.mp4',\n",
       " 'เดิน.mp4',\n",
       " 'เดิมพัน.mp4',\n",
       " 'เพลีย.mp4',\n",
       " 'เมื่อย.mp4',\n",
       " 'เม็กซิโก.mp4',\n",
       " 'เฮโรอีน.mp4',\n",
       " 'แกมเบีย.mp4',\n",
       " 'แซมเบีย.mp4',\n",
       " 'โกหก.mp4',\n",
       " 'โจทก์.mp4',\n",
       " 'โชจู.mp4',\n",
       " 'ใกล้.mp4',\n",
       " 'ไดโนเสาร์.mp4',\n",
       " 'ไอซ์.mp4']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for exported data, numpy arrays\n",
    "Model_Data=os.path.join('Data for different actions')\n",
    "\n",
    "actions = np.array(video_list)\n",
    "\n",
    "no_of_seqs = 1\n",
    "\n",
    "# 30 frames in length\n",
    "seq_length = 160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['กฎกระทรวง.mp4', 'กฎหมายรัฐธรรมนูญ.mp4', 'กรมอนามัย.mp4',\n",
       "       'กรรม.mp4', 'กรรมสิทธิ์.mp4', 'กระโดด.mp4', 'กล้วยบวชชี.mp4',\n",
       "       'กล้วยเชื่อม.mp4', 'กังวล.mp4', 'กีฬา.mp4', 'น้อง.mp4', 'เขิน.mp4',\n",
       "       'เขื่อนดิน.mp4', 'เขื่อนสิริกิติ์.mp4', 'เข้าใจผิด.mp4', 'เคย.mp4',\n",
       "       'เครียด.mp4', 'เครื่องปั่นดิน.mp4', 'เครื่องหมายการค้า.mp4',\n",
       "       'เจอ.mp4', 'เจ้าหนี้.mp4', 'เช่าซื้อ.mp4', 'เช่าทรัพย์.mp4',\n",
       "       'เซอร์เบีย.mp4', 'เซเนกัล.mp4', 'เซ็ง.mp4', 'เดิน.mp4',\n",
       "       'เดิมพัน.mp4', 'เพลีย.mp4', 'เมื่อย.mp4', 'เม็กซิโก.mp4',\n",
       "       'เฮโรอีน.mp4', 'แกมเบีย.mp4', 'แซมเบีย.mp4', 'โกหก.mp4',\n",
       "       'โจทก์.mp4', 'โชจู.mp4', 'ใกล้.mp4', 'ไดโนเสาร์.mp4', 'ไอซ์.mp4'],\n",
       "      dtype='<U21')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collecting keypoint values for Training nd Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory where your videos are stored\n",
    "directory = \"C:/Users/araya/Desktop/keypoints/video_extract\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:/Users/araya/Desktop/keypoints/video_extract'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/araya/Desktop/keypoints/video_extract/กฎกระทรวง.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/กฎหมายรัฐธรรมนูญ.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/กรมอนามัย.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/กรรม.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/กรรมสิทธิ์.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/กระโดด.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/กล้วยบวชชี.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/กล้วยเชื่อม.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/กังวล.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/กีฬา.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/น้อง.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/เขิน.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/เขื่อนดิน.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/เขื่อนสิริกิติ์.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/เข้าใจผิด.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/เคย.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/เครียด.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/เครื่องปั่นดิน.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/เครื่องหมายการค้า.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/เจอ.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/เจ้าหนี้.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/เช่าซื้อ.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/เช่าทรัพย์.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/เซอร์เบีย.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/เซเนกัล.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/เซ็ง.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/เดิน.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/เดิมพัน.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/เพลีย.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/เมื่อย.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/เม็กซิโก.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/เฮโรอีน.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/แกมเบีย.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/แซมเบีย.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/โกหก.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/โจทก์.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/โชจู.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/ใกล้.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/ไดโนเสาร์.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/ไอซ์.mp4\n"
     ]
    }
   ],
   "source": [
    "for filename in actions:\n",
    "    print(directory + '/' + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Data for different actions/กฎกระทรวง.mp4/กฎกระทรวง.npy', 'Data for different actions/กฎหมายรัฐธรรมนูญ.mp4/กฎหมายรัฐธรรมนูญ.npy', 'Data for different actions/กรมอนามัย.mp4/กรมอนามัย.npy', 'Data for different actions/กรรม.mp4/กรรม.npy', 'Data for different actions/กรรมสิทธิ์.mp4/กรรมสิทธิ์.npy', 'Data for different actions/กระโดด.mp4/กระโดด.npy', 'Data for different actions/กล้วยบวชชี.mp4/กล้วยบวชชี.npy', 'Data for different actions/กล้วยเชื่อม.mp4/กล้วยเชื่อม.npy', 'Data for different actions/กังวล.mp4/กังวล.npy', 'Data for different actions/กีฬา.mp4/กีฬา.npy', 'Data for different actions/น้อง.mp4/น้อง.npy', 'Data for different actions/เขิน.mp4/เขิน.npy', 'Data for different actions/เขื่อนดิน.mp4/เขื่อนดิน.npy', 'Data for different actions/เขื่อนสิริกิติ์.mp4/เขื่อนสิริกิติ์.npy', 'Data for different actions/เข้าใจผิด.mp4/เข้าใจผิด.npy', 'Data for different actions/เคย.mp4/เคย.npy', 'Data for different actions/เครียด.mp4/เครียด.npy', 'Data for different actions/เครื่องปั่นดิน.mp4/เครื่องปั่นดิน.npy', 'Data for different actions/เครื่องหมายการค้า.mp4/เครื่องหมายการค้า.npy', 'Data for different actions/เจอ.mp4/เจอ.npy', 'Data for different actions/เจ้าหนี้.mp4/เจ้าหนี้.npy', 'Data for different actions/เช่าซื้อ.mp4/เช่าซื้อ.npy', 'Data for different actions/เช่าทรัพย์.mp4/เช่าทรัพย์.npy', 'Data for different actions/เซอร์เบีย.mp4/เซอร์เบีย.npy', 'Data for different actions/เซเนกัล.mp4/เซเนกัล.npy', 'Data for different actions/เซ็ง.mp4/เซ็ง.npy', 'Data for different actions/เดิน.mp4/เดิน.npy', 'Data for different actions/เดิมพัน.mp4/เดิมพัน.npy', 'Data for different actions/เพลีย.mp4/เพลีย.npy', 'Data for different actions/เมื่อย.mp4/เมื่อย.npy', 'Data for different actions/เม็กซิโก.mp4/เม็กซิโก.npy', 'Data for different actions/เฮโรอีน.mp4/เฮโรอีน.npy', 'Data for different actions/แกมเบีย.mp4/แกมเบีย.npy', 'Data for different actions/แซมเบีย.mp4/แซมเบีย.npy', 'Data for different actions/โกหก.mp4/โกหก.npy', 'Data for different actions/โจทก์.mp4/โจทก์.npy', 'Data for different actions/โชจู.mp4/โชจู.npy', 'Data for different actions/ใกล้.mp4/ใกล้.npy', 'Data for different actions/ไดโนเสาร์.mp4/ไดโนเสาร์.npy', 'Data for different actions/ไอซ์.mp4/ไอซ์.npy']\n"
     ]
    }
   ],
   "source": [
    "file_paths = []\n",
    "for action in actions:\n",
    "    video_path = os.path.join('Data for different actions/', action)\n",
    "    # print(video_path)\n",
    "    # print(action)\n",
    "    file_paths.append(video_path + '/' + action.split(\".\")[0] + \".npy\")\n",
    "print(file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_keypoint_sequences(file_paths):\n",
    "    keypoint_sequences = []\n",
    "    for file_path in file_paths:\n",
    "        keypoints = np.load(file_path)\n",
    "        keypoint_sequences.append(torch.tensor(keypoints, dtype=torch.float32))\n",
    "    return keypoint_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 0.5013,  0.2452, -1.2167,  ...,  0.5663,  0.2188,  0.0098],\n",
       "         [ 0.4997,  0.2482, -1.4690,  ...,  0.5652,  0.2181,  0.0106],\n",
       "         [ 0.4984,  0.2500, -1.4853,  ...,  0.5654,  0.2185,  0.0112],\n",
       "         ...,\n",
       "         [ 0.4861,  0.2513, -1.3416,  ...,  0.5572,  0.2177,  0.0091],\n",
       "         [ 0.4873,  0.2514, -1.3574,  ...,  0.5575,  0.2172,  0.0097],\n",
       "         [ 0.4883,  0.2516, -1.3579,  ...,  0.5577,  0.2170,  0.0101]]),\n",
       " tensor([[ 0.4922,  0.2382, -1.2850,  ...,  0.5578,  0.2124,  0.0094],\n",
       "         [ 0.4920,  0.2405, -1.4288,  ...,  0.5571,  0.2116,  0.0099],\n",
       "         [ 0.4920,  0.2409, -1.4093,  ...,  0.5567,  0.2122,  0.0098],\n",
       "         ...,\n",
       "         [ 0.4814,  0.2260, -1.3318,  ...,  0.5503,  0.1923,  0.0123],\n",
       "         [ 0.4815,  0.2257, -1.3351,  ...,  0.5503,  0.1921,  0.0122],\n",
       "         [ 0.4815,  0.2255, -1.3497,  ...,  0.5501,  0.1919,  0.0124]]),\n",
       " tensor([[ 0.5049,  0.2371, -1.2115,  ...,  0.5643,  0.2082,  0.0088],\n",
       "         [ 0.5045,  0.2381, -1.1896,  ...,  0.5643,  0.2081,  0.0085],\n",
       "         [ 0.5041,  0.2385, -1.1915,  ...,  0.5643,  0.2080,  0.0089],\n",
       "         ...,\n",
       "         [ 0.4963,  0.2307, -1.3096,  ...,  0.5624,  0.2043,  0.0127],\n",
       "         [ 0.4965,  0.2308, -1.3085,  ...,  0.5625,  0.2045,  0.0126],\n",
       "         [ 0.4972,  0.2312, -1.3053,  ...,  0.5633,  0.2045,  0.0128]]),\n",
       " tensor([[ 0.5134,  0.2614, -1.4426,  ...,  0.5818,  0.2272,  0.0153],\n",
       "         [ 0.5130,  0.2604, -1.4262,  ...,  0.5810,  0.2273,  0.0147],\n",
       "         [ 0.5126,  0.2599, -1.4278,  ...,  0.5810,  0.2276,  0.0150],\n",
       "         ...,\n",
       "         [ 0.5079,  0.2693, -1.4999,  ...,  0.5780,  0.2349,  0.0115],\n",
       "         [ 0.5090,  0.2688, -1.4936,  ...,  0.5782,  0.2346,  0.0116],\n",
       "         [ 0.5092,  0.2683, -1.4518,  ...,  0.5786,  0.2341,  0.0116]]),\n",
       " tensor([[ 0.4883,  0.2402, -1.1024,  ...,  0.5482,  0.2132,  0.0081],\n",
       "         [ 0.4878,  0.2402, -1.1906,  ...,  0.5469,  0.2135,  0.0083],\n",
       "         [ 0.4863,  0.2402, -1.1774,  ...,  0.5477,  0.2141,  0.0087],\n",
       "         ...,\n",
       "         [ 0.4788,  0.3129, -1.6072,  ...,  0.5349,  0.2531,  0.0022],\n",
       "         [ 0.4782,  0.3129, -1.6350,  ...,  0.5344,  0.2525,  0.0022],\n",
       "         [ 0.4771,  0.3131, -1.6312,  ...,  0.5339,  0.2515,  0.0022]]),\n",
       " tensor([[ 0.4992,  0.1994, -1.1906,  ...,  0.5657,  0.1731,  0.0116],\n",
       "         [ 0.4988,  0.2047, -1.3590,  ...,  0.5663,  0.1723,  0.0123],\n",
       "         [ 0.4982,  0.2082, -1.3140,  ...,  0.5671,  0.1730,  0.0129],\n",
       "         ...,\n",
       "         [ 0.4743,  0.1974, -1.3230,  ...,  0.5475,  0.1620,  0.0155],\n",
       "         [ 0.4732,  0.1974, -1.3174,  ...,  0.5461,  0.1615,  0.0151],\n",
       "         [ 0.4720,  0.1974, -1.3149,  ...,  0.5453,  0.1614,  0.0149]]),\n",
       " tensor([[ 0.5023,  0.2809, -1.6242,  ...,  0.5847,  0.2321,  0.0112],\n",
       "         [ 0.5023,  0.2806, -1.6631,  ...,  0.5840,  0.2322,  0.0130],\n",
       "         [ 0.5022,  0.2805, -1.6912,  ...,  0.5837,  0.2322,  0.0122],\n",
       "         ...,\n",
       "         [ 0.5037,  0.2791, -1.5789,  ...,  0.5834,  0.2224,  0.0129],\n",
       "         [ 0.5044,  0.2774, -1.5686,  ...,  0.5831,  0.2220,  0.0130],\n",
       "         [ 0.5052,  0.2722, -1.5527,  ...,  0.5827,  0.2216,  0.0132]]),\n",
       " tensor([[ 0.4868,  0.2821, -1.4668,  ...,  0.5711,  0.2335,  0.0116],\n",
       "         [ 0.4861,  0.2786, -1.5812,  ...,  0.5702,  0.2334,  0.0129],\n",
       "         [ 0.4856,  0.2769, -1.6059,  ...,  0.5699,  0.2333,  0.0131],\n",
       "         ...,\n",
       "         [ 0.4826,  0.2583, -1.6140,  ...,  0.5606,  0.2261,  0.0154],\n",
       "         [ 0.4820,  0.2583, -1.5231,  ...,  0.5603,  0.2261,  0.0156],\n",
       "         [ 0.4818,  0.2580, -1.5255,  ...,  0.5600,  0.2263,  0.0156]]),\n",
       " tensor([[ 0.5016,  0.2321, -1.1859,  ...,  0.5647,  0.2139,  0.0072],\n",
       "         [ 0.5015,  0.2340, -1.1996,  ...,  0.5645,  0.2135,  0.0076],\n",
       "         [ 0.5016,  0.2346, -1.2272,  ...,  0.5644,  0.2137,  0.0082],\n",
       "         ...,\n",
       "         [ 0.4743,  0.2622, -1.4181,  ...,  0.5499,  0.2133,  0.0080],\n",
       "         [ 0.4763,  0.2599, -1.4742,  ...,  0.5519,  0.2118,  0.0090],\n",
       "         [ 0.4787,  0.2522, -1.6440,  ...,  0.5537,  0.2110,  0.0093]]),\n",
       " tensor([[ 0.4926,  0.1945, -1.2354,  ...,  0.5488,  0.1581,  0.0085],\n",
       "         [ 0.4942,  0.1949, -1.4254,  ...,  0.5481,  0.1576,  0.0085],\n",
       "         [ 0.4948,  0.1960, -1.4483,  ...,  0.5479,  0.1581,  0.0084],\n",
       "         ...,\n",
       "         [ 0.4917,  0.1882, -1.3355,  ...,  0.5458,  0.1536,  0.0126],\n",
       "         [ 0.4918,  0.1885, -1.3154,  ...,  0.5456,  0.1536,  0.0125],\n",
       "         [ 0.4918,  0.1886, -1.3147,  ...,  0.5453,  0.1537,  0.0124]]),\n",
       " tensor([[ 0.4956,  0.2681, -1.1361,  ...,  0.5554,  0.2332,  0.0147],\n",
       "         [ 0.4948,  0.2681, -1.3768,  ...,  0.5540,  0.2333,  0.0116],\n",
       "         [ 0.4943,  0.2681, -1.3754,  ...,  0.5538,  0.2332,  0.0116],\n",
       "         ...,\n",
       "         [ 0.4895,  0.2689, -1.2187,  ...,  0.5513,  0.2276,  0.0135],\n",
       "         [ 0.4889,  0.2688, -1.2198,  ...,  0.5508,  0.2276,  0.0134],\n",
       "         [ 0.4882,  0.2688, -1.2172,  ...,  0.5507,  0.2276,  0.0134]]),\n",
       " tensor([[ 0.5032,  0.2228, -1.1998,  ...,  0.5663,  0.1971,  0.0106],\n",
       "         [ 0.5035,  0.2228, -1.2072,  ...,  0.5662,  0.1969,  0.0106],\n",
       "         [ 0.5035,  0.2230, -1.2082,  ...,  0.5664,  0.1963,  0.0108],\n",
       "         ...,\n",
       "         [ 0.4982,  0.2206, -1.2743,  ...,  0.5578,  0.1892,  0.0092],\n",
       "         [ 0.4983,  0.2178, -1.2380,  ...,  0.5581,  0.1883,  0.0091],\n",
       "         [ 0.4979,  0.2173, -1.2264,  ...,  0.5578,  0.1877,  0.0096]]),\n",
       " tensor([[ 0.5442,  0.2507, -1.4443,  ...,  0.6105,  0.2203,  0.0123],\n",
       "         [ 0.5444,  0.2508, -1.6574,  ...,  0.6094,  0.2198,  0.0134],\n",
       "         [ 0.5445,  0.2510, -1.7250,  ...,  0.6093,  0.2195,  0.0144],\n",
       "         ...,\n",
       "         [ 0.5324,  0.2577, -1.6704,  ...,  0.5975,  0.2210,  0.0145],\n",
       "         [ 0.5323,  0.2560, -1.6673,  ...,  0.5966,  0.2206,  0.0148],\n",
       "         [ 0.5323,  0.2553, -1.6826,  ...,  0.5957,  0.2201,  0.0146]]),\n",
       " tensor([[ 0.5402,  0.2562, -1.5458,  ...,  0.6041,  0.2257,  0.0125],\n",
       "         [ 0.5389,  0.2596, -1.7020,  ...,  0.6031,  0.2264,  0.0130],\n",
       "         [ 0.5379,  0.2616, -1.7134,  ...,  0.6027,  0.2258,  0.0139],\n",
       "         ...,\n",
       "         [ 0.5153,  0.2631, -1.6030,  ...,  0.5956,  0.2125,  0.0142],\n",
       "         [ 0.5176,  0.2626, -1.6101,  ...,  0.5985,  0.2126,  0.0139],\n",
       "         [ 0.5197,  0.2624, -1.5662,  ...,  0.6011,  0.2119,  0.0145]]),\n",
       " tensor([[ 0.5030,  0.2553, -1.1988,  ...,  0.5699,  0.2265,  0.0097],\n",
       "         [ 0.5028,  0.2582, -1.2761,  ...,  0.5689,  0.2266,  0.0109],\n",
       "         [ 0.5028,  0.2605, -1.3315,  ...,  0.5690,  0.2269,  0.0105],\n",
       "         ...,\n",
       "         [ 0.5028,  0.2675, -1.4499,  ...,  0.5672,  0.2323,  0.0110],\n",
       "         [ 0.5007,  0.2672, -1.4234,  ...,  0.5668,  0.2317,  0.0115],\n",
       "         [ 0.4988,  0.2671, -1.4308,  ...,  0.5670,  0.2312,  0.0118]]),\n",
       " tensor([[ 0.5069,  0.2355, -1.3384,  ...,  0.5700,  0.2051,  0.0053],\n",
       "         [ 0.5045,  0.2395, -1.5097,  ...,  0.5699,  0.2043,  0.0064],\n",
       "         [ 0.5028,  0.2420, -1.5081,  ...,  0.5699,  0.2045,  0.0069],\n",
       "         ...,\n",
       "         [ 0.4952,  0.2462, -1.4319,  ...,  0.5658,  0.2015,  0.0122],\n",
       "         [ 0.4952,  0.2448, -1.4781,  ...,  0.5661,  0.2014,  0.0123],\n",
       "         [ 0.4953,  0.2436, -1.4689,  ...,  0.5662,  0.2013,  0.0124]]),\n",
       " tensor([[ 0.5035,  0.2282, -1.1379,  ...,  0.5710,  0.1953,  0.0120],\n",
       "         [ 0.5033,  0.2284, -1.2771,  ...,  0.5713,  0.1954,  0.0123],\n",
       "         [ 0.5032,  0.2286, -1.2743,  ...,  0.5715,  0.1957,  0.0122],\n",
       "         ...,\n",
       "         [ 0.4810,  0.2296, -1.4129,  ...,  0.5542,  0.1897,  0.0133],\n",
       "         [ 0.4812,  0.2296, -1.4275,  ...,  0.5549,  0.1900,  0.0130],\n",
       "         [ 0.4816,  0.2299, -1.4337,  ...,  0.5557,  0.1903,  0.0133]]),\n",
       " tensor([[ 0.5108,  0.2425, -1.1053,  ...,  0.5724,  0.2176,  0.0127],\n",
       "         [ 0.5091,  0.2430, -1.3007,  ...,  0.5713,  0.2177,  0.0129],\n",
       "         [ 0.5080,  0.2432, -1.3035,  ...,  0.5714,  0.2179,  0.0130],\n",
       "         ...,\n",
       "         [ 0.4966,  0.2620, -1.5367,  ...,  0.5653,  0.2259,  0.0082],\n",
       "         [ 0.4968,  0.2621, -1.5419,  ...,  0.5656,  0.2262,  0.0083],\n",
       "         [ 0.4971,  0.2623, -1.5482,  ...,  0.5658,  0.2263,  0.0086]]),\n",
       " tensor([[ 0.4878,  0.2235, -1.2515,  ...,  0.5511,  0.1981,  0.0114],\n",
       "         [ 0.4870,  0.2286, -1.4279,  ...,  0.5506,  0.1977,  0.0105],\n",
       "         [ 0.4865,  0.2315, -1.4431,  ...,  0.5508,  0.1984,  0.0110],\n",
       "         ...,\n",
       "         [ 0.4901,  0.2217, -1.3449,  ...,  0.5576,  0.1888,  0.0156],\n",
       "         [ 0.4898,  0.2224, -1.3780,  ...,  0.5575,  0.1892,  0.0157],\n",
       "         [ 0.4896,  0.2232, -1.4140,  ...,  0.5572,  0.1897,  0.0156]]),\n",
       " tensor([[ 0.5194,  0.2227, -1.3410,  ...,  0.5911,  0.1970,  0.0142],\n",
       "         [ 0.5200,  0.2228, -1.3298,  ...,  0.5912,  0.1965,  0.0131],\n",
       "         [ 0.5202,  0.2230, -1.3074,  ...,  0.5909,  0.1970,  0.0149],\n",
       "         ...,\n",
       "         [ 0.5146,  0.2198, -1.3267,  ...,  0.5821,  0.1907,  0.0159],\n",
       "         [ 0.5141,  0.2205, -1.3207,  ...,  0.5816,  0.1911,  0.0154],\n",
       "         [ 0.5137,  0.2209, -1.3223,  ...,  0.5812,  0.1917,  0.0161]]),\n",
       " tensor([[ 0.5103,  0.2428, -1.1660,  ...,  0.5757,  0.2179,  0.0081],\n",
       "         [ 0.5106,  0.2437, -1.2850,  ...,  0.5751,  0.2183,  0.0099],\n",
       "         [ 0.5112,  0.2443, -1.2880,  ...,  0.5748,  0.2189,  0.0108],\n",
       "         ...,\n",
       "         [ 0.4800,  0.2603, -1.3086,  ...,  0.5532,  0.2237,  0.0069],\n",
       "         [ 0.4807,  0.2604, -1.2544,  ...,  0.5539,  0.2238,  0.0067],\n",
       "         [ 0.4813,  0.2604, -1.2161,  ...,  0.5548,  0.2238,  0.0065]]),\n",
       " tensor([[ 0.4945,  0.2362, -1.1227,  ...,  0.5529,  0.2086,  0.0088],\n",
       "         [ 0.4947,  0.2363, -1.1264,  ...,  0.5528,  0.2094,  0.0088],\n",
       "         [ 0.4951,  0.2364, -1.1371,  ...,  0.5531,  0.2099,  0.0089],\n",
       "         ...,\n",
       "         [ 0.4933,  0.2480, -1.2424,  ...,  0.5563,  0.2184,  0.0091],\n",
       "         [ 0.4933,  0.2480, -1.2443,  ...,  0.5565,  0.2185,  0.0093],\n",
       "         [ 0.4932,  0.2480, -1.2944,  ...,  0.5564,  0.2185,  0.0095]]),\n",
       " tensor([[ 0.5038,  0.2425, -1.1550,  ...,  0.5665,  0.2136,  0.0080],\n",
       "         [ 0.5027,  0.2423, -1.2423,  ...,  0.5653,  0.2133,  0.0092],\n",
       "         [ 0.5018,  0.2423, -1.2565,  ...,  0.5649,  0.2130,  0.0094],\n",
       "         ...,\n",
       "         [ 0.4950,  0.2412, -1.1727,  ...,  0.5624,  0.2175,  0.0087],\n",
       "         [ 0.4951,  0.2412, -1.1690,  ...,  0.5625,  0.2177,  0.0082],\n",
       "         [ 0.4951,  0.2412, -1.1910,  ...,  0.5626,  0.2180,  0.0079]]),\n",
       " tensor([[ 0.4916,  0.2518, -1.3187,  ...,  0.5597,  0.2202,  0.0108],\n",
       "         [ 0.4907,  0.2519, -1.3672,  ...,  0.5580,  0.2209,  0.0105],\n",
       "         [ 0.4896,  0.2520, -1.4026,  ...,  0.5580,  0.2210,  0.0110],\n",
       "         ...,\n",
       "         [ 0.4850,  0.2552, -1.4312,  ...,  0.5509,  0.2199,  0.0138],\n",
       "         [ 0.4839,  0.2550, -1.4298,  ...,  0.5505,  0.2199,  0.0138],\n",
       "         [ 0.4831,  0.2549, -1.4164,  ...,  0.5501,  0.2200,  0.0137]]),\n",
       " tensor([[ 0.5051,  0.2347, -1.4390,  ...,  0.5717,  0.2094,  0.0091],\n",
       "         [ 0.5024,  0.2389, -1.5627,  ...,  0.5707,  0.2087,  0.0090],\n",
       "         [ 0.5005,  0.2417, -1.5896,  ...,  0.5704,  0.2090,  0.0098],\n",
       "         ...,\n",
       "         [ 0.4950,  0.2365, -1.4622,  ...,  0.5663,  0.2072,  0.0114],\n",
       "         [ 0.4950,  0.2366, -1.4772,  ...,  0.5667,  0.2075,  0.0115],\n",
       "         [ 0.4949,  0.2366, -1.4725,  ...,  0.5671,  0.2078,  0.0120]]),\n",
       " tensor([[ 0.5064,  0.2529, -1.3080,  ...,  0.5751,  0.2261,  0.0114],\n",
       "         [ 0.5055,  0.2550, -1.3556,  ...,  0.5752,  0.2257,  0.0106],\n",
       "         [ 0.5048,  0.2568, -1.3925,  ...,  0.5751,  0.2260,  0.0110],\n",
       "         ...,\n",
       "         [ 0.4960,  0.2624, -1.3587,  ...,  0.5642,  0.2311,  0.0101],\n",
       "         [ 0.4961,  0.2619, -1.3373,  ...,  0.5652,  0.2307,  0.0104],\n",
       "         [ 0.4961,  0.2615, -1.3405,  ...,  0.5653,  0.2302,  0.0102]]),\n",
       " tensor([[ 0.5267,  0.2322, -1.2986,  ...,  0.5921,  0.1996,  0.0123],\n",
       "         [ 0.5267,  0.2305, -1.3156,  ...,  0.5925,  0.1999,  0.0112],\n",
       "         [ 0.5266,  0.2296, -1.3147,  ...,  0.5928,  0.1997,  0.0121],\n",
       "         ...,\n",
       "         [ 0.5122,  0.2243, -1.2697,  ...,  0.5819,  0.1855,  0.0147],\n",
       "         [ 0.5114,  0.2239, -1.2687,  ...,  0.5817,  0.1857,  0.0145],\n",
       "         [ 0.5110,  0.2235, -1.2585,  ...,  0.5815,  0.1858,  0.0145]]),\n",
       " tensor([[ 0.5050,  0.2164, -1.1575,  ...,  0.5700,  0.1959,  0.0103],\n",
       "         [ 0.5041,  0.2187, -1.3621,  ...,  0.5689,  0.1959,  0.0100],\n",
       "         [ 0.5031,  0.2218, -1.3920,  ...,  0.5693,  0.1959,  0.0109],\n",
       "         ...,\n",
       "         [ 0.4987,  0.2252, -1.2609,  ...,  0.5655,  0.1932,  0.0101],\n",
       "         [ 0.4982,  0.2238, -1.2657,  ...,  0.5653,  0.1926,  0.0103],\n",
       "         [ 0.4969,  0.2230, -1.2967,  ...,  0.5649,  0.1919,  0.0105]]),\n",
       " tensor([[ 0.5097,  0.2301, -1.2305,  ...,  0.5773,  0.2070,  0.0135],\n",
       "         [ 0.5098,  0.2316, -1.4094,  ...,  0.5762,  0.2072,  0.0130],\n",
       "         [ 0.5099,  0.2328, -1.4219,  ...,  0.5754,  0.2074,  0.0134],\n",
       "         ...,\n",
       "         [ 0.5106,  0.2519, -1.1800,  ...,  0.5731,  0.2204,  0.0132],\n",
       "         [ 0.5082,  0.2514, -1.1958,  ...,  0.5717,  0.2193,  0.0134],\n",
       "         [ 0.5074,  0.2516, -1.2210,  ...,  0.5703,  0.2182,  0.0128]]),\n",
       " tensor([[ 0.5410,  0.2495, -1.3908,  ...,  0.6085,  0.2224,  0.0099],\n",
       "         [ 0.5399,  0.2502, -1.3960,  ...,  0.6066,  0.2219,  0.0111],\n",
       "         [ 0.5390,  0.2511, -1.3861,  ...,  0.6062,  0.2215,  0.0105],\n",
       "         ...,\n",
       "         [ 0.5131,  0.2461, -1.3199,  ...,  0.5857,  0.2158,  0.0118],\n",
       "         [ 0.5137,  0.2459, -1.3370,  ...,  0.5858,  0.2153,  0.0118],\n",
       "         [ 0.5141,  0.2456, -1.3376,  ...,  0.5860,  0.2150,  0.0116]]),\n",
       " tensor([[ 0.4813,  0.2267, -1.2462,  ...,  0.5499,  0.2013,  0.0123],\n",
       "         [ 0.4796,  0.2283, -1.4595,  ...,  0.5479,  0.2026,  0.0127],\n",
       "         [ 0.4786,  0.2300, -1.5108,  ...,  0.5480,  0.2025,  0.0122],\n",
       "         ...,\n",
       "         [ 0.4782,  0.2297, -1.2598,  ...,  0.5464,  0.1965,  0.0150],\n",
       "         [ 0.4781,  0.2300, -1.2686,  ...,  0.5459,  0.1960,  0.0147],\n",
       "         [ 0.4780,  0.2304, -1.2444,  ...,  0.5461,  0.1953,  0.0154]]),\n",
       " tensor([[ 0.5051,  0.2433, -1.3371,  ...,  0.5701,  0.2185,  0.0094],\n",
       "         [ 0.5042,  0.2464, -1.4811,  ...,  0.5696,  0.2193,  0.0109],\n",
       "         [ 0.5039,  0.2479, -1.4928,  ...,  0.5700,  0.2201,  0.0112],\n",
       "         ...,\n",
       "         [ 0.4965,  0.2517, -1.4369,  ...,  0.5645,  0.2217,  0.0130],\n",
       "         [ 0.4956,  0.2516, -1.4394,  ...,  0.5637,  0.2215,  0.0129],\n",
       "         [ 0.4943,  0.2514, -1.4394,  ...,  0.5628,  0.2207,  0.0124]]),\n",
       " tensor([[ 0.5269,  0.2488, -1.5232,  ...,  0.5929,  0.2181,  0.0118],\n",
       "         [ 0.5260,  0.2493, -1.4445,  ...,  0.5918,  0.2172,  0.0111],\n",
       "         [ 0.5250,  0.2497, -1.4588,  ...,  0.5916,  0.2171,  0.0113],\n",
       "         ...,\n",
       "         [ 0.5126,  0.2514, -1.5292,  ...,  0.5827,  0.2212,  0.0138],\n",
       "         [ 0.5125,  0.2512, -1.5273,  ...,  0.5813,  0.2204,  0.0136],\n",
       "         [ 0.5123,  0.2512, -1.5221,  ...,  0.5795,  0.2200,  0.0137]]),\n",
       " tensor([[ 0.5103,  0.2330, -1.4731,  ...,  0.5745,  0.2056,  0.0102],\n",
       "         [ 0.5071,  0.2360, -1.5453,  ...,  0.5731,  0.2055,  0.0103],\n",
       "         [ 0.5052,  0.2381, -1.5650,  ...,  0.5733,  0.2056,  0.0100],\n",
       "         ...,\n",
       "         [ 0.4875,  0.2316, -1.5252,  ...,  0.5606,  0.1993,  0.0132],\n",
       "         [ 0.4872,  0.2318, -1.5227,  ...,  0.5604,  0.1997,  0.0129],\n",
       "         [ 0.4870,  0.2318, -1.5220,  ...,  0.5604,  0.2000,  0.0129]]),\n",
       " tensor([[ 0.4952,  0.2338, -1.3905,  ...,  0.5593,  0.2078,  0.0128],\n",
       "         [ 0.4926,  0.2435, -1.4861,  ...,  0.5600,  0.2072,  0.0117],\n",
       "         [ 0.4916,  0.2483, -1.4905,  ...,  0.5596,  0.2079,  0.0126],\n",
       "         ...,\n",
       "         [ 0.4846,  0.2462, -1.4163,  ...,  0.5649,  0.2045,  0.0151],\n",
       "         [ 0.4852,  0.2450, -1.4038,  ...,  0.5648,  0.2038,  0.0156],\n",
       "         [ 0.4857,  0.2436, -1.3897,  ...,  0.5648,  0.2033,  0.0155]]),\n",
       " tensor([[ 0.4828,  0.2604, -1.2334,  ...,  0.5483,  0.2258,  0.0073],\n",
       "         [ 0.4816,  0.2606, -1.4555,  ...,  0.5474,  0.2267,  0.0087],\n",
       "         [ 0.4809,  0.2608, -1.4653,  ...,  0.5475,  0.2266,  0.0088],\n",
       "         ...,\n",
       "         [ 0.4832,  0.2490, -1.4275,  ...,  0.5557,  0.2156,  0.0123],\n",
       "         [ 0.4845,  0.2491, -1.4277,  ...,  0.5566,  0.2158,  0.0124],\n",
       "         [ 0.4855,  0.2492, -1.4367,  ...,  0.5573,  0.2163,  0.0122]]),\n",
       " tensor([[ 0.5122,  0.2456, -1.3706,  ...,  0.5836,  0.2207,  0.0125],\n",
       "         [ 0.5121,  0.2461, -1.4417,  ...,  0.5818,  0.2208,  0.0118],\n",
       "         [ 0.5121,  0.2465, -1.4591,  ...,  0.5815,  0.2204,  0.0118],\n",
       "         ...,\n",
       "         [ 0.5017,  0.2434, -1.4740,  ...,  0.5812,  0.2105,  0.0155],\n",
       "         [ 0.5022,  0.2433, -1.4619,  ...,  0.5818,  0.2102,  0.0157],\n",
       "         [ 0.5032,  0.2431, -1.4609,  ...,  0.5821,  0.2097,  0.0163]]),\n",
       " tensor([[ 0.4849,  0.2484, -1.3873,  ...,  0.5543,  0.2162,  0.0108],\n",
       "         [ 0.4846,  0.2500, -1.4881,  ...,  0.5539,  0.2157,  0.0111],\n",
       "         [ 0.4844,  0.2507, -1.4935,  ...,  0.5537,  0.2161,  0.0111],\n",
       "         ...,\n",
       "         [ 0.4589,  0.2435, -1.4798,  ...,  0.5299,  0.2058,  0.0120],\n",
       "         [ 0.4586,  0.2436, -1.4788,  ...,  0.5293,  0.2056,  0.0122],\n",
       "         [ 0.4583,  0.2437, -1.4790,  ...,  0.5289,  0.2052,  0.0123]]),\n",
       " tensor([[ 0.5199,  0.2290, -1.2810,  ...,  0.5876,  0.2058,  0.0103],\n",
       "         [ 0.5195,  0.2307, -1.3821,  ...,  0.5873,  0.2059,  0.0106],\n",
       "         [ 0.5193,  0.2327, -1.4100,  ...,  0.5877,  0.2059,  0.0109],\n",
       "         ...,\n",
       "         [ 0.5108,  0.2431, -1.3919,  ...,  0.5781,  0.2077,  0.0105],\n",
       "         [ 0.5093,  0.2422, -1.3894,  ...,  0.5768,  0.2068,  0.0103],\n",
       "         [ 0.5079,  0.2405, -1.3818,  ...,  0.5751,  0.2053,  0.0102]]),\n",
       " tensor([[ 0.5005,  0.2439, -1.3629,  ...,  0.5694,  0.2139,  0.0154],\n",
       "         [ 0.5002,  0.2448, -1.5049,  ...,  0.5688,  0.2128,  0.0146],\n",
       "         [ 0.4999,  0.2457, -1.5287,  ...,  0.5683,  0.2134,  0.0148],\n",
       "         ...,\n",
       "         [ 0.4965,  0.2433, -1.3673,  ...,  0.5670,  0.2114,  0.0169],\n",
       "         [ 0.4960,  0.2433, -1.3676,  ...,  0.5660,  0.2107,  0.0168],\n",
       "         [ 0.4950,  0.2433, -1.3637,  ...,  0.5651,  0.2100,  0.0170]])]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the sequences\n",
    "import torch\n",
    "sequences = load_keypoint_sequences(file_paths)\n",
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40, 160, 1662])\n"
     ]
    }
   ],
   "source": [
    "# Pad the sequences to the same length\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "padded_sequences = pad_sequence(sequences, batch_first=True)\n",
    "pad_sequence\n",
    "print(padded_sequences.shape) # (batch_size, max_sequence_length, num_keypoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['กฎกระทรวง',\n",
       " 'กฎหมายรัฐธรรมนูญ',\n",
       " 'กรมอนามัย',\n",
       " 'กรรม',\n",
       " 'กรรมสิทธิ์',\n",
       " 'กระโดด',\n",
       " 'กล้วยบวชชี',\n",
       " 'กล้วยเชื่อม',\n",
       " 'กังวล',\n",
       " 'กีฬา',\n",
       " 'น้อง',\n",
       " 'เขิน',\n",
       " 'เขื่อนดิน',\n",
       " 'เขื่อนสิริกิติ์',\n",
       " 'เข้าใจผิด',\n",
       " 'เคย',\n",
       " 'เครียด',\n",
       " 'เครื่องปั่นดิน',\n",
       " 'เครื่องหมายการค้า',\n",
       " 'เจอ',\n",
       " 'เจ้าหนี้',\n",
       " 'เช่าซื้อ',\n",
       " 'เช่าทรัพย์',\n",
       " 'เซอร์เบีย',\n",
       " 'เซเนกัล',\n",
       " 'เซ็ง',\n",
       " 'เดิน',\n",
       " 'เดิมพัน',\n",
       " 'เพลีย',\n",
       " 'เมื่อย',\n",
       " 'เม็กซิโก',\n",
       " 'เฮโรอีน',\n",
       " 'แกมเบีย',\n",
       " 'แซมเบีย',\n",
       " 'โกหก',\n",
       " 'โจทก์',\n",
       " 'โชจู',\n",
       " 'ใกล้',\n",
       " 'ไดโนเสาร์',\n",
       " 'ไอซ์']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = [action.split(\".\")[0] for action in actions]\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "       34, 35, 36, 37, 38, 39], dtype=int64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "labels = le.fit_transform(labels)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# y = pd.read_csv(\"script.csv\")\n",
    "# y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y[\"label\"] = y[\"label\"].astype(int)\n",
    "# labels = y.label\n",
    "# labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Create a custom dataset\n",
    "class KeypointDataset(Dataset):\n",
    "    def __init__(self, file_paths, labels):\n",
    "        self.file_paths = file_paths\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        keypoints = np.load(self.file_paths[idx])\n",
    "        label = self.labels[idx]\n",
    "        return torch.tensor(keypoints, dtype=torch.float32), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset\n",
    "dataset = KeypointDataset(file_paths, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Data for different actions/กฎกระทรวง.mp4/กฎกระทรวง.npy', 'Data for different actions/กฎหมายรัฐธรรมนูญ.mp4/กฎหมายรัฐธรรมนูญ.npy', 'Data for different actions/กรมอนามัย.mp4/กรมอนามัย.npy', 'Data for different actions/กรรม.mp4/กรรม.npy', 'Data for different actions/กรรมสิทธิ์.mp4/กรรมสิทธิ์.npy', 'Data for different actions/กระโดด.mp4/กระโดด.npy', 'Data for different actions/กล้วยบวชชี.mp4/กล้วยบวชชี.npy', 'Data for different actions/กล้วยเชื่อม.mp4/กล้วยเชื่อม.npy', 'Data for different actions/กังวล.mp4/กังวล.npy', 'Data for different actions/กีฬา.mp4/กีฬา.npy', 'Data for different actions/น้อง.mp4/น้อง.npy', 'Data for different actions/เขิน.mp4/เขิน.npy', 'Data for different actions/เขื่อนดิน.mp4/เขื่อนดิน.npy', 'Data for different actions/เขื่อนสิริกิติ์.mp4/เขื่อนสิริกิติ์.npy', 'Data for different actions/เข้าใจผิด.mp4/เข้าใจผิด.npy', 'Data for different actions/เคย.mp4/เคย.npy', 'Data for different actions/เครียด.mp4/เครียด.npy', 'Data for different actions/เครื่องปั่นดิน.mp4/เครื่องปั่นดิน.npy', 'Data for different actions/เครื่องหมายการค้า.mp4/เครื่องหมายการค้า.npy', 'Data for different actions/เจอ.mp4/เจอ.npy', 'Data for different actions/เจ้าหนี้.mp4/เจ้าหนี้.npy', 'Data for different actions/เช่าซื้อ.mp4/เช่าซื้อ.npy', 'Data for different actions/เช่าทรัพย์.mp4/เช่าทรัพย์.npy', 'Data for different actions/เซอร์เบีย.mp4/เซอร์เบีย.npy', 'Data for different actions/เซเนกัล.mp4/เซเนกัล.npy', 'Data for different actions/เซ็ง.mp4/เซ็ง.npy', 'Data for different actions/เดิน.mp4/เดิน.npy', 'Data for different actions/เดิมพัน.mp4/เดิมพัน.npy', 'Data for different actions/เพลีย.mp4/เพลีย.npy', 'Data for different actions/เมื่อย.mp4/เมื่อย.npy', 'Data for different actions/เม็กซิโก.mp4/เม็กซิโก.npy', 'Data for different actions/เฮโรอีน.mp4/เฮโรอีน.npy', 'Data for different actions/แกมเบีย.mp4/แกมเบีย.npy', 'Data for different actions/แซมเบีย.mp4/แซมเบีย.npy', 'Data for different actions/โกหก.mp4/โกหก.npy', 'Data for different actions/โจทก์.mp4/โจทก์.npy', 'Data for different actions/โชจู.mp4/โชจู.npy', 'Data for different actions/ใกล้.mp4/ใกล้.npy', 'Data for different actions/ไดโนเสาร์.mp4/ไดโนเสาร์.npy', 'Data for different actions/ไอซ์.mp4/ไอซ์.npy']\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39]\n"
     ]
    }
   ],
   "source": [
    "print(dataset.file_paths)\n",
    "print(dataset.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collate function for padding\n",
    "def collate_fn(batch):\n",
    "    sequences, labels = zip(*batch)\n",
    "    padded_sequences = pad_sequence(sequences, batch_first=True)\n",
    "    return padded_sequences, torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x20a403baff0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the DataLoader\n",
    "batch_size = 4\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set device (use GPU if available)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes,dropout):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc1 = nn.Linear(hidden_size, 256)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        # c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        # Forward propagate the LSTM\n",
    "        out, _ = self.lstm(self.dropout(x))\n",
    "        \n",
    "        # Use the last time step's output for classification\n",
    "        out = out[:, -1, :]\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model, loss function, and optimizer\n",
    "model = LSTMModel(input_size=1662, hidden_size=256, num_layers=2, num_classes=40, dropout=0.5).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()  # For multi-class classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1200], Loss: 3.7647 , Accuracy : 0.00%\n",
      "Epoch [2/1200], Loss: 3.5973 , Accuracy : 2.50%\n",
      "Epoch [3/1200], Loss: 3.7075 , Accuracy : 0.00%\n",
      "Epoch [4/1200], Loss: 3.5269 , Accuracy : 5.00%\n",
      "Epoch [5/1200], Loss: 3.6150 , Accuracy : 2.50%\n",
      "Epoch [6/1200], Loss: 3.4719 , Accuracy : 0.00%\n",
      "Epoch [7/1200], Loss: 3.4550 , Accuracy : 0.00%\n",
      "Epoch [8/1200], Loss: 3.4346 , Accuracy : 5.00%\n",
      "Epoch [9/1200], Loss: 3.7242 , Accuracy : 2.50%\n",
      "Epoch [10/1200], Loss: 3.6186 , Accuracy : 2.50%\n",
      "Epoch [11/1200], Loss: 3.4484 , Accuracy : 2.50%\n",
      "Epoch [12/1200], Loss: 3.4152 , Accuracy : 5.00%\n",
      "Epoch [13/1200], Loss: 3.4947 , Accuracy : 2.50%\n",
      "Epoch [14/1200], Loss: 2.9629 , Accuracy : 0.00%\n",
      "Epoch [15/1200], Loss: 3.8045 , Accuracy : 2.50%\n",
      "Epoch [16/1200], Loss: 3.2464 , Accuracy : 5.00%\n",
      "Epoch [17/1200], Loss: 4.4523 , Accuracy : 5.00%\n",
      "Epoch [18/1200], Loss: 3.1534 , Accuracy : 2.50%\n",
      "Epoch [19/1200], Loss: 3.5323 , Accuracy : 10.00%\n",
      "Epoch [20/1200], Loss: 3.2723 , Accuracy : 10.00%\n",
      "Epoch [21/1200], Loss: 4.3112 , Accuracy : 7.50%\n",
      "Epoch [22/1200], Loss: 3.4942 , Accuracy : 5.00%\n",
      "Epoch [23/1200], Loss: 3.2384 , Accuracy : 10.00%\n",
      "Epoch [24/1200], Loss: 2.9866 , Accuracy : 10.00%\n",
      "Epoch [25/1200], Loss: 2.8521 , Accuracy : 7.50%\n",
      "Epoch [26/1200], Loss: 4.1714 , Accuracy : 2.50%\n",
      "Epoch [27/1200], Loss: 3.4031 , Accuracy : 12.50%\n",
      "Epoch [28/1200], Loss: 3.7528 , Accuracy : 2.50%\n",
      "Epoch [29/1200], Loss: 3.2261 , Accuracy : 10.00%\n",
      "Epoch [30/1200], Loss: 3.0930 , Accuracy : 10.00%\n",
      "Epoch [31/1200], Loss: 3.0363 , Accuracy : 10.00%\n",
      "Epoch [32/1200], Loss: 3.6554 , Accuracy : 15.00%\n",
      "Epoch [33/1200], Loss: 3.3960 , Accuracy : 10.00%\n",
      "Epoch [34/1200], Loss: 3.1491 , Accuracy : 5.00%\n",
      "Epoch [35/1200], Loss: 3.4017 , Accuracy : 2.50%\n",
      "Epoch [36/1200], Loss: 4.0943 , Accuracy : 7.50%\n",
      "Epoch [37/1200], Loss: 3.0220 , Accuracy : 10.00%\n",
      "Epoch [38/1200], Loss: 3.0937 , Accuracy : 7.50%\n",
      "Epoch [39/1200], Loss: 3.2202 , Accuracy : 10.00%\n",
      "Epoch [40/1200], Loss: 3.8190 , Accuracy : 10.00%\n",
      "Epoch [41/1200], Loss: 3.0662 , Accuracy : 7.50%\n",
      "Epoch [42/1200], Loss: 3.6411 , Accuracy : 15.00%\n",
      "Epoch [43/1200], Loss: 3.7041 , Accuracy : 10.00%\n",
      "Epoch [44/1200], Loss: 2.7258 , Accuracy : 12.50%\n",
      "Epoch [45/1200], Loss: 2.6692 , Accuracy : 10.00%\n",
      "Epoch [46/1200], Loss: 2.8602 , Accuracy : 15.00%\n",
      "Epoch [47/1200], Loss: 3.3422 , Accuracy : 17.50%\n",
      "Epoch [48/1200], Loss: 3.4304 , Accuracy : 17.50%\n",
      "Epoch [49/1200], Loss: 2.3872 , Accuracy : 20.00%\n",
      "Epoch [50/1200], Loss: 3.3399 , Accuracy : 22.50%\n",
      "Epoch [51/1200], Loss: 3.4399 , Accuracy : 17.50%\n",
      "Epoch [52/1200], Loss: 3.3062 , Accuracy : 10.00%\n",
      "Epoch [53/1200], Loss: 2.1765 , Accuracy : 17.50%\n",
      "Epoch [54/1200], Loss: 3.0488 , Accuracy : 10.00%\n",
      "Epoch [55/1200], Loss: 3.1241 , Accuracy : 7.50%\n",
      "Epoch [56/1200], Loss: 2.9527 , Accuracy : 15.00%\n",
      "Epoch [57/1200], Loss: 3.1816 , Accuracy : 17.50%\n",
      "Epoch [58/1200], Loss: 2.3735 , Accuracy : 10.00%\n",
      "Epoch [59/1200], Loss: 2.9936 , Accuracy : 10.00%\n",
      "Epoch [60/1200], Loss: 4.0333 , Accuracy : 5.00%\n",
      "Epoch [61/1200], Loss: 2.1902 , Accuracy : 5.00%\n",
      "Epoch [62/1200], Loss: 3.8229 , Accuracy : 7.50%\n",
      "Epoch [63/1200], Loss: 4.5548 , Accuracy : 17.50%\n",
      "Epoch [64/1200], Loss: 4.8254 , Accuracy : 17.50%\n",
      "Epoch [65/1200], Loss: 2.6404 , Accuracy : 12.50%\n",
      "Epoch [66/1200], Loss: 3.4183 , Accuracy : 15.00%\n",
      "Epoch [67/1200], Loss: 2.1104 , Accuracy : 7.50%\n",
      "Epoch [68/1200], Loss: 2.1771 , Accuracy : 12.50%\n",
      "Epoch [69/1200], Loss: 2.4215 , Accuracy : 10.00%\n",
      "Epoch [70/1200], Loss: 3.0524 , Accuracy : 12.50%\n",
      "Epoch [71/1200], Loss: 2.5432 , Accuracy : 20.00%\n",
      "Epoch [72/1200], Loss: 2.4441 , Accuracy : 17.50%\n",
      "Epoch [73/1200], Loss: 3.2502 , Accuracy : 20.00%\n",
      "Epoch [74/1200], Loss: 2.7291 , Accuracy : 27.50%\n",
      "Epoch [75/1200], Loss: 1.7520 , Accuracy : 22.50%\n",
      "Epoch [76/1200], Loss: 1.5967 , Accuracy : 27.50%\n",
      "Epoch [77/1200], Loss: 3.4094 , Accuracy : 20.00%\n",
      "Epoch [78/1200], Loss: 2.0790 , Accuracy : 20.00%\n",
      "Epoch [79/1200], Loss: 3.0840 , Accuracy : 10.00%\n",
      "Epoch [80/1200], Loss: 2.2992 , Accuracy : 22.50%\n",
      "Epoch [81/1200], Loss: 2.4109 , Accuracy : 15.00%\n",
      "Epoch [82/1200], Loss: 2.4292 , Accuracy : 22.50%\n",
      "Epoch [83/1200], Loss: 1.7604 , Accuracy : 17.50%\n",
      "Epoch [84/1200], Loss: 1.7634 , Accuracy : 10.00%\n",
      "Epoch [85/1200], Loss: 2.7432 , Accuracy : 22.50%\n",
      "Epoch [86/1200], Loss: 2.5713 , Accuracy : 27.50%\n",
      "Epoch [87/1200], Loss: 1.8570 , Accuracy : 17.50%\n",
      "Epoch [88/1200], Loss: 2.2384 , Accuracy : 25.00%\n",
      "Epoch [89/1200], Loss: 2.8717 , Accuracy : 17.50%\n",
      "Epoch [90/1200], Loss: 2.1588 , Accuracy : 25.00%\n",
      "Epoch [91/1200], Loss: 2.5047 , Accuracy : 35.00%\n",
      "Epoch [92/1200], Loss: 4.2775 , Accuracy : 22.50%\n",
      "Epoch [93/1200], Loss: 2.0487 , Accuracy : 30.00%\n",
      "Epoch [94/1200], Loss: 1.1578 , Accuracy : 20.00%\n",
      "Epoch [95/1200], Loss: 3.0516 , Accuracy : 22.50%\n",
      "Epoch [96/1200], Loss: 1.8009 , Accuracy : 30.00%\n",
      "Epoch [97/1200], Loss: 2.0862 , Accuracy : 35.00%\n",
      "Epoch [98/1200], Loss: 2.4465 , Accuracy : 30.00%\n",
      "Epoch [99/1200], Loss: 2.2474 , Accuracy : 25.00%\n",
      "Epoch [100/1200], Loss: 1.7536 , Accuracy : 35.00%\n",
      "Epoch [101/1200], Loss: 1.7386 , Accuracy : 25.00%\n",
      "Epoch [102/1200], Loss: 2.4534 , Accuracy : 32.50%\n",
      "Epoch [103/1200], Loss: 3.2860 , Accuracy : 37.50%\n",
      "Epoch [104/1200], Loss: 1.6781 , Accuracy : 35.00%\n",
      "Epoch [105/1200], Loss: 1.5014 , Accuracy : 30.00%\n",
      "Epoch [106/1200], Loss: 2.1883 , Accuracy : 25.00%\n",
      "Epoch [107/1200], Loss: 1.3920 , Accuracy : 20.00%\n",
      "Epoch [108/1200], Loss: 2.3280 , Accuracy : 27.50%\n",
      "Epoch [109/1200], Loss: 2.5906 , Accuracy : 27.50%\n",
      "Epoch [110/1200], Loss: 2.2701 , Accuracy : 27.50%\n",
      "Epoch [111/1200], Loss: 1.8557 , Accuracy : 30.00%\n",
      "Epoch [112/1200], Loss: 2.1603 , Accuracy : 20.00%\n",
      "Epoch [113/1200], Loss: 1.4710 , Accuracy : 30.00%\n",
      "Epoch [114/1200], Loss: 2.3156 , Accuracy : 37.50%\n",
      "Epoch [115/1200], Loss: 1.8384 , Accuracy : 30.00%\n",
      "Epoch [116/1200], Loss: 1.7568 , Accuracy : 27.50%\n",
      "Epoch [117/1200], Loss: 1.4960 , Accuracy : 32.50%\n",
      "Epoch [118/1200], Loss: 1.7064 , Accuracy : 25.00%\n",
      "Epoch [119/1200], Loss: 1.1125 , Accuracy : 27.50%\n",
      "Epoch [120/1200], Loss: 0.8905 , Accuracy : 25.00%\n",
      "Epoch [121/1200], Loss: 1.8636 , Accuracy : 27.50%\n",
      "Epoch [122/1200], Loss: 1.3770 , Accuracy : 32.50%\n",
      "Epoch [123/1200], Loss: 2.3732 , Accuracy : 22.50%\n",
      "Epoch [124/1200], Loss: 1.5151 , Accuracy : 30.00%\n",
      "Epoch [125/1200], Loss: 1.4225 , Accuracy : 32.50%\n",
      "Epoch [126/1200], Loss: 2.1331 , Accuracy : 22.50%\n",
      "Epoch [127/1200], Loss: 1.7472 , Accuracy : 30.00%\n",
      "Epoch [128/1200], Loss: 1.9242 , Accuracy : 30.00%\n",
      "Epoch [129/1200], Loss: 1.2581 , Accuracy : 32.50%\n",
      "Epoch [130/1200], Loss: 1.2460 , Accuracy : 20.00%\n",
      "Epoch [131/1200], Loss: 2.5800 , Accuracy : 35.00%\n",
      "Epoch [132/1200], Loss: 2.4732 , Accuracy : 25.00%\n",
      "Epoch [133/1200], Loss: 1.6710 , Accuracy : 32.50%\n",
      "Epoch [134/1200], Loss: 2.2525 , Accuracy : 40.00%\n",
      "Epoch [135/1200], Loss: 1.7423 , Accuracy : 27.50%\n",
      "Epoch [136/1200], Loss: 1.1278 , Accuracy : 35.00%\n",
      "Epoch [137/1200], Loss: 1.9318 , Accuracy : 42.50%\n",
      "Epoch [138/1200], Loss: 2.2227 , Accuracy : 32.50%\n",
      "Epoch [139/1200], Loss: 1.2111 , Accuracy : 35.00%\n",
      "Epoch [140/1200], Loss: 3.0500 , Accuracy : 40.00%\n",
      "Epoch [141/1200], Loss: 1.2206 , Accuracy : 30.00%\n",
      "Epoch [142/1200], Loss: 2.0854 , Accuracy : 30.00%\n",
      "Epoch [143/1200], Loss: 3.2761 , Accuracy : 20.00%\n",
      "Epoch [144/1200], Loss: 2.3031 , Accuracy : 35.00%\n",
      "Epoch [145/1200], Loss: 2.4406 , Accuracy : 35.00%\n",
      "Epoch [146/1200], Loss: 1.9996 , Accuracy : 22.50%\n",
      "Epoch [147/1200], Loss: 1.3603 , Accuracy : 30.00%\n",
      "Epoch [148/1200], Loss: 1.6710 , Accuracy : 35.00%\n",
      "Epoch [149/1200], Loss: 0.9530 , Accuracy : 40.00%\n",
      "Epoch [150/1200], Loss: 2.0583 , Accuracy : 40.00%\n",
      "Epoch [151/1200], Loss: 4.1389 , Accuracy : 42.50%\n",
      "Epoch [152/1200], Loss: 1.4580 , Accuracy : 30.00%\n",
      "Epoch [153/1200], Loss: 2.0678 , Accuracy : 45.00%\n",
      "Epoch [154/1200], Loss: 2.1871 , Accuracy : 45.00%\n",
      "Epoch [155/1200], Loss: 1.4200 , Accuracy : 40.00%\n",
      "Epoch [156/1200], Loss: 2.0164 , Accuracy : 40.00%\n",
      "Epoch [157/1200], Loss: 2.3861 , Accuracy : 35.00%\n",
      "Epoch [158/1200], Loss: 1.6030 , Accuracy : 47.50%\n",
      "Epoch [159/1200], Loss: 1.6083 , Accuracy : 40.00%\n",
      "Epoch [160/1200], Loss: 1.4514 , Accuracy : 35.00%\n",
      "Epoch [161/1200], Loss: 1.8738 , Accuracy : 32.50%\n",
      "Epoch [162/1200], Loss: 0.9897 , Accuracy : 22.50%\n",
      "Epoch [163/1200], Loss: 1.4416 , Accuracy : 42.50%\n",
      "Epoch [164/1200], Loss: 1.1287 , Accuracy : 35.00%\n",
      "Epoch [165/1200], Loss: 1.0215 , Accuracy : 40.00%\n",
      "Epoch [166/1200], Loss: 1.5523 , Accuracy : 32.50%\n",
      "Epoch [167/1200], Loss: 1.2926 , Accuracy : 37.50%\n",
      "Epoch [168/1200], Loss: 0.8076 , Accuracy : 42.50%\n",
      "Epoch [169/1200], Loss: 0.8187 , Accuracy : 37.50%\n",
      "Epoch [170/1200], Loss: 2.2467 , Accuracy : 47.50%\n",
      "Epoch [171/1200], Loss: 0.9625 , Accuracy : 52.50%\n",
      "Epoch [172/1200], Loss: 2.8762 , Accuracy : 42.50%\n",
      "Epoch [173/1200], Loss: 1.4564 , Accuracy : 42.50%\n",
      "Epoch [174/1200], Loss: 1.7801 , Accuracy : 30.00%\n",
      "Epoch [175/1200], Loss: 0.6471 , Accuracy : 37.50%\n",
      "Epoch [176/1200], Loss: 1.4315 , Accuracy : 27.50%\n",
      "Epoch [177/1200], Loss: 1.1655 , Accuracy : 42.50%\n",
      "Epoch [178/1200], Loss: 1.1705 , Accuracy : 35.00%\n",
      "Epoch [179/1200], Loss: 2.3711 , Accuracy : 42.50%\n",
      "Epoch [180/1200], Loss: 1.0312 , Accuracy : 37.50%\n",
      "Epoch [181/1200], Loss: 3.0228 , Accuracy : 32.50%\n",
      "Epoch [182/1200], Loss: 1.8197 , Accuracy : 42.50%\n",
      "Epoch [183/1200], Loss: 1.4467 , Accuracy : 37.50%\n",
      "Epoch [184/1200], Loss: 1.2662 , Accuracy : 45.00%\n",
      "Epoch [185/1200], Loss: 1.0654 , Accuracy : 45.00%\n",
      "Epoch [186/1200], Loss: 1.7408 , Accuracy : 40.00%\n",
      "Epoch [187/1200], Loss: 1.6581 , Accuracy : 47.50%\n",
      "Epoch [188/1200], Loss: 2.1946 , Accuracy : 50.00%\n",
      "Epoch [189/1200], Loss: 1.8260 , Accuracy : 42.50%\n",
      "Epoch [190/1200], Loss: 2.0623 , Accuracy : 42.50%\n",
      "Epoch [191/1200], Loss: 1.0216 , Accuracy : 37.50%\n",
      "Epoch [192/1200], Loss: 1.6713 , Accuracy : 35.00%\n",
      "Epoch [193/1200], Loss: 3.2933 , Accuracy : 47.50%\n",
      "Epoch [194/1200], Loss: 1.2460 , Accuracy : 45.00%\n",
      "Epoch [195/1200], Loss: 1.6583 , Accuracy : 40.00%\n",
      "Epoch [196/1200], Loss: 1.8558 , Accuracy : 35.00%\n",
      "Epoch [197/1200], Loss: 1.6972 , Accuracy : 45.00%\n",
      "Epoch [198/1200], Loss: 2.7778 , Accuracy : 17.50%\n",
      "Epoch [199/1200], Loss: 0.9938 , Accuracy : 45.00%\n",
      "Epoch [200/1200], Loss: 1.1890 , Accuracy : 35.00%\n",
      "Epoch [201/1200], Loss: 2.5776 , Accuracy : 30.00%\n",
      "Epoch [202/1200], Loss: 2.7698 , Accuracy : 37.50%\n",
      "Epoch [203/1200], Loss: 0.7724 , Accuracy : 22.50%\n",
      "Epoch [204/1200], Loss: 1.6853 , Accuracy : 32.50%\n",
      "Epoch [205/1200], Loss: 2.2239 , Accuracy : 37.50%\n",
      "Epoch [206/1200], Loss: 1.2536 , Accuracy : 42.50%\n",
      "Epoch [207/1200], Loss: 1.2545 , Accuracy : 50.00%\n",
      "Epoch [208/1200], Loss: 1.9116 , Accuracy : 50.00%\n",
      "Epoch [209/1200], Loss: 0.8132 , Accuracy : 35.00%\n",
      "Epoch [210/1200], Loss: 2.2154 , Accuracy : 40.00%\n",
      "Epoch [211/1200], Loss: 1.1225 , Accuracy : 47.50%\n",
      "Epoch [212/1200], Loss: 0.7010 , Accuracy : 57.50%\n",
      "Epoch [213/1200], Loss: 1.5081 , Accuracy : 47.50%\n",
      "Epoch [214/1200], Loss: 1.9962 , Accuracy : 35.00%\n",
      "Epoch [215/1200], Loss: 0.7031 , Accuracy : 32.50%\n",
      "Epoch [216/1200], Loss: 2.1641 , Accuracy : 32.50%\n",
      "Epoch [217/1200], Loss: 1.0536 , Accuracy : 55.00%\n",
      "Epoch [218/1200], Loss: 0.7124 , Accuracy : 40.00%\n",
      "Epoch [219/1200], Loss: 0.3586 , Accuracy : 45.00%\n",
      "Epoch [220/1200], Loss: 1.4579 , Accuracy : 47.50%\n",
      "Epoch [221/1200], Loss: 0.9061 , Accuracy : 47.50%\n",
      "Epoch [222/1200], Loss: 0.9440 , Accuracy : 45.00%\n",
      "Epoch [223/1200], Loss: 3.2285 , Accuracy : 32.50%\n",
      "Epoch [224/1200], Loss: 1.7759 , Accuracy : 30.00%\n",
      "Epoch [225/1200], Loss: 1.2694 , Accuracy : 42.50%\n",
      "Epoch [226/1200], Loss: 0.7883 , Accuracy : 50.00%\n",
      "Epoch [227/1200], Loss: 2.1182 , Accuracy : 42.50%\n",
      "Epoch [228/1200], Loss: 1.8588 , Accuracy : 35.00%\n",
      "Epoch [229/1200], Loss: 2.0773 , Accuracy : 40.00%\n",
      "Epoch [230/1200], Loss: 1.7570 , Accuracy : 40.00%\n",
      "Epoch [231/1200], Loss: 1.2229 , Accuracy : 37.50%\n",
      "Epoch [232/1200], Loss: 1.6346 , Accuracy : 37.50%\n",
      "Epoch [233/1200], Loss: 0.6852 , Accuracy : 37.50%\n",
      "Epoch [234/1200], Loss: 1.3502 , Accuracy : 52.50%\n",
      "Epoch [235/1200], Loss: 0.6314 , Accuracy : 57.50%\n",
      "Epoch [236/1200], Loss: 1.7395 , Accuracy : 50.00%\n",
      "Epoch [237/1200], Loss: 0.7230 , Accuracy : 55.00%\n",
      "Epoch [238/1200], Loss: 2.0134 , Accuracy : 42.50%\n",
      "Epoch [239/1200], Loss: 0.7960 , Accuracy : 45.00%\n",
      "Epoch [240/1200], Loss: 1.7837 , Accuracy : 45.00%\n",
      "Epoch [241/1200], Loss: 1.6804 , Accuracy : 42.50%\n",
      "Epoch [242/1200], Loss: 1.8091 , Accuracy : 47.50%\n",
      "Epoch [243/1200], Loss: 1.6121 , Accuracy : 55.00%\n",
      "Epoch [244/1200], Loss: 0.6773 , Accuracy : 55.00%\n",
      "Epoch [245/1200], Loss: 3.7527 , Accuracy : 47.50%\n",
      "Epoch [246/1200], Loss: 0.7363 , Accuracy : 55.00%\n",
      "Epoch [247/1200], Loss: 2.2720 , Accuracy : 40.00%\n",
      "Epoch [248/1200], Loss: 1.5396 , Accuracy : 42.50%\n",
      "Epoch [249/1200], Loss: 0.6239 , Accuracy : 62.50%\n",
      "Epoch [250/1200], Loss: 1.0076 , Accuracy : 50.00%\n",
      "Epoch [251/1200], Loss: 1.5310 , Accuracy : 47.50%\n",
      "Epoch [252/1200], Loss: 0.7333 , Accuracy : 47.50%\n",
      "Epoch [253/1200], Loss: 0.9402 , Accuracy : 60.00%\n",
      "Epoch [254/1200], Loss: 0.9476 , Accuracy : 62.50%\n",
      "Epoch [255/1200], Loss: 1.4819 , Accuracy : 47.50%\n",
      "Epoch [256/1200], Loss: 0.4684 , Accuracy : 50.00%\n",
      "Epoch [257/1200], Loss: 1.3397 , Accuracy : 45.00%\n",
      "Epoch [258/1200], Loss: 1.8804 , Accuracy : 52.50%\n",
      "Epoch [259/1200], Loss: 0.2645 , Accuracy : 57.50%\n",
      "Epoch [260/1200], Loss: 1.5851 , Accuracy : 47.50%\n",
      "Epoch [261/1200], Loss: 0.7277 , Accuracy : 52.50%\n",
      "Epoch [262/1200], Loss: 1.9294 , Accuracy : 57.50%\n",
      "Epoch [263/1200], Loss: 2.2017 , Accuracy : 52.50%\n",
      "Epoch [264/1200], Loss: 0.5051 , Accuracy : 50.00%\n",
      "Epoch [265/1200], Loss: 1.6052 , Accuracy : 47.50%\n",
      "Epoch [266/1200], Loss: 1.2949 , Accuracy : 52.50%\n",
      "Epoch [267/1200], Loss: 1.4450 , Accuracy : 60.00%\n",
      "Epoch [268/1200], Loss: 1.7438 , Accuracy : 47.50%\n",
      "Epoch [269/1200], Loss: 0.8336 , Accuracy : 52.50%\n",
      "Epoch [270/1200], Loss: 2.1995 , Accuracy : 45.00%\n",
      "Epoch [271/1200], Loss: 1.4983 , Accuracy : 52.50%\n",
      "Epoch [272/1200], Loss: 0.7591 , Accuracy : 52.50%\n",
      "Epoch [273/1200], Loss: 1.5536 , Accuracy : 47.50%\n",
      "Epoch [274/1200], Loss: 2.0047 , Accuracy : 50.00%\n",
      "Epoch [275/1200], Loss: 1.5569 , Accuracy : 40.00%\n",
      "Epoch [276/1200], Loss: 0.9733 , Accuracy : 45.00%\n",
      "Epoch [277/1200], Loss: 0.5797 , Accuracy : 50.00%\n",
      "Epoch [278/1200], Loss: 0.6448 , Accuracy : 52.50%\n",
      "Epoch [279/1200], Loss: 0.7193 , Accuracy : 57.50%\n",
      "Epoch [280/1200], Loss: 0.8564 , Accuracy : 62.50%\n",
      "Epoch [281/1200], Loss: 1.2718 , Accuracy : 70.00%\n",
      "Epoch [282/1200], Loss: 1.1301 , Accuracy : 65.00%\n",
      "Epoch [283/1200], Loss: 0.4435 , Accuracy : 60.00%\n",
      "Epoch [284/1200], Loss: 0.7750 , Accuracy : 65.00%\n",
      "Epoch [285/1200], Loss: 0.5745 , Accuracy : 62.50%\n",
      "Epoch [286/1200], Loss: 1.3236 , Accuracy : 70.00%\n",
      "Epoch [287/1200], Loss: 0.5424 , Accuracy : 45.00%\n",
      "Epoch [288/1200], Loss: 1.5978 , Accuracy : 57.50%\n",
      "Epoch [289/1200], Loss: 0.6877 , Accuracy : 52.50%\n",
      "Epoch [290/1200], Loss: 0.4441 , Accuracy : 72.50%\n",
      "Epoch [291/1200], Loss: 1.6348 , Accuracy : 62.50%\n",
      "Epoch [292/1200], Loss: 0.3754 , Accuracy : 65.00%\n",
      "Epoch [293/1200], Loss: 0.5452 , Accuracy : 65.00%\n",
      "Epoch [294/1200], Loss: 0.2534 , Accuracy : 65.00%\n",
      "Epoch [295/1200], Loss: 0.8702 , Accuracy : 67.50%\n",
      "Epoch [296/1200], Loss: 0.7401 , Accuracy : 42.50%\n",
      "Epoch [297/1200], Loss: 1.0322 , Accuracy : 27.50%\n",
      "Epoch [298/1200], Loss: 4.0702 , Accuracy : 15.00%\n",
      "Epoch [299/1200], Loss: 2.8057 , Accuracy : 22.50%\n",
      "Epoch [300/1200], Loss: 1.4083 , Accuracy : 20.00%\n",
      "Epoch [301/1200], Loss: 2.0621 , Accuracy : 20.00%\n",
      "Epoch [302/1200], Loss: 1.5961 , Accuracy : 30.00%\n",
      "Epoch [303/1200], Loss: 2.5135 , Accuracy : 37.50%\n",
      "Epoch [304/1200], Loss: 2.0088 , Accuracy : 40.00%\n",
      "Epoch [305/1200], Loss: 1.3755 , Accuracy : 50.00%\n",
      "Epoch [306/1200], Loss: 3.6352 , Accuracy : 57.50%\n",
      "Epoch [307/1200], Loss: 1.9581 , Accuracy : 45.00%\n",
      "Epoch [308/1200], Loss: 1.4539 , Accuracy : 47.50%\n",
      "Epoch [309/1200], Loss: 1.4436 , Accuracy : 52.50%\n",
      "Epoch [310/1200], Loss: 0.5738 , Accuracy : 42.50%\n",
      "Epoch [311/1200], Loss: 0.7360 , Accuracy : 50.00%\n",
      "Epoch [312/1200], Loss: 1.3082 , Accuracy : 35.00%\n",
      "Epoch [313/1200], Loss: 2.3913 , Accuracy : 35.00%\n",
      "Epoch [314/1200], Loss: 1.7474 , Accuracy : 47.50%\n",
      "Epoch [315/1200], Loss: 1.4652 , Accuracy : 52.50%\n",
      "Epoch [316/1200], Loss: 1.4892 , Accuracy : 52.50%\n",
      "Epoch [317/1200], Loss: 1.8746 , Accuracy : 50.00%\n",
      "Epoch [318/1200], Loss: 0.2733 , Accuracy : 47.50%\n",
      "Epoch [319/1200], Loss: 1.7279 , Accuracy : 50.00%\n",
      "Epoch [320/1200], Loss: 1.1547 , Accuracy : 62.50%\n",
      "Epoch [321/1200], Loss: 1.6551 , Accuracy : 52.50%\n",
      "Epoch [322/1200], Loss: 1.0579 , Accuracy : 55.00%\n",
      "Epoch [323/1200], Loss: 1.0914 , Accuracy : 65.00%\n",
      "Epoch [324/1200], Loss: 1.2718 , Accuracy : 55.00%\n",
      "Epoch [325/1200], Loss: 0.6338 , Accuracy : 67.50%\n",
      "Epoch [326/1200], Loss: 0.6346 , Accuracy : 65.00%\n",
      "Epoch [327/1200], Loss: 0.7514 , Accuracy : 62.50%\n",
      "Epoch [328/1200], Loss: 1.4698 , Accuracy : 50.00%\n",
      "Epoch [329/1200], Loss: 0.5864 , Accuracy : 67.50%\n",
      "Epoch [330/1200], Loss: 0.3302 , Accuracy : 60.00%\n",
      "Epoch [331/1200], Loss: 1.4433 , Accuracy : 60.00%\n",
      "Epoch [332/1200], Loss: 0.4742 , Accuracy : 62.50%\n",
      "Epoch [333/1200], Loss: 2.1402 , Accuracy : 57.50%\n",
      "Epoch [334/1200], Loss: 0.1702 , Accuracy : 67.50%\n",
      "Epoch [335/1200], Loss: 1.6255 , Accuracy : 55.00%\n",
      "Epoch [336/1200], Loss: 1.0170 , Accuracy : 57.50%\n",
      "Epoch [337/1200], Loss: 0.4985 , Accuracy : 45.00%\n",
      "Epoch [338/1200], Loss: 0.1387 , Accuracy : 47.50%\n",
      "Epoch [339/1200], Loss: 2.4455 , Accuracy : 50.00%\n",
      "Epoch [340/1200], Loss: 0.6604 , Accuracy : 75.00%\n",
      "Epoch [341/1200], Loss: 0.5057 , Accuracy : 72.50%\n",
      "Epoch [342/1200], Loss: 0.1392 , Accuracy : 72.50%\n",
      "Epoch [343/1200], Loss: 0.6243 , Accuracy : 60.00%\n",
      "Epoch [344/1200], Loss: 1.2851 , Accuracy : 50.00%\n",
      "Epoch [345/1200], Loss: 0.2437 , Accuracy : 65.00%\n",
      "Epoch [346/1200], Loss: 1.1998 , Accuracy : 67.50%\n",
      "Epoch [347/1200], Loss: 0.5437 , Accuracy : 50.00%\n",
      "Epoch [348/1200], Loss: 2.1286 , Accuracy : 50.00%\n",
      "Epoch [349/1200], Loss: 0.9180 , Accuracy : 65.00%\n",
      "Epoch [350/1200], Loss: 0.5651 , Accuracy : 60.00%\n",
      "Epoch [351/1200], Loss: 0.6557 , Accuracy : 55.00%\n",
      "Epoch [352/1200], Loss: 0.6064 , Accuracy : 72.50%\n",
      "Epoch [353/1200], Loss: 1.2950 , Accuracy : 50.00%\n",
      "Epoch [354/1200], Loss: 1.1418 , Accuracy : 50.00%\n",
      "Epoch [355/1200], Loss: 1.0222 , Accuracy : 62.50%\n",
      "Epoch [356/1200], Loss: 0.2959 , Accuracy : 70.00%\n",
      "Epoch [357/1200], Loss: 1.8470 , Accuracy : 67.50%\n",
      "Epoch [358/1200], Loss: 0.3209 , Accuracy : 70.00%\n",
      "Epoch [359/1200], Loss: 0.8272 , Accuracy : 70.00%\n",
      "Epoch [360/1200], Loss: 0.9395 , Accuracy : 60.00%\n",
      "Epoch [361/1200], Loss: 0.7436 , Accuracy : 85.00%\n",
      "Epoch [362/1200], Loss: 0.5958 , Accuracy : 57.50%\n",
      "Epoch [363/1200], Loss: 0.9017 , Accuracy : 72.50%\n",
      "Epoch [364/1200], Loss: 0.6274 , Accuracy : 57.50%\n",
      "Epoch [365/1200], Loss: 1.7772 , Accuracy : 57.50%\n",
      "Epoch [366/1200], Loss: 1.4657 , Accuracy : 72.50%\n",
      "Epoch [367/1200], Loss: 1.5569 , Accuracy : 52.50%\n",
      "Epoch [368/1200], Loss: 2.4470 , Accuracy : 62.50%\n",
      "Epoch [369/1200], Loss: 0.5382 , Accuracy : 60.00%\n",
      "Epoch [370/1200], Loss: 0.5258 , Accuracy : 62.50%\n",
      "Epoch [371/1200], Loss: 1.8266 , Accuracy : 65.00%\n",
      "Epoch [372/1200], Loss: 1.5626 , Accuracy : 77.50%\n",
      "Epoch [373/1200], Loss: 0.4201 , Accuracy : 70.00%\n",
      "Epoch [374/1200], Loss: 1.0756 , Accuracy : 65.00%\n",
      "Epoch [375/1200], Loss: 0.2523 , Accuracy : 65.00%\n",
      "Epoch [376/1200], Loss: 1.3145 , Accuracy : 57.50%\n",
      "Epoch [377/1200], Loss: 1.1187 , Accuracy : 60.00%\n",
      "Epoch [378/1200], Loss: 0.4993 , Accuracy : 65.00%\n",
      "Epoch [379/1200], Loss: 0.7954 , Accuracy : 52.50%\n",
      "Epoch [380/1200], Loss: 0.5737 , Accuracy : 52.50%\n",
      "Epoch [381/1200], Loss: 0.4568 , Accuracy : 57.50%\n",
      "Epoch [382/1200], Loss: 0.7216 , Accuracy : 52.50%\n",
      "Epoch [383/1200], Loss: 1.2871 , Accuracy : 40.00%\n",
      "Epoch [384/1200], Loss: 0.7674 , Accuracy : 55.00%\n",
      "Epoch [385/1200], Loss: 0.9567 , Accuracy : 45.00%\n",
      "Epoch [386/1200], Loss: 0.6443 , Accuracy : 62.50%\n",
      "Epoch [387/1200], Loss: 1.3545 , Accuracy : 47.50%\n",
      "Epoch [388/1200], Loss: 1.3198 , Accuracy : 55.00%\n",
      "Epoch [389/1200], Loss: 0.9020 , Accuracy : 55.00%\n",
      "Epoch [390/1200], Loss: 1.5459 , Accuracy : 57.50%\n",
      "Epoch [391/1200], Loss: 0.5708 , Accuracy : 67.50%\n",
      "Epoch [392/1200], Loss: 1.1271 , Accuracy : 65.00%\n",
      "Epoch [393/1200], Loss: 0.0895 , Accuracy : 67.50%\n",
      "Epoch [394/1200], Loss: 1.6604 , Accuracy : 65.00%\n",
      "Epoch [395/1200], Loss: 0.5144 , Accuracy : 65.00%\n",
      "Epoch [396/1200], Loss: 0.1631 , Accuracy : 57.50%\n",
      "Epoch [397/1200], Loss: 0.9501 , Accuracy : 52.50%\n",
      "Epoch [398/1200], Loss: 1.8081 , Accuracy : 75.00%\n",
      "Epoch [399/1200], Loss: 0.2056 , Accuracy : 72.50%\n",
      "Epoch [400/1200], Loss: 0.7192 , Accuracy : 60.00%\n",
      "Epoch [401/1200], Loss: 0.4134 , Accuracy : 65.00%\n",
      "Epoch [402/1200], Loss: 0.8757 , Accuracy : 70.00%\n",
      "Epoch [403/1200], Loss: 0.1873 , Accuracy : 65.00%\n",
      "Epoch [404/1200], Loss: 1.9183 , Accuracy : 65.00%\n",
      "Epoch [405/1200], Loss: 0.5221 , Accuracy : 65.00%\n",
      "Epoch [406/1200], Loss: 0.3613 , Accuracy : 62.50%\n",
      "Epoch [407/1200], Loss: 0.4955 , Accuracy : 72.50%\n",
      "Epoch [408/1200], Loss: 0.5498 , Accuracy : 72.50%\n",
      "Epoch [409/1200], Loss: 1.4799 , Accuracy : 65.00%\n",
      "Epoch [410/1200], Loss: 0.6283 , Accuracy : 72.50%\n",
      "Epoch [411/1200], Loss: 1.2546 , Accuracy : 60.00%\n",
      "Epoch [412/1200], Loss: 0.2731 , Accuracy : 65.00%\n",
      "Epoch [413/1200], Loss: 0.6461 , Accuracy : 75.00%\n",
      "Epoch [414/1200], Loss: 0.2663 , Accuracy : 65.00%\n",
      "Epoch [415/1200], Loss: 1.1248 , Accuracy : 60.00%\n",
      "Epoch [416/1200], Loss: 1.3132 , Accuracy : 55.00%\n",
      "Epoch [417/1200], Loss: 2.4639 , Accuracy : 57.50%\n",
      "Epoch [418/1200], Loss: 0.5126 , Accuracy : 52.50%\n",
      "Epoch [419/1200], Loss: 1.4330 , Accuracy : 45.00%\n",
      "Epoch [420/1200], Loss: 0.5428 , Accuracy : 57.50%\n",
      "Epoch [421/1200], Loss: 1.7088 , Accuracy : 45.00%\n",
      "Epoch [422/1200], Loss: 0.6315 , Accuracy : 60.00%\n",
      "Epoch [423/1200], Loss: 0.7760 , Accuracy : 55.00%\n",
      "Epoch [424/1200], Loss: 0.6137 , Accuracy : 52.50%\n",
      "Epoch [425/1200], Loss: 1.3629 , Accuracy : 55.00%\n",
      "Epoch [426/1200], Loss: 2.1084 , Accuracy : 60.00%\n",
      "Epoch [427/1200], Loss: 0.7286 , Accuracy : 62.50%\n",
      "Epoch [428/1200], Loss: 0.4854 , Accuracy : 70.00%\n",
      "Epoch [429/1200], Loss: 1.0449 , Accuracy : 62.50%\n",
      "Epoch [430/1200], Loss: 0.7579 , Accuracy : 75.00%\n",
      "Epoch [431/1200], Loss: 0.8047 , Accuracy : 60.00%\n",
      "Epoch [432/1200], Loss: 2.8710 , Accuracy : 52.50%\n",
      "Epoch [433/1200], Loss: 1.4425 , Accuracy : 60.00%\n",
      "Epoch [434/1200], Loss: 0.7152 , Accuracy : 60.00%\n",
      "Epoch [435/1200], Loss: 0.6316 , Accuracy : 70.00%\n",
      "Epoch [436/1200], Loss: 0.7264 , Accuracy : 72.50%\n",
      "Epoch [437/1200], Loss: 0.3398 , Accuracy : 67.50%\n",
      "Epoch [438/1200], Loss: 0.7515 , Accuracy : 65.00%\n",
      "Epoch [439/1200], Loss: 0.4152 , Accuracy : 77.50%\n",
      "Epoch [440/1200], Loss: 0.5023 , Accuracy : 75.00%\n",
      "Epoch [441/1200], Loss: 1.7280 , Accuracy : 70.00%\n",
      "Epoch [442/1200], Loss: 0.7651 , Accuracy : 60.00%\n",
      "Epoch [443/1200], Loss: 0.7130 , Accuracy : 75.00%\n",
      "Epoch [444/1200], Loss: 0.7283 , Accuracy : 70.00%\n",
      "Epoch [445/1200], Loss: 1.8632 , Accuracy : 67.50%\n",
      "Epoch [446/1200], Loss: 0.7840 , Accuracy : 67.50%\n",
      "Epoch [447/1200], Loss: 0.4520 , Accuracy : 55.00%\n",
      "Epoch [448/1200], Loss: 0.9874 , Accuracy : 80.00%\n",
      "Epoch [449/1200], Loss: 0.3290 , Accuracy : 65.00%\n",
      "Epoch [450/1200], Loss: 1.0310 , Accuracy : 77.50%\n",
      "Epoch [451/1200], Loss: 0.4434 , Accuracy : 75.00%\n",
      "Epoch [452/1200], Loss: 0.7668 , Accuracy : 70.00%\n",
      "Epoch [453/1200], Loss: 0.3571 , Accuracy : 70.00%\n",
      "Epoch [454/1200], Loss: 1.2149 , Accuracy : 75.00%\n",
      "Epoch [455/1200], Loss: 1.5145 , Accuracy : 70.00%\n",
      "Epoch [456/1200], Loss: 1.4687 , Accuracy : 75.00%\n",
      "Epoch [457/1200], Loss: 1.2420 , Accuracy : 77.50%\n",
      "Epoch [458/1200], Loss: 1.7471 , Accuracy : 67.50%\n",
      "Epoch [459/1200], Loss: 0.8226 , Accuracy : 72.50%\n",
      "Epoch [460/1200], Loss: 1.0610 , Accuracy : 67.50%\n",
      "Epoch [461/1200], Loss: 0.4470 , Accuracy : 72.50%\n",
      "Epoch [462/1200], Loss: 0.1810 , Accuracy : 77.50%\n",
      "Epoch [463/1200], Loss: 0.2257 , Accuracy : 80.00%\n",
      "Epoch [464/1200], Loss: 0.6595 , Accuracy : 80.00%\n",
      "Epoch [465/1200], Loss: 1.1785 , Accuracy : 72.50%\n",
      "Epoch [466/1200], Loss: 2.0281 , Accuracy : 72.50%\n",
      "Epoch [467/1200], Loss: 0.5170 , Accuracy : 62.50%\n",
      "Epoch [468/1200], Loss: 1.0290 , Accuracy : 52.50%\n",
      "Epoch [469/1200], Loss: 0.3446 , Accuracy : 50.00%\n",
      "Epoch [470/1200], Loss: 0.2857 , Accuracy : 67.50%\n",
      "Epoch [471/1200], Loss: 0.4693 , Accuracy : 70.00%\n",
      "Epoch [472/1200], Loss: 0.4482 , Accuracy : 70.00%\n",
      "Epoch [473/1200], Loss: 0.4379 , Accuracy : 82.50%\n",
      "Epoch [474/1200], Loss: 0.5557 , Accuracy : 55.00%\n",
      "Epoch [475/1200], Loss: 0.0745 , Accuracy : 52.50%\n",
      "Epoch [476/1200], Loss: 1.0501 , Accuracy : 75.00%\n",
      "Epoch [477/1200], Loss: 1.8704 , Accuracy : 67.50%\n",
      "Epoch [478/1200], Loss: 0.3082 , Accuracy : 72.50%\n",
      "Epoch [479/1200], Loss: 2.7621 , Accuracy : 65.00%\n",
      "Epoch [480/1200], Loss: 1.6920 , Accuracy : 72.50%\n",
      "Epoch [481/1200], Loss: 0.4155 , Accuracy : 70.00%\n",
      "Epoch [482/1200], Loss: 1.3360 , Accuracy : 67.50%\n",
      "Epoch [483/1200], Loss: 0.6348 , Accuracy : 72.50%\n",
      "Epoch [484/1200], Loss: 1.2399 , Accuracy : 62.50%\n",
      "Epoch [485/1200], Loss: 1.2621 , Accuracy : 65.00%\n",
      "Epoch [486/1200], Loss: 1.2572 , Accuracy : 62.50%\n",
      "Epoch [487/1200], Loss: 0.1119 , Accuracy : 70.00%\n",
      "Epoch [488/1200], Loss: 0.1829 , Accuracy : 70.00%\n",
      "Epoch [489/1200], Loss: 0.0341 , Accuracy : 57.50%\n",
      "Epoch [490/1200], Loss: 0.4862 , Accuracy : 65.00%\n",
      "Epoch [491/1200], Loss: 0.7851 , Accuracy : 80.00%\n",
      "Epoch [492/1200], Loss: 0.6581 , Accuracy : 77.50%\n",
      "Epoch [493/1200], Loss: 0.3197 , Accuracy : 70.00%\n",
      "Epoch [494/1200], Loss: 0.7805 , Accuracy : 70.00%\n",
      "Epoch [495/1200], Loss: 0.2479 , Accuracy : 85.00%\n",
      "Epoch [496/1200], Loss: 0.4118 , Accuracy : 67.50%\n",
      "Epoch [497/1200], Loss: 0.5044 , Accuracy : 85.00%\n",
      "Epoch [498/1200], Loss: 0.8210 , Accuracy : 80.00%\n",
      "Epoch [499/1200], Loss: 0.5070 , Accuracy : 70.00%\n",
      "Epoch [500/1200], Loss: 2.2282 , Accuracy : 57.50%\n",
      "Epoch [501/1200], Loss: 1.1208 , Accuracy : 67.50%\n",
      "Epoch [502/1200], Loss: 0.7716 , Accuracy : 67.50%\n",
      "Epoch [503/1200], Loss: 0.1774 , Accuracy : 70.00%\n",
      "Epoch [504/1200], Loss: 0.3240 , Accuracy : 72.50%\n",
      "Epoch [505/1200], Loss: 0.2377 , Accuracy : 77.50%\n",
      "Epoch [506/1200], Loss: 0.4031 , Accuracy : 67.50%\n",
      "Epoch [507/1200], Loss: 0.6823 , Accuracy : 62.50%\n",
      "Epoch [508/1200], Loss: 0.4328 , Accuracy : 72.50%\n",
      "Epoch [509/1200], Loss: 0.6919 , Accuracy : 77.50%\n",
      "Epoch [510/1200], Loss: 1.0552 , Accuracy : 62.50%\n",
      "Epoch [511/1200], Loss: 0.9720 , Accuracy : 80.00%\n",
      "Epoch [512/1200], Loss: 0.2918 , Accuracy : 62.50%\n",
      "Epoch [513/1200], Loss: 0.3346 , Accuracy : 70.00%\n",
      "Epoch [514/1200], Loss: 0.7661 , Accuracy : 72.50%\n",
      "Epoch [515/1200], Loss: 0.0576 , Accuracy : 72.50%\n",
      "Epoch [516/1200], Loss: 0.6398 , Accuracy : 77.50%\n",
      "Epoch [517/1200], Loss: 0.6416 , Accuracy : 80.00%\n",
      "Epoch [518/1200], Loss: 0.3041 , Accuracy : 67.50%\n",
      "Epoch [519/1200], Loss: 0.3568 , Accuracy : 67.50%\n",
      "Epoch [520/1200], Loss: 1.6819 , Accuracy : 72.50%\n",
      "Epoch [521/1200], Loss: 0.8033 , Accuracy : 72.50%\n",
      "Epoch [522/1200], Loss: 1.0723 , Accuracy : 65.00%\n",
      "Epoch [523/1200], Loss: 0.2388 , Accuracy : 67.50%\n",
      "Epoch [524/1200], Loss: 1.1856 , Accuracy : 67.50%\n",
      "Epoch [525/1200], Loss: 0.4000 , Accuracy : 80.00%\n",
      "Epoch [526/1200], Loss: 0.0142 , Accuracy : 80.00%\n",
      "Epoch [527/1200], Loss: 0.2853 , Accuracy : 80.00%\n",
      "Epoch [528/1200], Loss: 0.4969 , Accuracy : 82.50%\n",
      "Epoch [529/1200], Loss: 0.5308 , Accuracy : 75.00%\n",
      "Epoch [530/1200], Loss: 0.3376 , Accuracy : 67.50%\n",
      "Epoch [531/1200], Loss: 0.5782 , Accuracy : 77.50%\n",
      "Epoch [532/1200], Loss: 0.6832 , Accuracy : 75.00%\n",
      "Epoch [533/1200], Loss: 0.4827 , Accuracy : 67.50%\n",
      "Epoch [534/1200], Loss: 0.1452 , Accuracy : 80.00%\n",
      "Epoch [535/1200], Loss: 0.6502 , Accuracy : 72.50%\n",
      "Epoch [536/1200], Loss: 1.9686 , Accuracy : 67.50%\n",
      "Epoch [537/1200], Loss: 1.3798 , Accuracy : 65.00%\n",
      "Epoch [538/1200], Loss: 0.2644 , Accuracy : 47.50%\n",
      "Epoch [539/1200], Loss: 0.2296 , Accuracy : 72.50%\n",
      "Epoch [540/1200], Loss: 0.9010 , Accuracy : 72.50%\n",
      "Epoch [541/1200], Loss: 1.2743 , Accuracy : 72.50%\n",
      "Epoch [542/1200], Loss: 0.8519 , Accuracy : 72.50%\n",
      "Epoch [543/1200], Loss: 0.1389 , Accuracy : 70.00%\n",
      "Epoch [544/1200], Loss: 1.1754 , Accuracy : 45.00%\n",
      "Epoch [545/1200], Loss: 1.7798 , Accuracy : 62.50%\n",
      "Epoch [546/1200], Loss: 1.3054 , Accuracy : 45.00%\n",
      "Epoch [547/1200], Loss: 1.0945 , Accuracy : 65.00%\n",
      "Epoch [548/1200], Loss: 0.7385 , Accuracy : 60.00%\n",
      "Epoch [549/1200], Loss: 1.8381 , Accuracy : 62.50%\n",
      "Epoch [550/1200], Loss: 0.9284 , Accuracy : 75.00%\n",
      "Epoch [551/1200], Loss: 0.4445 , Accuracy : 72.50%\n",
      "Epoch [552/1200], Loss: 0.3501 , Accuracy : 70.00%\n",
      "Epoch [553/1200], Loss: 0.5063 , Accuracy : 77.50%\n",
      "Epoch [554/1200], Loss: 0.4144 , Accuracy : 85.00%\n",
      "Epoch [555/1200], Loss: 0.8069 , Accuracy : 77.50%\n",
      "Epoch [556/1200], Loss: 0.1513 , Accuracy : 82.50%\n",
      "Epoch [557/1200], Loss: 0.4648 , Accuracy : 92.50%\n",
      "Epoch [558/1200], Loss: 0.2212 , Accuracy : 82.50%\n",
      "Epoch [559/1200], Loss: 0.0322 , Accuracy : 77.50%\n",
      "Epoch [560/1200], Loss: 0.5898 , Accuracy : 82.50%\n",
      "Epoch [561/1200], Loss: 0.8441 , Accuracy : 75.00%\n",
      "Epoch [562/1200], Loss: 1.5491 , Accuracy : 80.00%\n",
      "Epoch [563/1200], Loss: 0.5952 , Accuracy : 75.00%\n",
      "Epoch [564/1200], Loss: 0.3029 , Accuracy : 77.50%\n",
      "Epoch [565/1200], Loss: 0.3637 , Accuracy : 77.50%\n",
      "Epoch [566/1200], Loss: 0.3511 , Accuracy : 72.50%\n",
      "Epoch [567/1200], Loss: 0.8674 , Accuracy : 80.00%\n",
      "Epoch [568/1200], Loss: 0.2239 , Accuracy : 80.00%\n",
      "Epoch [569/1200], Loss: 1.1231 , Accuracy : 82.50%\n",
      "Epoch [570/1200], Loss: 0.2020 , Accuracy : 87.50%\n",
      "Epoch [571/1200], Loss: 0.5501 , Accuracy : 80.00%\n",
      "Epoch [572/1200], Loss: 0.4410 , Accuracy : 80.00%\n",
      "Epoch [573/1200], Loss: 0.0729 , Accuracy : 75.00%\n",
      "Epoch [574/1200], Loss: 0.3656 , Accuracy : 77.50%\n",
      "Epoch [575/1200], Loss: 0.2662 , Accuracy : 82.50%\n",
      "Epoch [576/1200], Loss: 0.6874 , Accuracy : 77.50%\n",
      "Epoch [577/1200], Loss: 0.6859 , Accuracy : 75.00%\n",
      "Epoch [578/1200], Loss: 0.5669 , Accuracy : 87.50%\n",
      "Epoch [579/1200], Loss: 0.1598 , Accuracy : 77.50%\n",
      "Epoch [580/1200], Loss: 0.5476 , Accuracy : 77.50%\n",
      "Epoch [581/1200], Loss: 1.4623 , Accuracy : 62.50%\n",
      "Epoch [582/1200], Loss: 1.0183 , Accuracy : 67.50%\n",
      "Epoch [583/1200], Loss: 0.5949 , Accuracy : 65.00%\n",
      "Epoch [584/1200], Loss: 1.0834 , Accuracy : 67.50%\n",
      "Epoch [585/1200], Loss: 3.3873 , Accuracy : 60.00%\n",
      "Epoch [586/1200], Loss: 1.4713 , Accuracy : 60.00%\n",
      "Epoch [587/1200], Loss: 1.4476 , Accuracy : 57.50%\n",
      "Epoch [588/1200], Loss: 1.0503 , Accuracy : 65.00%\n",
      "Epoch [589/1200], Loss: 0.7848 , Accuracy : 75.00%\n",
      "Epoch [590/1200], Loss: 0.8255 , Accuracy : 75.00%\n",
      "Epoch [591/1200], Loss: 1.0018 , Accuracy : 70.00%\n",
      "Epoch [592/1200], Loss: 0.2507 , Accuracy : 82.50%\n",
      "Epoch [593/1200], Loss: 0.2952 , Accuracy : 80.00%\n",
      "Epoch [594/1200], Loss: 0.0809 , Accuracy : 77.50%\n",
      "Epoch [595/1200], Loss: 0.0396 , Accuracy : 82.50%\n",
      "Epoch [596/1200], Loss: 1.1451 , Accuracy : 80.00%\n",
      "Epoch [597/1200], Loss: 0.2104 , Accuracy : 77.50%\n",
      "Epoch [598/1200], Loss: 0.1232 , Accuracy : 82.50%\n",
      "Epoch [599/1200], Loss: 0.4340 , Accuracy : 77.50%\n",
      "Epoch [600/1200], Loss: 0.2573 , Accuracy : 90.00%\n",
      "Epoch [601/1200], Loss: 0.0752 , Accuracy : 87.50%\n",
      "Epoch [602/1200], Loss: 0.4365 , Accuracy : 87.50%\n",
      "Epoch [603/1200], Loss: 0.5591 , Accuracy : 82.50%\n",
      "Epoch [604/1200], Loss: 0.3355 , Accuracy : 87.50%\n",
      "Epoch [605/1200], Loss: 0.0904 , Accuracy : 82.50%\n",
      "Epoch [606/1200], Loss: 0.0371 , Accuracy : 85.00%\n",
      "Epoch [607/1200], Loss: 0.2100 , Accuracy : 82.50%\n",
      "Epoch [608/1200], Loss: 0.0913 , Accuracy : 85.00%\n",
      "Epoch [609/1200], Loss: 0.2043 , Accuracy : 82.50%\n",
      "Epoch [610/1200], Loss: 0.7708 , Accuracy : 92.50%\n",
      "Epoch [611/1200], Loss: 0.0567 , Accuracy : 87.50%\n",
      "Epoch [612/1200], Loss: 0.5135 , Accuracy : 75.00%\n",
      "Epoch [613/1200], Loss: 0.2173 , Accuracy : 87.50%\n",
      "Epoch [614/1200], Loss: 0.1901 , Accuracy : 87.50%\n",
      "Epoch [615/1200], Loss: 0.3666 , Accuracy : 85.00%\n",
      "Epoch [616/1200], Loss: 0.2007 , Accuracy : 87.50%\n",
      "Epoch [617/1200], Loss: 1.5733 , Accuracy : 80.00%\n",
      "Epoch [618/1200], Loss: 0.9421 , Accuracy : 80.00%\n",
      "Epoch [619/1200], Loss: 0.0144 , Accuracy : 90.00%\n",
      "Epoch [620/1200], Loss: 0.9170 , Accuracy : 82.50%\n",
      "Epoch [621/1200], Loss: 0.0278 , Accuracy : 82.50%\n",
      "Epoch [622/1200], Loss: 0.0955 , Accuracy : 82.50%\n",
      "Epoch [623/1200], Loss: 0.5346 , Accuracy : 75.00%\n",
      "Epoch [624/1200], Loss: 0.0466 , Accuracy : 82.50%\n",
      "Epoch [625/1200], Loss: 0.1068 , Accuracy : 85.00%\n",
      "Epoch [626/1200], Loss: 0.0561 , Accuracy : 85.00%\n",
      "Epoch [627/1200], Loss: 0.8672 , Accuracy : 70.00%\n",
      "Epoch [628/1200], Loss: 0.0139 , Accuracy : 85.00%\n",
      "Epoch [629/1200], Loss: 0.0589 , Accuracy : 82.50%\n",
      "Epoch [630/1200], Loss: 0.3078 , Accuracy : 82.50%\n",
      "Epoch [631/1200], Loss: 0.0347 , Accuracy : 80.00%\n",
      "Epoch [632/1200], Loss: 0.7369 , Accuracy : 82.50%\n",
      "Epoch [633/1200], Loss: 0.6035 , Accuracy : 77.50%\n",
      "Epoch [634/1200], Loss: 0.8009 , Accuracy : 70.00%\n",
      "Epoch [635/1200], Loss: 0.0472 , Accuracy : 87.50%\n",
      "Epoch [636/1200], Loss: 0.2296 , Accuracy : 90.00%\n",
      "Epoch [637/1200], Loss: 0.0778 , Accuracy : 85.00%\n",
      "Epoch [638/1200], Loss: 0.3696 , Accuracy : 77.50%\n",
      "Epoch [639/1200], Loss: 0.1861 , Accuracy : 80.00%\n",
      "Epoch [640/1200], Loss: 0.2328 , Accuracy : 80.00%\n",
      "Epoch [641/1200], Loss: 0.7393 , Accuracy : 80.00%\n",
      "Epoch [642/1200], Loss: 0.0171 , Accuracy : 82.50%\n",
      "Epoch [643/1200], Loss: 0.1285 , Accuracy : 85.00%\n",
      "Epoch [644/1200], Loss: 0.3872 , Accuracy : 82.50%\n",
      "Epoch [645/1200], Loss: 0.9008 , Accuracy : 90.00%\n",
      "Epoch [646/1200], Loss: 0.6326 , Accuracy : 77.50%\n",
      "Epoch [647/1200], Loss: 0.1192 , Accuracy : 87.50%\n",
      "Epoch [648/1200], Loss: 1.4556 , Accuracy : 85.00%\n",
      "Epoch [649/1200], Loss: 0.0685 , Accuracy : 85.00%\n",
      "Epoch [650/1200], Loss: 0.4000 , Accuracy : 82.50%\n",
      "Epoch [651/1200], Loss: 1.1165 , Accuracy : 72.50%\n",
      "Epoch [652/1200], Loss: 1.9173 , Accuracy : 65.00%\n",
      "Epoch [653/1200], Loss: 0.6562 , Accuracy : 67.50%\n",
      "Epoch [654/1200], Loss: 0.3441 , Accuracy : 75.00%\n",
      "Epoch [655/1200], Loss: 0.0110 , Accuracy : 82.50%\n",
      "Epoch [656/1200], Loss: 0.2795 , Accuracy : 87.50%\n",
      "Epoch [657/1200], Loss: 0.7773 , Accuracy : 82.50%\n",
      "Epoch [658/1200], Loss: 0.5675 , Accuracy : 77.50%\n",
      "Epoch [659/1200], Loss: 1.5396 , Accuracy : 82.50%\n",
      "Epoch [660/1200], Loss: 0.6441 , Accuracy : 80.00%\n",
      "Epoch [661/1200], Loss: 0.2701 , Accuracy : 87.50%\n",
      "Epoch [662/1200], Loss: 0.3893 , Accuracy : 90.00%\n",
      "Epoch [663/1200], Loss: 0.0858 , Accuracy : 90.00%\n",
      "Epoch [664/1200], Loss: 0.4867 , Accuracy : 82.50%\n",
      "Epoch [665/1200], Loss: 0.4905 , Accuracy : 77.50%\n",
      "Epoch [666/1200], Loss: 0.4794 , Accuracy : 77.50%\n",
      "Epoch [667/1200], Loss: 0.4771 , Accuracy : 82.50%\n",
      "Epoch [668/1200], Loss: 0.4131 , Accuracy : 85.00%\n",
      "Epoch [669/1200], Loss: 0.0903 , Accuracy : 75.00%\n",
      "Epoch [670/1200], Loss: 0.5642 , Accuracy : 77.50%\n",
      "Epoch [671/1200], Loss: 0.0362 , Accuracy : 85.00%\n",
      "Epoch [672/1200], Loss: 0.0844 , Accuracy : 90.00%\n",
      "Epoch [673/1200], Loss: 0.5909 , Accuracy : 80.00%\n",
      "Epoch [674/1200], Loss: 0.3389 , Accuracy : 85.00%\n",
      "Epoch [675/1200], Loss: 0.1030 , Accuracy : 87.50%\n",
      "Epoch [676/1200], Loss: 0.0181 , Accuracy : 82.50%\n",
      "Epoch [677/1200], Loss: 0.0912 , Accuracy : 85.00%\n",
      "Epoch [678/1200], Loss: 0.6077 , Accuracy : 82.50%\n",
      "Epoch [679/1200], Loss: 0.2778 , Accuracy : 87.50%\n",
      "Epoch [680/1200], Loss: 0.7034 , Accuracy : 85.00%\n",
      "Epoch [681/1200], Loss: 0.1487 , Accuracy : 85.00%\n",
      "Epoch [682/1200], Loss: 0.8739 , Accuracy : 85.00%\n",
      "Epoch [683/1200], Loss: 0.4056 , Accuracy : 80.00%\n",
      "Epoch [684/1200], Loss: 0.0460 , Accuracy : 82.50%\n",
      "Epoch [685/1200], Loss: 0.5111 , Accuracy : 87.50%\n",
      "Epoch [686/1200], Loss: 0.0532 , Accuracy : 82.50%\n",
      "Epoch [687/1200], Loss: 0.1236 , Accuracy : 85.00%\n",
      "Epoch [688/1200], Loss: 0.6008 , Accuracy : 70.00%\n",
      "Epoch [689/1200], Loss: 1.1445 , Accuracy : 62.50%\n",
      "Epoch [690/1200], Loss: 0.0145 , Accuracy : 65.00%\n",
      "Epoch [691/1200], Loss: 2.9283 , Accuracy : 65.00%\n",
      "Epoch [692/1200], Loss: 1.2881 , Accuracy : 47.50%\n",
      "Epoch [693/1200], Loss: 1.6801 , Accuracy : 45.00%\n",
      "Epoch [694/1200], Loss: 0.2429 , Accuracy : 75.00%\n",
      "Epoch [695/1200], Loss: 0.3070 , Accuracy : 75.00%\n",
      "Epoch [696/1200], Loss: 0.1861 , Accuracy : 77.50%\n",
      "Epoch [697/1200], Loss: 0.4673 , Accuracy : 80.00%\n",
      "Epoch [698/1200], Loss: 0.6361 , Accuracy : 92.50%\n",
      "Epoch [699/1200], Loss: 0.2372 , Accuracy : 80.00%\n",
      "Epoch [700/1200], Loss: 0.4025 , Accuracy : 82.50%\n",
      "Epoch [701/1200], Loss: 0.0088 , Accuracy : 80.00%\n",
      "Epoch [702/1200], Loss: 0.2384 , Accuracy : 87.50%\n",
      "Epoch [703/1200], Loss: 0.1118 , Accuracy : 85.00%\n",
      "Epoch [704/1200], Loss: 0.8297 , Accuracy : 82.50%\n",
      "Epoch [705/1200], Loss: 0.1479 , Accuracy : 87.50%\n",
      "Epoch [706/1200], Loss: 0.0668 , Accuracy : 92.50%\n",
      "Epoch [707/1200], Loss: 0.8889 , Accuracy : 80.00%\n",
      "Epoch [708/1200], Loss: 0.0069 , Accuracy : 87.50%\n",
      "Epoch [709/1200], Loss: 0.5967 , Accuracy : 95.00%\n",
      "Epoch [710/1200], Loss: 0.0983 , Accuracy : 92.50%\n",
      "Epoch [711/1200], Loss: 0.1477 , Accuracy : 85.00%\n",
      "Epoch [712/1200], Loss: 0.2300 , Accuracy : 85.00%\n",
      "Epoch [713/1200], Loss: 0.4068 , Accuracy : 87.50%\n",
      "Epoch [714/1200], Loss: 0.3643 , Accuracy : 97.50%\n",
      "Epoch [715/1200], Loss: 1.2702 , Accuracy : 90.00%\n",
      "Epoch [716/1200], Loss: 0.1051 , Accuracy : 97.50%\n",
      "Epoch [717/1200], Loss: 1.3148 , Accuracy : 92.50%\n",
      "Epoch [718/1200], Loss: 0.0046 , Accuracy : 87.50%\n",
      "Epoch [719/1200], Loss: 0.4580 , Accuracy : 87.50%\n",
      "Epoch [720/1200], Loss: 0.0396 , Accuracy : 90.00%\n",
      "Epoch [721/1200], Loss: 0.0049 , Accuracy : 90.00%\n",
      "Epoch [722/1200], Loss: 0.1184 , Accuracy : 90.00%\n",
      "Epoch [723/1200], Loss: 0.0235 , Accuracy : 85.00%\n",
      "Epoch [724/1200], Loss: 0.0151 , Accuracy : 90.00%\n",
      "Epoch [725/1200], Loss: 0.4124 , Accuracy : 82.50%\n",
      "Epoch [726/1200], Loss: 0.0490 , Accuracy : 90.00%\n",
      "Epoch [727/1200], Loss: 0.0602 , Accuracy : 90.00%\n",
      "Epoch [728/1200], Loss: 0.3874 , Accuracy : 90.00%\n",
      "Epoch [729/1200], Loss: 0.2685 , Accuracy : 90.00%\n",
      "Epoch [730/1200], Loss: 0.1700 , Accuracy : 95.00%\n",
      "Epoch [731/1200], Loss: 0.8744 , Accuracy : 82.50%\n",
      "Epoch [732/1200], Loss: 0.1838 , Accuracy : 90.00%\n",
      "Epoch [733/1200], Loss: 0.4223 , Accuracy : 92.50%\n",
      "Epoch [734/1200], Loss: 0.4106 , Accuracy : 87.50%\n",
      "Epoch [735/1200], Loss: 0.1820 , Accuracy : 92.50%\n",
      "Epoch [736/1200], Loss: 0.0254 , Accuracy : 85.00%\n",
      "Epoch [737/1200], Loss: 0.0925 , Accuracy : 92.50%\n",
      "Epoch [738/1200], Loss: 0.5339 , Accuracy : 85.00%\n",
      "Epoch [739/1200], Loss: 0.0066 , Accuracy : 92.50%\n",
      "Epoch [740/1200], Loss: 0.0497 , Accuracy : 95.00%\n",
      "Epoch [741/1200], Loss: 0.0877 , Accuracy : 87.50%\n",
      "Epoch [742/1200], Loss: 0.2459 , Accuracy : 92.50%\n",
      "Epoch [743/1200], Loss: 0.4340 , Accuracy : 95.00%\n",
      "Epoch [744/1200], Loss: 0.4117 , Accuracy : 92.50%\n",
      "Epoch [745/1200], Loss: 0.7606 , Accuracy : 87.50%\n",
      "Epoch [746/1200], Loss: 0.0322 , Accuracy : 90.00%\n",
      "Epoch [747/1200], Loss: 0.0653 , Accuracy : 92.50%\n",
      "Epoch [748/1200], Loss: 0.5214 , Accuracy : 90.00%\n",
      "Epoch [749/1200], Loss: 0.0110 , Accuracy : 87.50%\n",
      "Epoch [750/1200], Loss: 0.4579 , Accuracy : 80.00%\n",
      "Epoch [751/1200], Loss: 0.5571 , Accuracy : 90.00%\n",
      "Epoch [752/1200], Loss: 0.4757 , Accuracy : 82.50%\n",
      "Epoch [753/1200], Loss: 0.4748 , Accuracy : 92.50%\n",
      "Epoch [754/1200], Loss: 0.3499 , Accuracy : 95.00%\n",
      "Epoch [755/1200], Loss: 0.4896 , Accuracy : 92.50%\n",
      "Epoch [756/1200], Loss: 0.0319 , Accuracy : 90.00%\n",
      "Epoch [757/1200], Loss: 0.1415 , Accuracy : 97.50%\n",
      "Epoch [758/1200], Loss: 0.0517 , Accuracy : 85.00%\n",
      "Epoch [759/1200], Loss: 0.0068 , Accuracy : 87.50%\n",
      "Epoch [760/1200], Loss: 0.1559 , Accuracy : 82.50%\n",
      "Epoch [761/1200], Loss: 0.4125 , Accuracy : 80.00%\n",
      "Epoch [762/1200], Loss: 0.2734 , Accuracy : 77.50%\n",
      "Epoch [763/1200], Loss: 0.7046 , Accuracy : 87.50%\n",
      "Epoch [764/1200], Loss: 0.3935 , Accuracy : 92.50%\n",
      "Epoch [765/1200], Loss: 0.9379 , Accuracy : 87.50%\n",
      "Epoch [766/1200], Loss: 0.9923 , Accuracy : 65.00%\n",
      "Epoch [767/1200], Loss: 0.7960 , Accuracy : 62.50%\n",
      "Epoch [768/1200], Loss: 0.5006 , Accuracy : 77.50%\n",
      "Epoch [769/1200], Loss: 0.9735 , Accuracy : 67.50%\n",
      "Epoch [770/1200], Loss: 0.9786 , Accuracy : 65.00%\n",
      "Epoch [771/1200], Loss: 0.3165 , Accuracy : 72.50%\n",
      "Epoch [772/1200], Loss: 1.6221 , Accuracy : 82.50%\n",
      "Epoch [773/1200], Loss: 1.0963 , Accuracy : 77.50%\n",
      "Epoch [774/1200], Loss: 0.1741 , Accuracy : 67.50%\n",
      "Epoch [775/1200], Loss: 0.8285 , Accuracy : 65.00%\n",
      "Epoch [776/1200], Loss: 0.8030 , Accuracy : 72.50%\n",
      "Epoch [777/1200], Loss: 0.9546 , Accuracy : 80.00%\n",
      "Epoch [778/1200], Loss: 0.7721 , Accuracy : 77.50%\n",
      "Epoch [779/1200], Loss: 0.3086 , Accuracy : 70.00%\n",
      "Epoch [780/1200], Loss: 0.5007 , Accuracy : 77.50%\n",
      "Epoch [781/1200], Loss: 0.7599 , Accuracy : 82.50%\n",
      "Epoch [782/1200], Loss: 0.3102 , Accuracy : 75.00%\n",
      "Epoch [783/1200], Loss: 0.4617 , Accuracy : 75.00%\n",
      "Epoch [784/1200], Loss: 0.3474 , Accuracy : 90.00%\n",
      "Epoch [785/1200], Loss: 0.0417 , Accuracy : 90.00%\n",
      "Epoch [786/1200], Loss: 0.0963 , Accuracy : 85.00%\n",
      "Epoch [787/1200], Loss: 0.2579 , Accuracy : 87.50%\n",
      "Epoch [788/1200], Loss: 0.3348 , Accuracy : 82.50%\n",
      "Epoch [789/1200], Loss: 0.1525 , Accuracy : 82.50%\n",
      "Epoch [790/1200], Loss: 0.3901 , Accuracy : 92.50%\n",
      "Epoch [791/1200], Loss: 0.3265 , Accuracy : 95.00%\n",
      "Epoch [792/1200], Loss: 0.3674 , Accuracy : 87.50%\n",
      "Epoch [793/1200], Loss: 0.1901 , Accuracy : 87.50%\n",
      "Epoch [794/1200], Loss: 0.0325 , Accuracy : 82.50%\n",
      "Epoch [795/1200], Loss: 0.0903 , Accuracy : 80.00%\n",
      "Epoch [796/1200], Loss: 0.0648 , Accuracy : 90.00%\n",
      "Epoch [797/1200], Loss: 0.1783 , Accuracy : 92.50%\n",
      "Epoch [798/1200], Loss: 0.1768 , Accuracy : 95.00%\n",
      "Epoch [799/1200], Loss: 0.5521 , Accuracy : 92.50%\n",
      "Epoch [800/1200], Loss: 0.4194 , Accuracy : 90.00%\n",
      "Epoch [801/1200], Loss: 0.6052 , Accuracy : 90.00%\n",
      "Epoch [802/1200], Loss: 0.6555 , Accuracy : 80.00%\n",
      "Epoch [803/1200], Loss: 0.1110 , Accuracy : 90.00%\n",
      "Epoch [804/1200], Loss: 0.5962 , Accuracy : 92.50%\n",
      "Epoch [805/1200], Loss: 0.5570 , Accuracy : 92.50%\n",
      "Epoch [806/1200], Loss: 0.5445 , Accuracy : 85.00%\n",
      "Epoch [807/1200], Loss: 0.6469 , Accuracy : 77.50%\n",
      "Epoch [808/1200], Loss: 0.2828 , Accuracy : 72.50%\n",
      "Epoch [809/1200], Loss: 1.1682 , Accuracy : 77.50%\n",
      "Epoch [810/1200], Loss: 0.0659 , Accuracy : 90.00%\n",
      "Epoch [811/1200], Loss: 1.2553 , Accuracy : 90.00%\n",
      "Epoch [812/1200], Loss: 0.5786 , Accuracy : 82.50%\n",
      "Epoch [813/1200], Loss: 0.6092 , Accuracy : 77.50%\n",
      "Epoch [814/1200], Loss: 0.4042 , Accuracy : 82.50%\n",
      "Epoch [815/1200], Loss: 0.0641 , Accuracy : 85.00%\n",
      "Epoch [816/1200], Loss: 0.2642 , Accuracy : 92.50%\n",
      "Epoch [817/1200], Loss: 0.2124 , Accuracy : 97.50%\n",
      "Epoch [818/1200], Loss: 0.4537 , Accuracy : 82.50%\n",
      "Epoch [819/1200], Loss: 0.1423 , Accuracy : 82.50%\n",
      "Epoch [820/1200], Loss: 0.1949 , Accuracy : 65.00%\n",
      "Epoch [821/1200], Loss: 1.4403 , Accuracy : 60.00%\n",
      "Epoch [822/1200], Loss: 0.1172 , Accuracy : 77.50%\n",
      "Epoch [823/1200], Loss: 1.3944 , Accuracy : 65.00%\n",
      "Epoch [824/1200], Loss: 0.0940 , Accuracy : 80.00%\n",
      "Epoch [825/1200], Loss: 0.1551 , Accuracy : 77.50%\n",
      "Epoch [826/1200], Loss: 0.0610 , Accuracy : 87.50%\n",
      "Epoch [827/1200], Loss: 0.4232 , Accuracy : 90.00%\n",
      "Epoch [828/1200], Loss: 0.4763 , Accuracy : 92.50%\n",
      "Epoch [829/1200], Loss: 0.3516 , Accuracy : 85.00%\n",
      "Epoch [830/1200], Loss: 0.1119 , Accuracy : 95.00%\n",
      "Epoch [831/1200], Loss: 0.1771 , Accuracy : 95.00%\n",
      "Epoch [832/1200], Loss: 0.4591 , Accuracy : 82.50%\n",
      "Epoch [833/1200], Loss: 0.0114 , Accuracy : 90.00%\n",
      "Epoch [834/1200], Loss: 0.3806 , Accuracy : 85.00%\n",
      "Epoch [835/1200], Loss: 0.4123 , Accuracy : 75.00%\n",
      "Epoch [836/1200], Loss: 0.1389 , Accuracy : 85.00%\n",
      "Epoch [837/1200], Loss: 0.8530 , Accuracy : 80.00%\n",
      "Epoch [838/1200], Loss: 0.0303 , Accuracy : 75.00%\n",
      "Epoch [839/1200], Loss: 0.4987 , Accuracy : 87.50%\n",
      "Epoch [840/1200], Loss: 0.1797 , Accuracy : 90.00%\n",
      "Epoch [841/1200], Loss: 0.2889 , Accuracy : 80.00%\n",
      "Epoch [842/1200], Loss: 0.0451 , Accuracy : 87.50%\n",
      "Epoch [843/1200], Loss: 0.5322 , Accuracy : 92.50%\n",
      "Epoch [844/1200], Loss: 0.4754 , Accuracy : 92.50%\n",
      "Epoch [845/1200], Loss: 0.9270 , Accuracy : 85.00%\n",
      "Epoch [846/1200], Loss: 0.0567 , Accuracy : 97.50%\n",
      "Epoch [847/1200], Loss: 0.2497 , Accuracy : 95.00%\n",
      "Epoch [848/1200], Loss: 0.0149 , Accuracy : 97.50%\n",
      "Epoch [849/1200], Loss: 0.0284 , Accuracy : 87.50%\n",
      "Epoch [850/1200], Loss: 0.2211 , Accuracy : 85.00%\n",
      "Epoch [851/1200], Loss: 0.5788 , Accuracy : 90.00%\n",
      "Epoch [852/1200], Loss: 0.0342 , Accuracy : 90.00%\n",
      "Epoch [853/1200], Loss: 0.3510 , Accuracy : 97.50%\n",
      "Epoch [854/1200], Loss: 0.2606 , Accuracy : 90.00%\n",
      "Epoch [855/1200], Loss: 0.3348 , Accuracy : 90.00%\n",
      "Epoch [856/1200], Loss: 0.5273 , Accuracy : 85.00%\n",
      "Epoch [857/1200], Loss: 0.2454 , Accuracy : 92.50%\n",
      "Epoch [858/1200], Loss: 0.7951 , Accuracy : 80.00%\n",
      "Epoch [859/1200], Loss: 0.1433 , Accuracy : 85.00%\n",
      "Epoch [860/1200], Loss: 0.6756 , Accuracy : 82.50%\n",
      "Epoch [861/1200], Loss: 0.3987 , Accuracy : 65.00%\n",
      "Epoch [862/1200], Loss: 0.0086 , Accuracy : 77.50%\n",
      "Epoch [863/1200], Loss: 0.4851 , Accuracy : 80.00%\n",
      "Epoch [864/1200], Loss: 0.0640 , Accuracy : 80.00%\n",
      "Epoch [865/1200], Loss: 0.0085 , Accuracy : 87.50%\n",
      "Epoch [866/1200], Loss: 0.0288 , Accuracy : 97.50%\n",
      "Epoch [867/1200], Loss: 0.7176 , Accuracy : 92.50%\n",
      "Epoch [868/1200], Loss: 0.0614 , Accuracy : 95.00%\n",
      "Epoch [869/1200], Loss: 1.1462 , Accuracy : 87.50%\n",
      "Epoch [870/1200], Loss: 0.0712 , Accuracy : 80.00%\n",
      "Epoch [871/1200], Loss: 0.1479 , Accuracy : 87.50%\n",
      "Epoch [872/1200], Loss: 0.2340 , Accuracy : 90.00%\n",
      "Epoch [873/1200], Loss: 0.0025 , Accuracy : 95.00%\n",
      "Epoch [874/1200], Loss: 0.2734 , Accuracy : 92.50%\n",
      "Epoch [875/1200], Loss: 0.0671 , Accuracy : 97.50%\n",
      "Epoch [876/1200], Loss: 0.0960 , Accuracy : 90.00%\n",
      "Epoch [877/1200], Loss: 0.0733 , Accuracy : 85.00%\n",
      "Epoch [878/1200], Loss: 0.5236 , Accuracy : 87.50%\n",
      "Epoch [879/1200], Loss: 1.1871 , Accuracy : 87.50%\n",
      "Epoch [880/1200], Loss: 2.2978 , Accuracy : 85.00%\n",
      "Epoch [881/1200], Loss: 0.4558 , Accuracy : 80.00%\n",
      "Epoch [882/1200], Loss: 0.2507 , Accuracy : 72.50%\n",
      "Epoch [883/1200], Loss: 0.0222 , Accuracy : 92.50%\n",
      "Epoch [884/1200], Loss: 0.0538 , Accuracy : 92.50%\n",
      "Epoch [885/1200], Loss: 0.4879 , Accuracy : 90.00%\n",
      "Epoch [886/1200], Loss: 0.2444 , Accuracy : 92.50%\n",
      "Epoch [887/1200], Loss: 0.1277 , Accuracy : 90.00%\n",
      "Epoch [888/1200], Loss: 0.5630 , Accuracy : 92.50%\n",
      "Epoch [889/1200], Loss: 0.0364 , Accuracy : 87.50%\n",
      "Epoch [890/1200], Loss: 0.1151 , Accuracy : 85.00%\n",
      "Epoch [891/1200], Loss: 0.2554 , Accuracy : 90.00%\n",
      "Epoch [892/1200], Loss: 1.1495 , Accuracy : 85.00%\n",
      "Epoch [893/1200], Loss: 0.0225 , Accuracy : 92.50%\n",
      "Epoch [894/1200], Loss: 0.0318 , Accuracy : 87.50%\n",
      "Epoch [895/1200], Loss: 0.0568 , Accuracy : 90.00%\n",
      "Epoch [896/1200], Loss: 0.1074 , Accuracy : 82.50%\n",
      "Epoch [897/1200], Loss: 0.0449 , Accuracy : 95.00%\n",
      "Epoch [898/1200], Loss: 0.1422 , Accuracy : 87.50%\n",
      "Epoch [899/1200], Loss: 0.2837 , Accuracy : 85.00%\n",
      "Epoch [900/1200], Loss: 0.1645 , Accuracy : 85.00%\n",
      "Epoch [901/1200], Loss: 0.0067 , Accuracy : 87.50%\n",
      "Epoch [902/1200], Loss: 0.7869 , Accuracy : 90.00%\n",
      "Epoch [903/1200], Loss: 0.5600 , Accuracy : 90.00%\n",
      "Epoch [904/1200], Loss: 0.3765 , Accuracy : 90.00%\n",
      "Epoch [905/1200], Loss: 0.1523 , Accuracy : 92.50%\n",
      "Epoch [906/1200], Loss: 0.8792 , Accuracy : 90.00%\n",
      "Epoch [907/1200], Loss: 0.0086 , Accuracy : 97.50%\n",
      "Epoch [908/1200], Loss: 0.0984 , Accuracy : 97.50%\n",
      "Epoch [909/1200], Loss: 0.4767 , Accuracy : 92.50%\n",
      "Epoch [910/1200], Loss: 0.0743 , Accuracy : 90.00%\n",
      "Epoch [911/1200], Loss: 0.0609 , Accuracy : 87.50%\n",
      "Epoch [912/1200], Loss: 0.4740 , Accuracy : 87.50%\n",
      "Epoch [913/1200], Loss: 0.5950 , Accuracy : 87.50%\n",
      "Epoch [914/1200], Loss: 0.1601 , Accuracy : 97.50%\n",
      "Epoch [915/1200], Loss: 0.0243 , Accuracy : 87.50%\n",
      "Epoch [916/1200], Loss: 0.7489 , Accuracy : 82.50%\n",
      "Epoch [917/1200], Loss: 0.0394 , Accuracy : 87.50%\n",
      "Epoch [918/1200], Loss: 0.0185 , Accuracy : 85.00%\n",
      "Epoch [919/1200], Loss: 0.0096 , Accuracy : 95.00%\n",
      "Epoch [920/1200], Loss: 0.0096 , Accuracy : 87.50%\n",
      "Epoch [921/1200], Loss: 0.0574 , Accuracy : 85.00%\n",
      "Epoch [922/1200], Loss: 1.2027 , Accuracy : 75.00%\n",
      "Epoch [923/1200], Loss: 0.6989 , Accuracy : 85.00%\n",
      "Epoch [924/1200], Loss: 1.3729 , Accuracy : 77.50%\n",
      "Epoch [925/1200], Loss: 0.1814 , Accuracy : 90.00%\n",
      "Epoch [926/1200], Loss: 1.4421 , Accuracy : 90.00%\n",
      "Epoch [927/1200], Loss: 0.0606 , Accuracy : 85.00%\n",
      "Epoch [928/1200], Loss: 0.0106 , Accuracy : 85.00%\n",
      "Epoch [929/1200], Loss: 0.0253 , Accuracy : 92.50%\n",
      "Epoch [930/1200], Loss: 0.2533 , Accuracy : 90.00%\n",
      "Epoch [931/1200], Loss: 0.6317 , Accuracy : 92.50%\n",
      "Epoch [932/1200], Loss: 0.0262 , Accuracy : 87.50%\n",
      "Epoch [933/1200], Loss: 0.6294 , Accuracy : 90.00%\n",
      "Epoch [934/1200], Loss: 0.0066 , Accuracy : 87.50%\n",
      "Epoch [935/1200], Loss: 0.1733 , Accuracy : 87.50%\n",
      "Epoch [936/1200], Loss: 0.0605 , Accuracy : 95.00%\n",
      "Epoch [937/1200], Loss: 0.0044 , Accuracy : 90.00%\n",
      "Epoch [938/1200], Loss: 0.0195 , Accuracy : 97.50%\n",
      "Epoch [939/1200], Loss: 0.0366 , Accuracy : 92.50%\n",
      "Epoch [940/1200], Loss: 0.1621 , Accuracy : 90.00%\n",
      "Epoch [941/1200], Loss: 0.6875 , Accuracy : 90.00%\n",
      "Epoch [942/1200], Loss: 0.4225 , Accuracy : 87.50%\n",
      "Epoch [943/1200], Loss: 0.1931 , Accuracy : 95.00%\n",
      "Epoch [944/1200], Loss: 0.3237 , Accuracy : 82.50%\n",
      "Epoch [945/1200], Loss: 0.1422 , Accuracy : 90.00%\n",
      "Epoch [946/1200], Loss: 0.2469 , Accuracy : 90.00%\n",
      "Epoch [947/1200], Loss: 0.2941 , Accuracy : 90.00%\n",
      "Epoch [948/1200], Loss: 0.2243 , Accuracy : 92.50%\n",
      "Epoch [949/1200], Loss: 0.0213 , Accuracy : 87.50%\n",
      "Epoch [950/1200], Loss: 1.1330 , Accuracy : 85.00%\n",
      "Epoch [951/1200], Loss: 0.1745 , Accuracy : 87.50%\n",
      "Epoch [952/1200], Loss: 0.3359 , Accuracy : 90.00%\n",
      "Epoch [953/1200], Loss: 0.2783 , Accuracy : 85.00%\n",
      "Epoch [954/1200], Loss: 0.0493 , Accuracy : 92.50%\n",
      "Epoch [955/1200], Loss: 0.1650 , Accuracy : 95.00%\n",
      "Epoch [956/1200], Loss: 0.1056 , Accuracy : 92.50%\n",
      "Epoch [957/1200], Loss: 0.2655 , Accuracy : 95.00%\n",
      "Epoch [958/1200], Loss: 0.8559 , Accuracy : 95.00%\n",
      "Epoch [959/1200], Loss: 0.3009 , Accuracy : 90.00%\n",
      "Epoch [960/1200], Loss: 0.0537 , Accuracy : 87.50%\n",
      "Epoch [961/1200], Loss: 0.1064 , Accuracy : 95.00%\n",
      "Epoch [962/1200], Loss: 0.0310 , Accuracy : 90.00%\n",
      "Epoch [963/1200], Loss: 0.0779 , Accuracy : 92.50%\n",
      "Epoch [964/1200], Loss: 0.4780 , Accuracy : 92.50%\n",
      "Epoch [965/1200], Loss: 0.3881 , Accuracy : 90.00%\n",
      "Epoch [966/1200], Loss: 0.0061 , Accuracy : 90.00%\n",
      "Epoch [967/1200], Loss: 0.5287 , Accuracy : 87.50%\n",
      "Epoch [968/1200], Loss: 0.0773 , Accuracy : 97.50%\n",
      "Epoch [969/1200], Loss: 0.0690 , Accuracy : 95.00%\n",
      "Epoch [970/1200], Loss: 0.0370 , Accuracy : 100.00%\n",
      "Epoch [971/1200], Loss: 0.0549 , Accuracy : 100.00%\n",
      "Epoch [972/1200], Loss: 0.2306 , Accuracy : 92.50%\n",
      "Epoch [973/1200], Loss: 0.0681 , Accuracy : 95.00%\n",
      "Epoch [974/1200], Loss: 0.1332 , Accuracy : 92.50%\n",
      "Epoch [975/1200], Loss: 0.0073 , Accuracy : 95.00%\n",
      "Epoch [976/1200], Loss: 0.0429 , Accuracy : 92.50%\n",
      "Epoch [977/1200], Loss: 0.0250 , Accuracy : 100.00%\n",
      "Epoch [978/1200], Loss: 0.0172 , Accuracy : 95.00%\n",
      "Epoch [979/1200], Loss: 0.2263 , Accuracy : 100.00%\n",
      "Epoch [980/1200], Loss: 0.0024 , Accuracy : 92.50%\n",
      "Epoch [981/1200], Loss: 0.0165 , Accuracy : 100.00%\n",
      "Epoch [982/1200], Loss: 0.0041 , Accuracy : 97.50%\n",
      "Epoch [983/1200], Loss: 0.4684 , Accuracy : 97.50%\n",
      "Epoch [984/1200], Loss: 0.5756 , Accuracy : 97.50%\n",
      "Epoch [985/1200], Loss: 0.0891 , Accuracy : 100.00%\n",
      "Epoch [986/1200], Loss: 0.8694 , Accuracy : 95.00%\n",
      "Epoch [987/1200], Loss: 0.0042 , Accuracy : 97.50%\n",
      "Epoch [988/1200], Loss: 0.2593 , Accuracy : 90.00%\n",
      "Epoch [989/1200], Loss: 0.0306 , Accuracy : 95.00%\n",
      "Epoch [990/1200], Loss: 0.0405 , Accuracy : 95.00%\n",
      "Epoch [991/1200], Loss: 0.0706 , Accuracy : 92.50%\n",
      "Epoch [992/1200], Loss: 0.0215 , Accuracy : 95.00%\n",
      "Epoch [993/1200], Loss: 0.0132 , Accuracy : 90.00%\n",
      "Epoch [994/1200], Loss: 0.1478 , Accuracy : 92.50%\n",
      "Epoch [995/1200], Loss: 0.1049 , Accuracy : 90.00%\n",
      "Epoch [996/1200], Loss: 0.0224 , Accuracy : 92.50%\n",
      "Epoch [997/1200], Loss: 0.0282 , Accuracy : 92.50%\n",
      "Epoch [998/1200], Loss: 1.2770 , Accuracy : 85.00%\n",
      "Epoch [999/1200], Loss: 0.3321 , Accuracy : 85.00%\n",
      "Epoch [1000/1200], Loss: 0.0337 , Accuracy : 87.50%\n",
      "Epoch [1001/1200], Loss: 0.0071 , Accuracy : 95.00%\n",
      "Epoch [1002/1200], Loss: 0.0332 , Accuracy : 95.00%\n",
      "Epoch [1003/1200], Loss: 0.0061 , Accuracy : 90.00%\n",
      "Epoch [1004/1200], Loss: 0.0159 , Accuracy : 92.50%\n",
      "Epoch [1005/1200], Loss: 0.4585 , Accuracy : 95.00%\n",
      "Epoch [1006/1200], Loss: 0.0461 , Accuracy : 92.50%\n",
      "Epoch [1007/1200], Loss: 0.0112 , Accuracy : 87.50%\n",
      "Epoch [1008/1200], Loss: 0.4894 , Accuracy : 92.50%\n",
      "Epoch [1009/1200], Loss: 0.0533 , Accuracy : 90.00%\n",
      "Epoch [1010/1200], Loss: 1.1465 , Accuracy : 70.00%\n",
      "Epoch [1011/1200], Loss: 0.0058 , Accuracy : 70.00%\n",
      "Epoch [1012/1200], Loss: 0.4052 , Accuracy : 85.00%\n",
      "Epoch [1013/1200], Loss: 0.3263 , Accuracy : 82.50%\n",
      "Epoch [1014/1200], Loss: 1.7844 , Accuracy : 77.50%\n",
      "Epoch [1015/1200], Loss: 0.1308 , Accuracy : 97.50%\n",
      "Epoch [1016/1200], Loss: 0.0239 , Accuracy : 90.00%\n",
      "Epoch [1017/1200], Loss: 1.3164 , Accuracy : 82.50%\n",
      "Epoch [1018/1200], Loss: 0.3081 , Accuracy : 80.00%\n",
      "Epoch [1019/1200], Loss: 0.1180 , Accuracy : 90.00%\n",
      "Epoch [1020/1200], Loss: 0.0089 , Accuracy : 95.00%\n",
      "Epoch [1021/1200], Loss: 0.0087 , Accuracy : 92.50%\n",
      "Epoch [1022/1200], Loss: 0.1946 , Accuracy : 90.00%\n",
      "Epoch [1023/1200], Loss: 0.2755 , Accuracy : 95.00%\n",
      "Epoch [1024/1200], Loss: 0.0172 , Accuracy : 95.00%\n",
      "Epoch [1025/1200], Loss: 0.0268 , Accuracy : 97.50%\n",
      "Epoch [1026/1200], Loss: 1.6453 , Accuracy : 92.50%\n",
      "Epoch [1027/1200], Loss: 0.1370 , Accuracy : 95.00%\n",
      "Epoch [1028/1200], Loss: 0.0229 , Accuracy : 92.50%\n",
      "Epoch [1029/1200], Loss: 0.0101 , Accuracy : 95.00%\n",
      "Epoch [1030/1200], Loss: 0.0030 , Accuracy : 95.00%\n",
      "Epoch [1031/1200], Loss: 0.0114 , Accuracy : 92.50%\n",
      "Epoch [1032/1200], Loss: 0.0150 , Accuracy : 95.00%\n",
      "Epoch [1033/1200], Loss: 0.6803 , Accuracy : 97.50%\n",
      "Epoch [1034/1200], Loss: 0.0181 , Accuracy : 95.00%\n",
      "Epoch [1035/1200], Loss: 0.0252 , Accuracy : 97.50%\n",
      "Epoch [1036/1200], Loss: 0.0530 , Accuracy : 97.50%\n",
      "Epoch [1037/1200], Loss: 0.4664 , Accuracy : 95.00%\n",
      "Epoch [1038/1200], Loss: 0.1542 , Accuracy : 100.00%\n",
      "Epoch [1039/1200], Loss: 0.0112 , Accuracy : 97.50%\n",
      "Epoch [1040/1200], Loss: 0.0047 , Accuracy : 100.00%\n",
      "Epoch [1041/1200], Loss: 0.0406 , Accuracy : 92.50%\n",
      "Epoch [1042/1200], Loss: 0.4185 , Accuracy : 95.00%\n",
      "Epoch [1043/1200], Loss: 0.0093 , Accuracy : 97.50%\n",
      "Epoch [1044/1200], Loss: 0.0570 , Accuracy : 97.50%\n",
      "Epoch [1045/1200], Loss: 0.0050 , Accuracy : 97.50%\n",
      "Epoch [1046/1200], Loss: 0.0080 , Accuracy : 97.50%\n",
      "Epoch [1047/1200], Loss: 0.0094 , Accuracy : 90.00%\n",
      "Epoch [1048/1200], Loss: 0.0055 , Accuracy : 87.50%\n",
      "Epoch [1049/1200], Loss: 0.4714 , Accuracy : 90.00%\n",
      "Epoch [1050/1200], Loss: 1.0010 , Accuracy : 85.00%\n",
      "Epoch [1051/1200], Loss: 0.0422 , Accuracy : 90.00%\n",
      "Epoch [1052/1200], Loss: 0.0204 , Accuracy : 90.00%\n",
      "Epoch [1053/1200], Loss: 0.0164 , Accuracy : 90.00%\n",
      "Epoch [1054/1200], Loss: 0.0056 , Accuracy : 92.50%\n",
      "Epoch [1055/1200], Loss: 0.5805 , Accuracy : 90.00%\n",
      "Epoch [1056/1200], Loss: 0.0166 , Accuracy : 97.50%\n",
      "Epoch [1057/1200], Loss: 0.1039 , Accuracy : 100.00%\n",
      "Epoch [1058/1200], Loss: 0.0188 , Accuracy : 92.50%\n",
      "Epoch [1059/1200], Loss: 1.4020 , Accuracy : 90.00%\n",
      "Epoch [1060/1200], Loss: 0.1312 , Accuracy : 97.50%\n",
      "Epoch [1061/1200], Loss: 0.7195 , Accuracy : 87.50%\n",
      "Epoch [1062/1200], Loss: 0.5460 , Accuracy : 85.00%\n",
      "Epoch [1063/1200], Loss: 0.2973 , Accuracy : 90.00%\n",
      "Epoch [1064/1200], Loss: 0.0009 , Accuracy : 95.00%\n",
      "Epoch [1065/1200], Loss: 0.0562 , Accuracy : 97.50%\n",
      "Epoch [1066/1200], Loss: 0.0134 , Accuracy : 97.50%\n",
      "Epoch [1067/1200], Loss: 0.0030 , Accuracy : 97.50%\n",
      "Epoch [1068/1200], Loss: 0.0119 , Accuracy : 100.00%\n",
      "Epoch [1069/1200], Loss: 0.0497 , Accuracy : 100.00%\n",
      "Epoch [1070/1200], Loss: 0.0059 , Accuracy : 97.50%\n",
      "Epoch [1071/1200], Loss: 0.2981 , Accuracy : 90.00%\n",
      "Epoch [1072/1200], Loss: 0.0169 , Accuracy : 92.50%\n",
      "Epoch [1073/1200], Loss: 0.0204 , Accuracy : 97.50%\n",
      "Epoch [1074/1200], Loss: 0.1189 , Accuracy : 97.50%\n",
      "Epoch [1075/1200], Loss: 0.0049 , Accuracy : 95.00%\n",
      "Epoch [1076/1200], Loss: 0.0090 , Accuracy : 97.50%\n",
      "Epoch [1077/1200], Loss: 0.0214 , Accuracy : 97.50%\n",
      "Epoch [1078/1200], Loss: 0.0315 , Accuracy : 95.00%\n",
      "Epoch [1079/1200], Loss: 0.1956 , Accuracy : 97.50%\n",
      "Epoch [1080/1200], Loss: 0.0071 , Accuracy : 97.50%\n",
      "Epoch [1081/1200], Loss: 0.0077 , Accuracy : 97.50%\n",
      "Epoch [1082/1200], Loss: 0.0023 , Accuracy : 95.00%\n",
      "Epoch [1083/1200], Loss: 0.0024 , Accuracy : 95.00%\n",
      "Epoch [1084/1200], Loss: 0.5948 , Accuracy : 90.00%\n",
      "Epoch [1085/1200], Loss: 0.3378 , Accuracy : 87.50%\n",
      "Epoch [1086/1200], Loss: 1.0244 , Accuracy : 87.50%\n",
      "Epoch [1087/1200], Loss: 0.0493 , Accuracy : 72.50%\n",
      "Epoch [1088/1200], Loss: 0.2707 , Accuracy : 85.00%\n",
      "Epoch [1089/1200], Loss: 0.5636 , Accuracy : 77.50%\n",
      "Epoch [1090/1200], Loss: 0.0486 , Accuracy : 85.00%\n",
      "Epoch [1091/1200], Loss: 0.3047 , Accuracy : 87.50%\n",
      "Epoch [1092/1200], Loss: 0.8701 , Accuracy : 85.00%\n",
      "Epoch [1093/1200], Loss: 0.3728 , Accuracy : 77.50%\n",
      "Epoch [1094/1200], Loss: 1.0236 , Accuracy : 55.00%\n",
      "Epoch [1095/1200], Loss: 1.4045 , Accuracy : 77.50%\n",
      "Epoch [1096/1200], Loss: 0.0190 , Accuracy : 80.00%\n",
      "Epoch [1097/1200], Loss: 0.9340 , Accuracy : 72.50%\n",
      "Epoch [1098/1200], Loss: 0.1177 , Accuracy : 75.00%\n",
      "Epoch [1099/1200], Loss: 0.7792 , Accuracy : 87.50%\n",
      "Epoch [1100/1200], Loss: 0.7304 , Accuracy : 72.50%\n",
      "Epoch [1101/1200], Loss: 1.2610 , Accuracy : 82.50%\n",
      "Epoch [1102/1200], Loss: 0.4265 , Accuracy : 92.50%\n",
      "Epoch [1103/1200], Loss: 0.0517 , Accuracy : 92.50%\n",
      "Epoch [1104/1200], Loss: 0.0252 , Accuracy : 95.00%\n",
      "Epoch [1105/1200], Loss: 0.0270 , Accuracy : 92.50%\n",
      "Epoch [1106/1200], Loss: 0.0093 , Accuracy : 90.00%\n",
      "Epoch [1107/1200], Loss: 0.0107 , Accuracy : 97.50%\n",
      "Epoch [1108/1200], Loss: 0.0415 , Accuracy : 95.00%\n",
      "Epoch [1109/1200], Loss: 0.2829 , Accuracy : 95.00%\n",
      "Epoch [1110/1200], Loss: 0.0639 , Accuracy : 95.00%\n",
      "Epoch [1111/1200], Loss: 0.0057 , Accuracy : 95.00%\n",
      "Epoch [1112/1200], Loss: 0.0133 , Accuracy : 92.50%\n",
      "Epoch [1113/1200], Loss: 0.2050 , Accuracy : 97.50%\n",
      "Epoch [1114/1200], Loss: 0.0875 , Accuracy : 100.00%\n",
      "Epoch [1115/1200], Loss: 0.2640 , Accuracy : 92.50%\n",
      "Epoch [1116/1200], Loss: 0.4121 , Accuracy : 95.00%\n",
      "Epoch [1117/1200], Loss: 0.0397 , Accuracy : 95.00%\n",
      "Epoch [1118/1200], Loss: 0.0044 , Accuracy : 100.00%\n",
      "Epoch [1119/1200], Loss: 0.0672 , Accuracy : 92.50%\n",
      "Epoch [1120/1200], Loss: 0.2817 , Accuracy : 97.50%\n",
      "Epoch [1121/1200], Loss: 0.0267 , Accuracy : 95.00%\n",
      "Epoch [1122/1200], Loss: 0.3742 , Accuracy : 97.50%\n",
      "Epoch [1123/1200], Loss: 0.1427 , Accuracy : 100.00%\n",
      "Epoch [1124/1200], Loss: 0.0412 , Accuracy : 97.50%\n",
      "Epoch [1125/1200], Loss: 0.2784 , Accuracy : 92.50%\n",
      "Epoch [1126/1200], Loss: 0.0054 , Accuracy : 100.00%\n",
      "Epoch [1127/1200], Loss: 0.2941 , Accuracy : 100.00%\n",
      "Epoch [1128/1200], Loss: 0.0030 , Accuracy : 95.00%\n",
      "Epoch [1129/1200], Loss: 0.3600 , Accuracy : 97.50%\n",
      "Epoch [1130/1200], Loss: 0.0029 , Accuracy : 100.00%\n",
      "Epoch [1131/1200], Loss: 0.1996 , Accuracy : 100.00%\n",
      "Epoch [1132/1200], Loss: 0.1145 , Accuracy : 97.50%\n",
      "Epoch [1133/1200], Loss: 0.0384 , Accuracy : 100.00%\n",
      "Epoch [1134/1200], Loss: 0.0051 , Accuracy : 100.00%\n",
      "Epoch [1135/1200], Loss: 0.0029 , Accuracy : 95.00%\n",
      "Epoch [1136/1200], Loss: 0.0030 , Accuracy : 92.50%\n",
      "Epoch [1137/1200], Loss: 0.0308 , Accuracy : 97.50%\n",
      "Epoch [1138/1200], Loss: 0.0293 , Accuracy : 97.50%\n",
      "Epoch [1139/1200], Loss: 0.0076 , Accuracy : 95.00%\n",
      "Epoch [1140/1200], Loss: 0.1501 , Accuracy : 95.00%\n",
      "Epoch [1141/1200], Loss: 0.0052 , Accuracy : 95.00%\n",
      "Epoch [1142/1200], Loss: 0.1896 , Accuracy : 95.00%\n",
      "Epoch [1143/1200], Loss: 0.0375 , Accuracy : 97.50%\n",
      "Epoch [1144/1200], Loss: 0.0461 , Accuracy : 95.00%\n",
      "Epoch [1145/1200], Loss: 0.0204 , Accuracy : 97.50%\n",
      "Epoch [1146/1200], Loss: 0.0446 , Accuracy : 97.50%\n",
      "Epoch [1147/1200], Loss: 0.0042 , Accuracy : 97.50%\n",
      "Epoch [1148/1200], Loss: 0.1670 , Accuracy : 92.50%\n",
      "Epoch [1149/1200], Loss: 0.2766 , Accuracy : 97.50%\n",
      "Epoch [1150/1200], Loss: 0.7311 , Accuracy : 95.00%\n",
      "Epoch [1151/1200], Loss: 0.0037 , Accuracy : 97.50%\n",
      "Epoch [1152/1200], Loss: 0.0141 , Accuracy : 97.50%\n",
      "Epoch [1153/1200], Loss: 0.0066 , Accuracy : 97.50%\n",
      "Epoch [1154/1200], Loss: 0.3057 , Accuracy : 97.50%\n",
      "Epoch [1155/1200], Loss: 0.0138 , Accuracy : 90.00%\n",
      "Epoch [1156/1200], Loss: 0.0540 , Accuracy : 85.00%\n",
      "Epoch [1157/1200], Loss: 0.3222 , Accuracy : 92.50%\n",
      "Epoch [1158/1200], Loss: 3.7221 , Accuracy : 85.00%\n",
      "Epoch [1159/1200], Loss: 1.8081 , Accuracy : 80.00%\n",
      "Epoch [1160/1200], Loss: 0.6700 , Accuracy : 80.00%\n",
      "Epoch [1161/1200], Loss: 2.0447 , Accuracy : 75.00%\n",
      "Epoch [1162/1200], Loss: 0.0464 , Accuracy : 90.00%\n",
      "Epoch [1163/1200], Loss: 0.0906 , Accuracy : 85.00%\n",
      "Epoch [1164/1200], Loss: 0.7337 , Accuracy : 92.50%\n",
      "Epoch [1165/1200], Loss: 0.4913 , Accuracy : 87.50%\n",
      "Epoch [1166/1200], Loss: 0.1827 , Accuracy : 95.00%\n",
      "Epoch [1167/1200], Loss: 0.3044 , Accuracy : 97.50%\n",
      "Epoch [1168/1200], Loss: 0.0239 , Accuracy : 87.50%\n",
      "Epoch [1169/1200], Loss: 0.0588 , Accuracy : 90.00%\n",
      "Epoch [1170/1200], Loss: 0.0191 , Accuracy : 90.00%\n",
      "Epoch [1171/1200], Loss: 0.4979 , Accuracy : 90.00%\n",
      "Epoch [1172/1200], Loss: 0.5681 , Accuracy : 72.50%\n",
      "Epoch [1173/1200], Loss: 1.5486 , Accuracy : 75.00%\n",
      "Epoch [1174/1200], Loss: 0.0674 , Accuracy : 90.00%\n",
      "Epoch [1175/1200], Loss: 0.4817 , Accuracy : 82.50%\n",
      "Epoch [1176/1200], Loss: 0.1350 , Accuracy : 82.50%\n",
      "Epoch [1177/1200], Loss: 0.9890 , Accuracy : 85.00%\n",
      "Epoch [1178/1200], Loss: 0.0062 , Accuracy : 82.50%\n",
      "Epoch [1179/1200], Loss: 0.0118 , Accuracy : 80.00%\n",
      "Epoch [1180/1200], Loss: 0.3375 , Accuracy : 85.00%\n",
      "Epoch [1181/1200], Loss: 0.3689 , Accuracy : 85.00%\n",
      "Epoch [1182/1200], Loss: 0.5949 , Accuracy : 92.50%\n",
      "Epoch [1183/1200], Loss: 0.0146 , Accuracy : 87.50%\n",
      "Epoch [1184/1200], Loss: 0.0109 , Accuracy : 87.50%\n",
      "Epoch [1185/1200], Loss: 0.0909 , Accuracy : 92.50%\n",
      "Epoch [1186/1200], Loss: 0.0396 , Accuracy : 95.00%\n",
      "Epoch [1187/1200], Loss: 0.4406 , Accuracy : 92.50%\n",
      "Epoch [1188/1200], Loss: 0.0126 , Accuracy : 97.50%\n",
      "Epoch [1189/1200], Loss: 0.0076 , Accuracy : 97.50%\n",
      "Epoch [1190/1200], Loss: 0.0038 , Accuracy : 95.00%\n",
      "Epoch [1191/1200], Loss: 0.2864 , Accuracy : 97.50%\n",
      "Epoch [1192/1200], Loss: 0.0714 , Accuracy : 95.00%\n",
      "Epoch [1193/1200], Loss: 0.4947 , Accuracy : 95.00%\n",
      "Epoch [1194/1200], Loss: 0.5951 , Accuracy : 90.00%\n",
      "Epoch [1195/1200], Loss: 0.0121 , Accuracy : 95.00%\n",
      "Epoch [1196/1200], Loss: 0.0061 , Accuracy : 95.00%\n",
      "Epoch [1197/1200], Loss: 0.0078 , Accuracy : 95.00%\n",
      "Epoch [1198/1200], Loss: 0.0983 , Accuracy : 97.50%\n",
      "Epoch [1199/1200], Loss: 0.0063 , Accuracy : 95.00%\n",
      "Epoch [1200/1200], Loss: 0.0145 , Accuracy : 95.00%\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "# References : https://saturncloud.io/blog/calculating-the-accuracy-of-pytorch-models-every-epoch/#:~:text=In%20order%20to%20calculate%20the,tensor%20along%20a%20specified%20dimension\n",
    "num_epochs = 1200\n",
    "loss_logger = []\n",
    "accuracy_logger = []\n",
    "# n_epochs = []\n",
    "for epoch in range(num_epochs):\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    model.train()\n",
    "    for i, (sequences, labels) in enumerate(data_loader):\n",
    "        # Move data to the device\n",
    "        # labels = labels.type(torch.LongTensor)   # casting to long\n",
    "        sequences = sequences.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(sequences)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        total_correct += (predicted == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # loss_logger.append(loss.item())\n",
    "    loss_logger.append(loss.item())\n",
    "    accuracy = 100 * total_correct /total_samples\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f} , Accuracy : {accuracy:.2f}%')\n",
    "    accuracy_logger.append(accuracy)\n",
    "    # n_epochs.append(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3.764673948287964,\n",
       " 3.5973055362701416,\n",
       " 3.7074666023254395,\n",
       " 3.5268964767456055,\n",
       " 3.615044116973877,\n",
       " 3.47186017036438,\n",
       " 3.4550113677978516,\n",
       " 3.434576988220215,\n",
       " 3.7242326736450195,\n",
       " 3.61862850189209,\n",
       " 3.44840669631958,\n",
       " 3.415172576904297,\n",
       " 3.4947283267974854,\n",
       " 2.9629485607147217,\n",
       " 3.804520606994629,\n",
       " 3.246426582336426,\n",
       " 4.4522576332092285,\n",
       " 3.1533632278442383,\n",
       " 3.532270669937134,\n",
       " 3.2722678184509277,\n",
       " 4.311208248138428,\n",
       " 3.4941768646240234,\n",
       " 3.2383675575256348,\n",
       " 2.98659086227417,\n",
       " 2.852052688598633,\n",
       " 4.171398162841797,\n",
       " 3.403076648712158,\n",
       " 3.7528018951416016,\n",
       " 3.226107120513916,\n",
       " 3.092977523803711,\n",
       " 3.0363235473632812,\n",
       " 3.655421018600464,\n",
       " 3.396028518676758,\n",
       " 3.149094581604004,\n",
       " 3.4016778469085693,\n",
       " 4.094308853149414,\n",
       " 3.0219614505767822,\n",
       " 3.093695640563965,\n",
       " 3.2201571464538574,\n",
       " 3.819020986557007,\n",
       " 3.06616473197937,\n",
       " 3.6410601139068604,\n",
       " 3.704092502593994,\n",
       " 2.7257683277130127,\n",
       " 2.6691887378692627,\n",
       " 2.860197067260742,\n",
       " 3.3422155380249023,\n",
       " 3.430356979370117,\n",
       " 2.3872108459472656,\n",
       " 3.339860439300537,\n",
       " 3.4398508071899414,\n",
       " 3.306182861328125,\n",
       " 2.176501512527466,\n",
       " 3.0487635135650635,\n",
       " 3.1241424083709717,\n",
       " 2.952664852142334,\n",
       " 3.18157958984375,\n",
       " 2.3735430240631104,\n",
       " 2.993649482727051,\n",
       " 4.033298492431641,\n",
       " 2.1902177333831787,\n",
       " 3.8228540420532227,\n",
       " 4.554790496826172,\n",
       " 4.825445175170898,\n",
       " 2.6403610706329346,\n",
       " 3.418346405029297,\n",
       " 2.110367774963379,\n",
       " 2.1771347522735596,\n",
       " 2.4215145111083984,\n",
       " 3.052412748336792,\n",
       " 2.5432403087615967,\n",
       " 2.4441165924072266,\n",
       " 3.2502267360687256,\n",
       " 2.729069709777832,\n",
       " 1.7520220279693604,\n",
       " 1.5966768264770508,\n",
       " 3.409369707107544,\n",
       " 2.0790321826934814,\n",
       " 3.0840072631835938,\n",
       " 2.2991857528686523,\n",
       " 2.410862445831299,\n",
       " 2.4292027950286865,\n",
       " 1.7604337930679321,\n",
       " 1.7634083032608032,\n",
       " 2.743192434310913,\n",
       " 2.571322441101074,\n",
       " 1.8570080995559692,\n",
       " 2.2383761405944824,\n",
       " 2.8717451095581055,\n",
       " 2.158787250518799,\n",
       " 2.504713535308838,\n",
       " 4.27749490737915,\n",
       " 2.0486669540405273,\n",
       " 1.1578344106674194,\n",
       " 3.0516350269317627,\n",
       " 1.8008716106414795,\n",
       " 2.0862154960632324,\n",
       " 2.446486473083496,\n",
       " 2.2474365234375,\n",
       " 1.7535673379898071,\n",
       " 1.738594889640808,\n",
       " 2.4533772468566895,\n",
       " 3.2860324382781982,\n",
       " 1.6780807971954346,\n",
       " 1.5013644695281982,\n",
       " 2.1883351802825928,\n",
       " 1.3920483589172363,\n",
       " 2.3280394077301025,\n",
       " 2.5905632972717285,\n",
       " 2.270108222961426,\n",
       " 1.8556511402130127,\n",
       " 2.1603407859802246,\n",
       " 1.471037745475769,\n",
       " 2.315595865249634,\n",
       " 1.8383500576019287,\n",
       " 1.7568280696868896,\n",
       " 1.4960427284240723,\n",
       " 1.706394076347351,\n",
       " 1.1124963760375977,\n",
       " 0.890450656414032,\n",
       " 1.8635820150375366,\n",
       " 1.3769646883010864,\n",
       " 2.373206615447998,\n",
       " 1.5151245594024658,\n",
       " 1.4225432872772217,\n",
       " 2.133134603500366,\n",
       " 1.7471716403961182,\n",
       " 1.9242111444473267,\n",
       " 1.2580819129943848,\n",
       " 1.2460029125213623,\n",
       " 2.580047369003296,\n",
       " 2.473219871520996,\n",
       " 1.6709668636322021,\n",
       " 2.2525360584259033,\n",
       " 1.7423310279846191,\n",
       " 1.1278114318847656,\n",
       " 1.9318313598632812,\n",
       " 2.2227234840393066,\n",
       " 1.2110735177993774,\n",
       " 3.0500378608703613,\n",
       " 1.2206001281738281,\n",
       " 2.0854203701019287,\n",
       " 3.276101589202881,\n",
       " 2.303051710128784,\n",
       " 2.44062876701355,\n",
       " 1.9996225833892822,\n",
       " 1.360293984413147,\n",
       " 1.6709617376327515,\n",
       " 0.953039288520813,\n",
       " 2.05832839012146,\n",
       " 4.138932228088379,\n",
       " 1.4579644203186035,\n",
       " 2.0678398609161377,\n",
       " 2.1871156692504883,\n",
       " 1.419954538345337,\n",
       " 2.0164010524749756,\n",
       " 2.3860514163970947,\n",
       " 1.6029502153396606,\n",
       " 1.6082767248153687,\n",
       " 1.4514137506484985,\n",
       " 1.8737707138061523,\n",
       " 0.9896804690361023,\n",
       " 1.4415597915649414,\n",
       " 1.1286627054214478,\n",
       " 1.0215107202529907,\n",
       " 1.5522600412368774,\n",
       " 1.2926028966903687,\n",
       " 0.8076381683349609,\n",
       " 0.818744957447052,\n",
       " 2.246654987335205,\n",
       " 0.9625294208526611,\n",
       " 2.8762078285217285,\n",
       " 1.4563848972320557,\n",
       " 1.780068039894104,\n",
       " 0.6470787525177002,\n",
       " 1.4314603805541992,\n",
       " 1.1655175685882568,\n",
       " 1.170533299446106,\n",
       " 2.371065139770508,\n",
       " 1.0311522483825684,\n",
       " 3.0227890014648438,\n",
       " 1.819716453552246,\n",
       " 1.446720838546753,\n",
       " 1.266221046447754,\n",
       " 1.0654404163360596,\n",
       " 1.7407732009887695,\n",
       " 1.6580997705459595,\n",
       " 2.1945736408233643,\n",
       " 1.8259974718093872,\n",
       " 2.062282085418701,\n",
       " 1.0216429233551025,\n",
       " 1.6712753772735596,\n",
       " 3.293292760848999,\n",
       " 1.2460083961486816,\n",
       " 1.658295750617981,\n",
       " 1.8557918071746826,\n",
       " 1.6972147226333618,\n",
       " 2.7777507305145264,\n",
       " 0.9937705993652344,\n",
       " 1.1889643669128418,\n",
       " 2.577571153640747,\n",
       " 2.7697911262512207,\n",
       " 0.7723813652992249,\n",
       " 1.6852517127990723,\n",
       " 2.2239112854003906,\n",
       " 1.253568172454834,\n",
       " 1.2545489072799683,\n",
       " 1.911604881286621,\n",
       " 0.8132322430610657,\n",
       " 2.2153844833374023,\n",
       " 1.122488260269165,\n",
       " 0.7010148167610168,\n",
       " 1.508133888244629,\n",
       " 1.9961721897125244,\n",
       " 0.7030655741691589,\n",
       " 2.164111375808716,\n",
       " 1.0535991191864014,\n",
       " 0.7123624086380005,\n",
       " 0.3586312532424927,\n",
       " 1.4578851461410522,\n",
       " 0.906098484992981,\n",
       " 0.9439738392829895,\n",
       " 3.2285027503967285,\n",
       " 1.7758899927139282,\n",
       " 1.2694456577301025,\n",
       " 0.7883110046386719,\n",
       " 2.1181719303131104,\n",
       " 1.8587925434112549,\n",
       " 2.0773329734802246,\n",
       " 1.7570106983184814,\n",
       " 1.2228882312774658,\n",
       " 1.6345875263214111,\n",
       " 0.6852432489395142,\n",
       " 1.350172519683838,\n",
       " 0.6314365863800049,\n",
       " 1.7394957542419434,\n",
       " 0.7229834794998169,\n",
       " 2.013352870941162,\n",
       " 0.7960062623023987,\n",
       " 1.783733606338501,\n",
       " 1.68037748336792,\n",
       " 1.8090763092041016,\n",
       " 1.6120917797088623,\n",
       " 0.6773377060890198,\n",
       " 3.7526586055755615,\n",
       " 0.7362938523292542,\n",
       " 2.2720422744750977,\n",
       " 1.5396029949188232,\n",
       " 0.6238535046577454,\n",
       " 1.0075643062591553,\n",
       " 1.5309979915618896,\n",
       " 0.7333353161811829,\n",
       " 0.9402071237564087,\n",
       " 0.9475760459899902,\n",
       " 1.4818551540374756,\n",
       " 0.46842366456985474,\n",
       " 1.339701533317566,\n",
       " 1.8803502321243286,\n",
       " 0.2645224928855896,\n",
       " 1.585082769393921,\n",
       " 0.7276879549026489,\n",
       " 1.929370641708374,\n",
       " 2.2017369270324707,\n",
       " 0.5051068663597107,\n",
       " 1.6052279472351074,\n",
       " 1.2948893308639526,\n",
       " 1.4449658393859863,\n",
       " 1.7437865734100342,\n",
       " 0.8336293697357178,\n",
       " 2.199493646621704,\n",
       " 1.4982898235321045,\n",
       " 0.7591018080711365,\n",
       " 1.5535587072372437,\n",
       " 2.0046982765197754,\n",
       " 1.5568891763687134,\n",
       " 0.9733314514160156,\n",
       " 0.5796884894371033,\n",
       " 0.6448205709457397,\n",
       " 0.7193101644515991,\n",
       " 0.856420636177063,\n",
       " 1.2718443870544434,\n",
       " 1.1300914287567139,\n",
       " 0.44348597526550293,\n",
       " 0.7750453948974609,\n",
       " 0.5744799971580505,\n",
       " 1.3236446380615234,\n",
       " 0.5424238443374634,\n",
       " 1.59779691696167,\n",
       " 0.68767249584198,\n",
       " 0.44405439496040344,\n",
       " 1.6347695589065552,\n",
       " 0.3753735423088074,\n",
       " 0.5452044010162354,\n",
       " 0.2533559799194336,\n",
       " 0.8701971769332886,\n",
       " 0.74005126953125,\n",
       " 1.0321615934371948,\n",
       " 4.070185661315918,\n",
       " 2.8056695461273193,\n",
       " 1.4082520008087158,\n",
       " 2.0620779991149902,\n",
       " 1.5961190462112427,\n",
       " 2.5135340690612793,\n",
       " 2.0087668895721436,\n",
       " 1.3754600286483765,\n",
       " 3.635201930999756,\n",
       " 1.9580549001693726,\n",
       " 1.453922986984253,\n",
       " 1.44362211227417,\n",
       " 0.5737932324409485,\n",
       " 0.7360205054283142,\n",
       " 1.30817711353302,\n",
       " 2.3913421630859375,\n",
       " 1.7473771572113037,\n",
       " 1.465155005455017,\n",
       " 1.4891663789749146,\n",
       " 1.8745604753494263,\n",
       " 0.2733481526374817,\n",
       " 1.7279276847839355,\n",
       " 1.154707908630371,\n",
       " 1.655091643333435,\n",
       " 1.0578784942626953,\n",
       " 1.0913728475570679,\n",
       " 1.271780014038086,\n",
       " 0.6338297128677368,\n",
       " 0.634642481803894,\n",
       " 0.7514342069625854,\n",
       " 1.4698432683944702,\n",
       " 0.586355984210968,\n",
       " 0.3302050828933716,\n",
       " 1.4432923793792725,\n",
       " 0.4741833209991455,\n",
       " 2.140218496322632,\n",
       " 0.1701636016368866,\n",
       " 1.6254534721374512,\n",
       " 1.0170414447784424,\n",
       " 0.49850162863731384,\n",
       " 0.13868920505046844,\n",
       " 2.4455182552337646,\n",
       " 0.6604076027870178,\n",
       " 0.5057202577590942,\n",
       " 0.1391657292842865,\n",
       " 0.6242658495903015,\n",
       " 1.285148024559021,\n",
       " 0.24365293979644775,\n",
       " 1.1998224258422852,\n",
       " 0.5437398552894592,\n",
       " 2.1285746097564697,\n",
       " 0.9179717302322388,\n",
       " 0.5651096701622009,\n",
       " 0.6556566953659058,\n",
       " 0.6063576340675354,\n",
       " 1.2949649095535278,\n",
       " 1.1418094635009766,\n",
       " 1.0222046375274658,\n",
       " 0.29589125514030457,\n",
       " 1.8469511270523071,\n",
       " 0.3209075927734375,\n",
       " 0.8271607160568237,\n",
       " 0.9394592046737671,\n",
       " 0.7435596585273743,\n",
       " 0.5957742929458618,\n",
       " 0.9017231464385986,\n",
       " 0.6274421215057373,\n",
       " 1.7771954536437988,\n",
       " 1.4657129049301147,\n",
       " 1.5568903684616089,\n",
       " 2.4470319747924805,\n",
       " 0.5382092595100403,\n",
       " 0.5257989168167114,\n",
       " 1.826615333557129,\n",
       " 1.5625519752502441,\n",
       " 0.42011672258377075,\n",
       " 1.0755929946899414,\n",
       " 0.2523462772369385,\n",
       " 1.3144959211349487,\n",
       " 1.1186695098876953,\n",
       " 0.4993312358856201,\n",
       " 0.7953699827194214,\n",
       " 0.573664128780365,\n",
       " 0.45679426193237305,\n",
       " 0.7216336131095886,\n",
       " 1.2871198654174805,\n",
       " 0.7674314975738525,\n",
       " 0.9567042589187622,\n",
       " 0.6443314552307129,\n",
       " 1.354508399963379,\n",
       " 1.3197665214538574,\n",
       " 0.9019526243209839,\n",
       " 1.5458612442016602,\n",
       " 0.5707938075065613,\n",
       " 1.1271438598632812,\n",
       " 0.0894598588347435,\n",
       " 1.6603503227233887,\n",
       " 0.5143679976463318,\n",
       " 0.16310028731822968,\n",
       " 0.9501162767410278,\n",
       " 1.8080673217773438,\n",
       " 0.20561139285564423,\n",
       " 0.7191787958145142,\n",
       " 0.4133663773536682,\n",
       " 0.875677227973938,\n",
       " 0.18726086616516113,\n",
       " 1.9183388948440552,\n",
       " 0.5221317410469055,\n",
       " 0.36134737730026245,\n",
       " 0.4954773485660553,\n",
       " 0.5497870445251465,\n",
       " 1.4799368381500244,\n",
       " 0.6282819509506226,\n",
       " 1.2546401023864746,\n",
       " 0.2730593979358673,\n",
       " 0.6461074352264404,\n",
       " 0.26634475588798523,\n",
       " 1.1247568130493164,\n",
       " 1.313187837600708,\n",
       " 2.463872194290161,\n",
       " 0.5126277208328247,\n",
       " 1.4329986572265625,\n",
       " 0.5428403615951538,\n",
       " 1.7087774276733398,\n",
       " 0.6314618587493896,\n",
       " 0.776029109954834,\n",
       " 0.6136939525604248,\n",
       " 1.3629096746444702,\n",
       " 2.1084275245666504,\n",
       " 0.7286146879196167,\n",
       " 0.4854210913181305,\n",
       " 1.0448898077011108,\n",
       " 0.7579432725906372,\n",
       " 0.8047387003898621,\n",
       " 2.8709938526153564,\n",
       " 1.4425172805786133,\n",
       " 0.7152007818222046,\n",
       " 0.6315576434135437,\n",
       " 0.7263739109039307,\n",
       " 0.3398277759552002,\n",
       " 0.7514764666557312,\n",
       " 0.4151957035064697,\n",
       " 0.5023139119148254,\n",
       " 1.7280032634735107,\n",
       " 0.7651431560516357,\n",
       " 0.7130100727081299,\n",
       " 0.7282953858375549,\n",
       " 1.8631713390350342,\n",
       " 0.7839622497558594,\n",
       " 0.45203593373298645,\n",
       " 0.9874382019042969,\n",
       " 0.3290036916732788,\n",
       " 1.0310167074203491,\n",
       " 0.44342565536499023,\n",
       " 0.7667691111564636,\n",
       " 0.3571015000343323,\n",
       " 1.2149016857147217,\n",
       " 1.5145399570465088,\n",
       " 1.4687360525131226,\n",
       " 1.2420096397399902,\n",
       " 1.7470548152923584,\n",
       " 0.8226390480995178,\n",
       " 1.061035394668579,\n",
       " 0.44697123765945435,\n",
       " 0.18097449839115143,\n",
       " 0.22569222748279572,\n",
       " 0.6595132350921631,\n",
       " 1.1785093545913696,\n",
       " 2.0280890464782715,\n",
       " 0.5170177817344666,\n",
       " 1.0290181636810303,\n",
       " 0.3446122407913208,\n",
       " 0.2856607139110565,\n",
       " 0.46934792399406433,\n",
       " 0.4482156038284302,\n",
       " 0.4379113018512726,\n",
       " 0.5556976795196533,\n",
       " 0.07445245236158371,\n",
       " 1.0500633716583252,\n",
       " 1.8703501224517822,\n",
       " 0.3081974983215332,\n",
       " 2.762129783630371,\n",
       " 1.6920171976089478,\n",
       " 0.4155026078224182,\n",
       " 1.3360178470611572,\n",
       " 0.6348133087158203,\n",
       " 1.2398967742919922,\n",
       " 1.2621158361434937,\n",
       " 1.257206916809082,\n",
       " 0.11187896132469177,\n",
       " 0.1829175055027008,\n",
       " 0.034119509160518646,\n",
       " 0.4861832857131958,\n",
       " 0.7851457595825195,\n",
       " 0.6580546498298645,\n",
       " 0.3197481334209442,\n",
       " 0.7805127501487732,\n",
       " 0.24793608486652374,\n",
       " 0.41184765100479126,\n",
       " 0.50439453125,\n",
       " 0.8210301995277405,\n",
       " 0.5069869160652161,\n",
       " 2.2282090187072754,\n",
       " 1.1207623481750488,\n",
       " 0.7715892791748047,\n",
       " 0.17740634083747864,\n",
       " 0.32400384545326233,\n",
       " 0.23771770298480988,\n",
       " 0.40312808752059937,\n",
       " 0.6822742223739624,\n",
       " 0.432779461145401,\n",
       " 0.6918636560440063,\n",
       " 1.0551888942718506,\n",
       " 0.9719988703727722,\n",
       " 0.29178544878959656,\n",
       " 0.33455073833465576,\n",
       " 0.766080379486084,\n",
       " 0.0576261505484581,\n",
       " 0.6397807002067566,\n",
       " 0.6416494250297546,\n",
       " 0.304067999124527,\n",
       " 0.3568211793899536,\n",
       " 1.681859016418457,\n",
       " 0.8032641410827637,\n",
       " 1.0722743272781372,\n",
       " 0.23882797360420227,\n",
       " 1.1855534315109253,\n",
       " 0.4000089168548584,\n",
       " 0.014184149913489819,\n",
       " 0.2852516174316406,\n",
       " 0.49691104888916016,\n",
       " 0.5308345556259155,\n",
       " 0.3376200497150421,\n",
       " 0.5781586766242981,\n",
       " 0.6831914782524109,\n",
       " 0.4827360510826111,\n",
       " 0.1451675444841385,\n",
       " 0.6502087116241455,\n",
       " 1.9686036109924316,\n",
       " 1.3798203468322754,\n",
       " 0.26435938477516174,\n",
       " 0.22960442304611206,\n",
       " 0.9009550213813782,\n",
       " 1.27433443069458,\n",
       " 0.8519140481948853,\n",
       " 0.13891538977622986,\n",
       " 1.1754446029663086,\n",
       " 1.779761552810669,\n",
       " 1.3054184913635254,\n",
       " 1.0945069789886475,\n",
       " 0.7384955286979675,\n",
       " 1.838055968284607,\n",
       " 0.9283621907234192,\n",
       " 0.44452619552612305,\n",
       " 0.35011065006256104,\n",
       " 0.5063445568084717,\n",
       " 0.41437068581581116,\n",
       " 0.8068932890892029,\n",
       " 0.15131528675556183,\n",
       " 0.4648159444332123,\n",
       " 0.22122690081596375,\n",
       " 0.03222186118364334,\n",
       " 0.5898047685623169,\n",
       " 0.8440908789634705,\n",
       " 1.5490766763687134,\n",
       " 0.5952313542366028,\n",
       " 0.3028721213340759,\n",
       " 0.3637121021747589,\n",
       " 0.3510727882385254,\n",
       " 0.8674095869064331,\n",
       " 0.22385714948177338,\n",
       " 1.1230956315994263,\n",
       " 0.20199717581272125,\n",
       " 0.5501437187194824,\n",
       " 0.4409923255443573,\n",
       " 0.07288271188735962,\n",
       " 0.36562812328338623,\n",
       " 0.26618677377700806,\n",
       " 0.6873632669448853,\n",
       " 0.6858596801757812,\n",
       " 0.5668586492538452,\n",
       " 0.15977716445922852,\n",
       " 0.5476206541061401,\n",
       " 1.462315559387207,\n",
       " 1.0182616710662842,\n",
       " 0.5949329733848572,\n",
       " 1.0834048986434937,\n",
       " 3.387328863143921,\n",
       " 1.4712997674942017,\n",
       " 1.4475802183151245,\n",
       " 1.050341248512268,\n",
       " 0.784753143787384,\n",
       " 0.8254568576812744,\n",
       " 1.0017846822738647,\n",
       " 0.25073274970054626,\n",
       " 0.2952224016189575,\n",
       " 0.08085975795984268,\n",
       " 0.03960346058011055,\n",
       " 1.145135760307312,\n",
       " 0.21039628982543945,\n",
       " 0.1231781393289566,\n",
       " 0.4340050220489502,\n",
       " 0.2573067545890808,\n",
       " 0.07515361160039902,\n",
       " 0.4365014135837555,\n",
       " 0.5591059327125549,\n",
       " 0.33547505736351013,\n",
       " 0.09040428698062897,\n",
       " 0.03711303323507309,\n",
       " 0.20999018847942352,\n",
       " 0.09132601320743561,\n",
       " 0.20431600511074066,\n",
       " 0.7708341479301453,\n",
       " 0.056699495762586594,\n",
       " 0.5135208964347839,\n",
       " 0.21734853088855743,\n",
       " 0.19005855917930603,\n",
       " 0.36658746004104614,\n",
       " 0.20071248710155487,\n",
       " 1.5733435153961182,\n",
       " 0.9420777559280396,\n",
       " 0.0144011490046978,\n",
       " 0.9169971942901611,\n",
       " 0.02784087508916855,\n",
       " 0.09553986042737961,\n",
       " 0.5345965027809143,\n",
       " 0.04662095755338669,\n",
       " 0.10684581100940704,\n",
       " 0.05606832355260849,\n",
       " 0.8672188520431519,\n",
       " 0.01390332542359829,\n",
       " 0.05893339961767197,\n",
       " 0.3077792823314667,\n",
       " 0.034710973501205444,\n",
       " 0.7369235754013062,\n",
       " 0.6035106182098389,\n",
       " 0.80094975233078,\n",
       " 0.047191694378852844,\n",
       " 0.22963938117027283,\n",
       " 0.07779808342456818,\n",
       " 0.36958959698677063,\n",
       " 0.18612515926361084,\n",
       " 0.23278535902500153,\n",
       " 0.7393168210983276,\n",
       " 0.01707889325916767,\n",
       " 0.12852483987808228,\n",
       " 0.38721078634262085,\n",
       " 0.9008379578590393,\n",
       " 0.6325852274894714,\n",
       " 0.11922532320022583,\n",
       " 1.4555944204330444,\n",
       " 0.06852225959300995,\n",
       " 0.39998477697372437,\n",
       " 1.1165194511413574,\n",
       " 1.9173226356506348,\n",
       " 0.6561776399612427,\n",
       " 0.3440614640712738,\n",
       " 0.011046421714127064,\n",
       " 0.2794586420059204,\n",
       " 0.7772629857063293,\n",
       " 0.5674844980239868,\n",
       " 1.539644479751587,\n",
       " 0.6440588235855103,\n",
       " 0.27012234926223755,\n",
       " 0.38928931951522827,\n",
       " 0.0858239084482193,\n",
       " 0.48671695590019226,\n",
       " 0.4904819130897522,\n",
       " 0.47942882776260376,\n",
       " 0.4771116077899933,\n",
       " 0.4131476879119873,\n",
       " 0.09029502421617508,\n",
       " 0.5641772150993347,\n",
       " 0.036156848073005676,\n",
       " 0.08439239114522934,\n",
       " 0.5908712148666382,\n",
       " 0.3389485776424408,\n",
       " 0.10296417772769928,\n",
       " 0.01812434196472168,\n",
       " 0.09121508151292801,\n",
       " 0.6077345013618469,\n",
       " 0.2778318524360657,\n",
       " 0.7034319043159485,\n",
       " 0.14866501092910767,\n",
       " 0.8739155530929565,\n",
       " 0.4056386351585388,\n",
       " 0.04603157937526703,\n",
       " 0.511103093624115,\n",
       " 0.05321214348077774,\n",
       " 0.12362190335988998,\n",
       " 0.6007943153381348,\n",
       " 1.1444814205169678,\n",
       " 0.014456630684435368,\n",
       " 2.92830753326416,\n",
       " 1.2880983352661133,\n",
       " 1.6800885200500488,\n",
       " 0.2429121732711792,\n",
       " 0.307018518447876,\n",
       " 0.18607670068740845,\n",
       " 0.4673263430595398,\n",
       " 0.6361200213432312,\n",
       " 0.23721015453338623,\n",
       " 0.40249982476234436,\n",
       " 0.008763297460973263,\n",
       " 0.2384343147277832,\n",
       " 0.11181633919477463,\n",
       " 0.8296517133712769,\n",
       " 0.14791707694530487,\n",
       " 0.06684786826372147,\n",
       " 0.8889012336730957,\n",
       " 0.006907875649631023,\n",
       " 0.596688985824585,\n",
       " 0.09833882749080658,\n",
       " 0.14773274958133698,\n",
       " 0.23003843426704407,\n",
       " 0.4067595303058624,\n",
       " 0.3643001616001129,\n",
       " 1.2702018022537231,\n",
       " 0.10514470189809799,\n",
       " 1.3148186206817627,\n",
       " 0.0045527126640081406,\n",
       " 0.45796409249305725,\n",
       " 0.039640769362449646,\n",
       " 0.004899932071566582,\n",
       " 0.11841446906328201,\n",
       " 0.02352192997932434,\n",
       " 0.015131818130612373,\n",
       " 0.41242897510528564,\n",
       " 0.04895378276705742,\n",
       " 0.06024094671010971,\n",
       " 0.38738641142845154,\n",
       " 0.26852545142173767,\n",
       " 0.17000846564769745,\n",
       " 0.8743883371353149,\n",
       " 0.18382515013217926,\n",
       " 0.42234277725219727,\n",
       " 0.410555362701416,\n",
       " 0.18202665448188782,\n",
       " 0.025364529341459274,\n",
       " 0.0925324484705925,\n",
       " 0.5339198112487793,\n",
       " 0.006613119039684534,\n",
       " 0.049747295677661896,\n",
       " 0.08770112693309784,\n",
       " 0.2458638846874237,\n",
       " 0.43399885296821594,\n",
       " 0.4116758704185486,\n",
       " 0.7605551481246948,\n",
       " 0.032247185707092285,\n",
       " 0.06532652676105499,\n",
       " 0.5213837027549744,\n",
       " 0.011031290516257286,\n",
       " 0.4579136073589325,\n",
       " 0.5571038722991943,\n",
       " 0.47574761509895325,\n",
       " 0.4747787117958069,\n",
       " 0.349871963262558,\n",
       " 0.48955652117729187,\n",
       " 0.03189190849661827,\n",
       " 0.14149674773216248,\n",
       " 0.05174361541867256,\n",
       " 0.006821090821176767,\n",
       " 0.1559033989906311,\n",
       " 0.4124996066093445,\n",
       " 0.2733953595161438,\n",
       " 0.7045565247535706,\n",
       " 0.39346843957901,\n",
       " 0.9378621578216553,\n",
       " 0.9923126697540283,\n",
       " 0.7960157990455627,\n",
       " 0.5005833506584167,\n",
       " 0.9734688997268677,\n",
       " 0.9785661697387695,\n",
       " 0.31645917892456055,\n",
       " 1.6221293210983276,\n",
       " 1.096272587776184,\n",
       " 0.17406320571899414,\n",
       " 0.8284587860107422,\n",
       " 0.8029688596725464,\n",
       " 0.9546498656272888,\n",
       " 0.7721188068389893,\n",
       " 0.30863282084465027,\n",
       " 0.5007182359695435,\n",
       " 0.7599263191223145,\n",
       " 0.3101986348628998,\n",
       " 0.4617150127887726,\n",
       " 0.34736108779907227,\n",
       " 0.041731566190719604,\n",
       " 0.09633147716522217,\n",
       " 0.2578897774219513,\n",
       " 0.3347770869731903,\n",
       " 0.1525009125471115,\n",
       " 0.39013320207595825,\n",
       " 0.32649436593055725,\n",
       " 0.3674266040325165,\n",
       " 0.19014547765254974,\n",
       " 0.03252892196178436,\n",
       " 0.09026063233613968,\n",
       " 0.0648174062371254,\n",
       " 0.17828747630119324,\n",
       " 0.17677953839302063,\n",
       " 0.552099347114563,\n",
       " 0.4193580150604248,\n",
       " 0.6051658391952515,\n",
       " 0.655537486076355,\n",
       " 0.11095812916755676,\n",
       " 0.5961786508560181,\n",
       " 0.5569759607315063,\n",
       " 0.5445418953895569,\n",
       " 0.6468708515167236,\n",
       " 0.2827926278114319,\n",
       " 1.16818368434906,\n",
       " 0.0659136101603508,\n",
       " 1.255277156829834,\n",
       " 0.5786046385765076,\n",
       " 0.6091883778572083,\n",
       " 0.4041808843612671,\n",
       " 0.06405360251665115,\n",
       " 0.2641652524471283,\n",
       " 0.2124006152153015,\n",
       " 0.45365631580352783,\n",
       " 0.14228639006614685,\n",
       " 0.19490812718868256,\n",
       " 1.4403409957885742,\n",
       " 0.11723470687866211,\n",
       " 1.3944411277770996,\n",
       " 0.09404723346233368,\n",
       " 0.15512534976005554,\n",
       " 0.0610334649682045,\n",
       " 0.4232470989227295,\n",
       " 0.4763282835483551,\n",
       " 0.3515778183937073,\n",
       " 0.1119060218334198,\n",
       " 0.1770542711019516,\n",
       " 0.45913931727409363,\n",
       " 0.011407275684177876,\n",
       " 0.3805740475654602,\n",
       " 0.412325382232666,\n",
       " 0.13891993463039398,\n",
       " 0.8529849052429199,\n",
       " 0.030307527631521225,\n",
       " 0.49874162673950195,\n",
       " 0.17965491116046906,\n",
       " 0.2889096438884735,\n",
       " 0.045070309191942215,\n",
       " 0.5321541428565979,\n",
       " 0.4754485487937927,\n",
       " 0.9270117282867432,\n",
       " 0.05673935264348984,\n",
       " 0.24972295761108398,\n",
       " 0.014907769858837128,\n",
       " 0.028432060033082962,\n",
       " 0.22105316817760468,\n",
       " 0.5787844657897949,\n",
       " 0.03420981764793396,\n",
       " 0.35096418857574463,\n",
       " 0.26057595014572144,\n",
       " 0.33479607105255127,\n",
       " 0.5273227691650391,\n",
       " 0.2453688234090805,\n",
       " 0.7951448559761047,\n",
       " 0.14326462149620056,\n",
       " 0.6756170392036438,\n",
       " 0.39872080087661743,\n",
       " 0.008632334880530834,\n",
       " 0.4851161539554596,\n",
       " 0.06399665027856827,\n",
       " 0.008549604564905167,\n",
       " 0.02876940369606018,\n",
       " 0.7175846099853516,\n",
       " 0.06144212558865547,\n",
       " 1.1462217569351196,\n",
       " 0.0712478831410408,\n",
       " 0.14789825677871704,\n",
       " 0.23402296006679535,\n",
       " 0.0024701484944671392,\n",
       " 0.2733997106552124,\n",
       " 0.06707736849784851,\n",
       " 0.09604113548994064,\n",
       " 0.07329283654689789,\n",
       " 0.5236268043518066,\n",
       " 1.1871426105499268,\n",
       " 2.297755241394043,\n",
       " 0.4558117091655731,\n",
       " 0.25073930621147156,\n",
       " 0.02223210036754608,\n",
       " 0.05381114408373833,\n",
       " 0.48786160349845886,\n",
       " 0.24442869424819946,\n",
       " 0.12773765623569489,\n",
       " 0.5630376935005188,\n",
       " 0.03643759340047836,\n",
       " 0.11512147635221481,\n",
       " 0.25537651777267456,\n",
       " 1.1495400667190552,\n",
       " 0.022523727267980576,\n",
       " 0.03176770731806755,\n",
       " 0.05684790015220642,\n",
       " 0.10744020342826843,\n",
       " 0.0448700375854969,\n",
       " 0.1422383338212967,\n",
       " 0.28367775678634644,\n",
       " 0.16449034214019775,\n",
       " 0.0066611384972929955,\n",
       " 0.786870002746582,\n",
       " 0.5599702000617981,\n",
       " 0.37654048204421997,\n",
       " 0.15228530764579773,\n",
       " 0.8791634440422058,\n",
       " 0.008555574342608452,\n",
       " 0.09842723608016968,\n",
       " 0.47669675946235657,\n",
       " 0.07429256290197372,\n",
       " 0.060904815793037415,\n",
       " 0.4740216135978699,\n",
       " 0.5949751138687134,\n",
       " 0.16011933982372284,\n",
       " 0.02430545724928379,\n",
       " 0.748923659324646,\n",
       " 0.03941185027360916,\n",
       " 0.018495779484510422,\n",
       " 0.009604924358427525,\n",
       " 0.009600884281098843,\n",
       " 0.05739644914865494,\n",
       " 1.2026718854904175,\n",
       " 0.6988683938980103,\n",
       " 1.3728969097137451,\n",
       " 0.1814139038324356,\n",
       " 1.44206702709198,\n",
       " 0.060597751289606094,\n",
       " 0.010579288937151432,\n",
       " 0.02529532089829445,\n",
       " 0.2532857060432434,\n",
       " 0.6317484974861145,\n",
       " 0.026178136467933655,\n",
       " 0.6293648481369019,\n",
       " 0.006639466620981693,\n",
       " 0.1732938587665558,\n",
       " 0.06054147705435753,\n",
       " 0.0043760864064097404,\n",
       " 0.01949726790189743,\n",
       " 0.03659922257065773,\n",
       " 0.1621398776769638,\n",
       " 0.6874872446060181,\n",
       " 0.4225461781024933,\n",
       " 0.19311875104904175,\n",
       " 0.3237069547176361,\n",
       " 0.1421569585800171,\n",
       " 0.2468605488538742,\n",
       " 0.2941463887691498,\n",
       " 0.22431303560733795,\n",
       " 0.021311484277248383,\n",
       " 1.1329606771469116,\n",
       " 0.17445871233940125,\n",
       " 0.3359445631504059,\n",
       " 0.2783236801624298,\n",
       " 0.04932229220867157,\n",
       " 0.16496536135673523,\n",
       " 0.10556422173976898,\n",
       " 0.2654673457145691,\n",
       " 0.8559319972991943,\n",
       " 0.30088138580322266,\n",
       " 0.05371391028165817,\n",
       " 0.10643927752971649,\n",
       " 0.03097524493932724,\n",
       " 0.07792026549577713,\n",
       " 0.4779945909976959,\n",
       " 0.38811174035072327,\n",
       " 0.006102059502154589,\n",
       " 0.5286862850189209,\n",
       " 0.07728175818920135,\n",
       " 0.06896817684173584,\n",
       " 0.03697729483246803,\n",
       " 0.05490969866514206,\n",
       " 0.2305888533592224,\n",
       " 0.06806478649377823,\n",
       " 0.1332254260778427,\n",
       " 0.00732287485152483,\n",
       " 0.04285655915737152,\n",
       " 0.02504207380115986,\n",
       " 0.017189495265483856,\n",
       " 0.2262582927942276,\n",
       " 0.002440634649246931,\n",
       " 0.016541093587875366,\n",
       " 0.004108055494725704,\n",
       " 0.46835723519325256,\n",
       " 0.5756223201751709,\n",
       " 0.08909931778907776,\n",
       " 0.8693661689758301,\n",
       " 0.00415304210036993,\n",
       " 0.2593167722225189,\n",
       " 0.0306180901825428,\n",
       " 0.04051291570067406,\n",
       " 0.07062303274869919,\n",
       " 0.02150612138211727,\n",
       " 0.013187034986913204,\n",
       " 0.14780154824256897,\n",
       " 0.10492832958698273,\n",
       " 0.022376075387001038,\n",
       " 0.028194548562169075,\n",
       " 1.2769713401794434,\n",
       " 0.3321211338043213,\n",
       " 0.033727969974279404,\n",
       " ...]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAApIAAAK9CAYAAACXcoyFAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAADRl0lEQVR4nOydd7wU1dnHf7O7t3CBS69SBQRUUMHegopijRpb1NhiTExMYoopxtiN+KrJq4k1sSavsSYae40ICoIgRar03tstwC278/5x2b0zs1POzJyZnZ39ffMh7p1yzpl6fvOc53mOoqqqCkIIIYQQQlySKHQDCCGEEEJIcUIhSQghhBBCPEEhSQghhBBCPEEhSQghhBBCPEEhSQghhBBCPEEhSQghhBBCPEEhSQghhBBCPEEhSQghhBBCPEEhSQghhBBCPEEhSQghhBBCPEEhSQiJHM888wwURcH06dML3RQhZs2ahe985zvo27cvKioq0LlzZ4wdOxZPP/000ul0oZtHCCGBkSp0AwghpJh54okncO2116JHjx647LLLMGTIENTW1uKjjz7C1VdfjfXr1+N3v/tdoZtJCCGBQCFJCCEe+fzzz3HttdfiqKOOwttvv4327dvn1v3sZz/D9OnTMXfuXCl11dfXo23btlLKIoQQWXBomxBStMycOROnnXYaqqur0a5dO5x00kn4/PPPdds0NTXh9ttvx5AhQ1BZWYkuXbrg2GOPxQcffJDbZsOGDbjqqqvQp08fVFRUoFevXjj77LOxYsUK2/pvv/12KIqC5557Ticisxx66KG48sorAQATJkyAoiiYMGGCbpsVK1ZAURQ888wzuWVXXnkl2rVrh6VLl+L0009H+/btcemll+LHP/4x2rVrh127duXVdfHFF6Nnz566ofR33nkHxx13HNq2bYv27dvjjDPOwLx582yPiRBC3EAhSQgpSubNm4fjjjsOs2fPxq9//WvcfPPNWL58OcaMGYOpU6fmtrvttttw++2344QTTsBDDz2Em266Cf369cOXX36Z2+a8887Dq6++iquuugqPPPIIfvrTn6K2tharVq2yrH/Xrl346KOPcPzxx6Nfv37Sj6+5uRnjxo1D9+7dcf/99+O8887DRRddhPr6erz11lt5bXnjjTdw/vnnI5lMAgD+8Y9/4IwzzkC7du3wP//zP7j55psxf/58HHvssY4CmRBCROHQNiGkKPn973+PpqYmfPrpp9h3330BAJdffjmGDh2KX//61/jkk08AAG+99RZOP/10/PWvfzUtZ8eOHZg8eTLuu+8+3HDDDbnlN954o239S5YsQVNTE0aMGCHpiPQ0NDTgggsuwPjx43PLVFXFPvvsgxdffBEXXHBBbvlbb72F+vp6XHTRRQCAuro6/PSnP8X3vvc93XFfccUVGDp0KO6++27L80EIIW6gRZIQUnSk02m8//77OOecc3IiEgB69eqFSy65BJ9++ilqamoAAB07dsS8efOwePFi07LatGmD8vJyTJgwAdu3bxduQ7Z8syFtWfzwhz/U/a0oCi644AK8/fbbqKuryy1/8cUXsc8+++DYY48FAHzwwQfYsWMHLr74YmzZsiX3L5lM4ogjjsDHH38cWJsJIaUFhSQhpOjYvHkzdu3ahaFDh+atGz58ODKZDFavXg0AuOOOO7Bjxw7st99+GDFiBH71q19hzpw5ue0rKirwP//zP3jnnXfQo0cPHH/88bj33nuxYcMG2zZUV1cDAGprayUeWSupVAp9+vTJW37RRRdh9+7deP311wG0WB/ffvttXHDBBVAUBQByovnEE09Et27ddP/ef/99bNq0KZA2E0JKDwpJQkisOf7447F06VI89dRTOPDAA/HEE09g1KhReOKJJ3Lb/OxnP8PXX3+N8ePHo7KyEjfffDOGDx+OmTNnWpY7ePBgpFIpfPXVV0LtyIo8I1Z5JisqKpBI5L+ijzzySAwYMAAvvfQSAOCNN97A7t27c8PaAJDJZAC0+El+8MEHef/+85//CLWZEEKcoJAkhBQd3bp1Q1VVFRYtWpS3buHChUgkEujbt29uWefOnXHVVVfh+eefx+rVqzFy5Ejcdtttuv0GDRqEX/7yl3j//fcxd+5cNDY24o9//KNlG6qqqnDiiSdi4sSJOeunHZ06dQLQ4pOpZeXKlY77Grnwwgvx7rvvoqamBi+++CIGDBiAI488UncsANC9e3eMHTs279+YMWNc10kIIWZQSBJCio5kMolTTjkF//nPf3QRyBs3bsQ///lPHHvssbmh561bt+r2bdeuHQYPHoyGhgYALRHPe/bs0W0zaNAgtG/fPreNFbfeeitUVcVll12m81nMMmPGDDz77LMAgP79+yOZTGLixIm6bR555BGxg9Zw0UUXoaGhAc8++yzeffddXHjhhbr148aNQ3V1Ne6++240NTXl7b9582bXdRJCiBmM2iaERJannnoK7777bt7y66+/HnfddRc++OADHHvssfjRj36EVCqFxx9/HA0NDbj33ntz2+6///4YM2YMRo8ejc6dO2P69Ol45ZVX8OMf/xgA8PXXX+Okk07ChRdeiP333x+pVAqvvvoqNm7ciG9/+9u27Tv66KPx8MMP40c/+hGGDRumm9lmwoQJeP3113HXXXcBADp06IALLrgAf/nLX6AoCgYNGoQ333zTk7/iqFGjMHjwYNx0001oaGjQDWsDLf6bjz76KC677DKMGjUK3/72t9GtWzesWrUKb731Fo455hg89NBDruslhJA8VEIIiRhPP/20CsDy3+rVq1VVVdUvv/xSHTdunNquXTu1qqpKPeGEE9TJkyfryrrrrrvUww8/XO3YsaPapk0bddiwYeof/vAHtbGxUVVVVd2yZYt63XXXqcOGDVPbtm2rdujQQT3iiCPUl156Sbi9M2bMUC+55BK1d+/eallZmdqpUyf1pJNOUp999lk1nU7nttu8ebN63nnnqVVVVWqnTp3UH/zgB+rcuXNVAOrTTz+d2+6KK65Q27Zta1vnTTfdpAJQBw8ebLnNxx9/rI4bN07t0KGDWllZqQ4aNEi98sor1enTpwsfGyGE2KGoqqoWTMUSQgghhJCihT6ShBBCCCHEExSShBBCCCHEExSShBBCCCHEExSShBBCCCHEExSShBBCCCHEExSShBBCCCHEE6EnJM9kMli3bh3at29vOfcsIYQQQggpHKqqora2Fr1790YiYW13DF1Irlu3TjcHLiGEEEIIiSarV69Gnz59LNeHLiTbt28PoKVh2blwCSGEEEJIdKipqUHfvn1zus2K0IVkdji7urqaQpIQQgghJMI4uSEy2IYQQgghhHjClZC87bbboCiK7t+wYcOCahshhBBCCIkwroe2DzjgAHz44YetBaRCHx0nhBBCCCERwLUKTKVS6Nmzp/D2DQ0NaGhoyP1dU1PjtkpCCCGERIh0Oo2mpqZCN4P4IJlMIpVK+U7F6FpILl68GL1790ZlZSWOOuoojB8/Hv369bPcfvz48bj99tt9NZIQQggh0aCurg5r1qyBqqqFbgrxSVVVFXr16oXy8nLPZSiqizvhnXfeQV1dHYYOHYr169fj9ttvx9q1azF37lzL8HAzi2Tfvn2xc+dORm0TQgghRUQ6ncbixYtRVVWFbt26cWKRIkVVVTQ2NmLz5s1Ip9MYMmRIXtLxmpoadOjQwVGvubJInnbaabnfI0eOxBFHHIH+/fvjpZdewtVXX226T0VFBSoqKtxUQwghhJAI0tTUBFVV0a1bN7Rp06bQzSE+aNOmDcrKyrBy5Uo0NjaisrLSUzm+0v907NgR++23H5YsWeKnGEIIIYQUEbRExgO7qQ+Fy/Czc11dHZYuXYpevXr5bgghhBBCCCkuXAnJG264AZ988glWrFiByZMn49xzz0UymcTFF18cVPsIIYQQQkhEcSUk16xZg4svvhhDhw7FhRdeiC5duuDzzz9Ht27dgmofIYQQQkhkGDBgAB544AEpZU2YMAGKomDHjh1SyisEroJtXnjhhaDaQQghhBASCGPGjMHBBx8sRQB+8cUXaNu2rf9GxQROS0MIIYSQkkZVVaTTaaHZ+jgKq8d/uA4hhBBCShJVVbGrsbkg/0TTYF955ZX45JNP8OCDD0JRFCiKgmeeeQaKouCdd97B6NGjUVFRgU8//RRLly7F2WefjR49eqBdu3Y47LDDdNNCA/lD24qi4IknnsC5556LqqoqDBkyBK+//rrnc/qvf/0LBxxwACoqKjBgwAD88Y9/1K1/5JFHMGTIEFRWVqJHjx44//zzc+teeeUVjBgxAm3atEGXLl0wduxY1NfXe26LCLRIEkIIIcQTu5vS2P+W9wpS9/w7xqGq3FnGPPjgg/j6669x4IEH4o477gAAzJs3DwDw29/+Fvfffz/23XdfdOrUCatXr8bpp5+OP/zhD6ioqMDf//53nHXWWVi0aJHtLH6333477r33Xtx33334y1/+gksvvRQrV65E586dXR3TjBkzcOGFF+K2227DRRddhMmTJ+NHP/oRunTpgiuvvBLTp0/HT3/6U/zjH//A0UcfjW3btmHSpEkAgPXr1+Piiy/Gvffei3PPPRe1tbWYNGlS4DMQUUgSQgghJLZ06NAB5eXlqKqqQs+ePQEACxcuBADccccdOPnkk3Pbdu7cGQcddFDu7zvvvBOvvvoqXn/9dfz4xz+2rOPKK6/MZbC5++678ec//xnTpk3Dqaee6qqtf/rTn3DSSSfh5ptvBgDst99+mD9/Pu677z5ceeWVWLVqFdq2bYszzzwT7du3R//+/XHIIYcAaBGSzc3N+Na3voX+/fsDAEaMGOGqfi9QSBJCCCHEE23Kkph/x7iC1e2XQw89VPd3XV0dbrvtNrz11ls5YbZ7926sWrXKtpyRI0fmfrdt2xbV1dXYtGmT6/YsWLAAZ599tm7ZMcccgwceeADpdBonn3wy+vfvj3333RennnoqTj311NyQ+kEHHYSTTjoJI0aMwLhx43DKKafg/PPPR6dOnVy3ww30kSSEEEKIJxRFQVV5qiD/ZMyuY4y+vuGGG/Dqq6/i7rvvxqRJkzBr1iyMGDECjY2NtuWUlZXlnZdMJuO7fUbat2+PL7/8Es8//zx69eqFW265BQcddBB27NiBZDKJDz74AO+88w72339//OUvf8HQoUOxfPly6e3QQiFJCCGEkFhTXl6OdDrtuN1nn32GK6+8Eueeey5GjBiBnj17YsWKFcE3cC/Dhw/HZ599ltem/fbbD8lkiwU2lUph7NixuPfeezFnzhysWLEC//3vfwG0CNhjjjkGt99+O2bOnIny8nK8+uqrgbaZQ9uEEEIIiTUDBgzA1KlTsWLFCrRr187SWjhkyBD8+9//xllnnQVFUXDzzTcHYlm04pe//CUOO+ww3HnnnbjoooswZcoUPPTQQ3jkkUcAAG+++SaWLVuG448/Hp06dcLbb7+NTCaDoUOHYurUqfjoo49wyimnoHv37pg6dSo2b96M4cOHB9pmWiQJIYQQEmtuuOEGJJNJ7L///ujWrZulz+Of/vQndOrUCUcffTTOOussjBs3DqNGjQqtnaNGjcJLL72EF154AQceeCBuueUW3HHHHbjyyisBAB07dsS///1vnHjiiRg+fDgee+wxPP/88zjggANQXV2NiRMn4vTTT8d+++2H3//+9/jjH/+I0047LdA2K2rQceEGampq0KFDB+zcuRPV1dVhVk0IIYQQH+zZswfLly/HwIEDUVlZWejmEJ/YXU9RvUaLJCGEEEII8QSFZADsaUpj5qrtyGRCNfYSQgghJEJce+21aNeunem/a6+9ttDNkwKDbQLgmr9Px6TFW/D7M4bje8ftW+jmEEIIIaQA3HHHHbjhhhtM18XFvY9CMgAmLd4CAPj7lJUUkoQQQkiJ0r17d3Tv3r3QzQgUDm0HSDLhP1kqIYQQEjVCjtMlASHjOlJIBgiFJCGEkDiRTYrtNNMLKQ527doFIH9mHjdwaDtAkhKmbyKEEEKiQiqVQlVVFTZv3oyysjIkErRHFSOqqmLXrl3YtGkTOnbsmPtA8AKFZIDQIkkIISROKIqCXr16Yfny5Vi5cmWhm0N80rFjR/Ts2dNXGRSSAZJKUkgSQgiJF+Xl5RgyZAiHt4ucsrIyX5bILBSSAZLg0DYhhJAYkkgkOLMNAcBgm0Dh0DYhhBBC4gyFZIAw2IYQQgghcYZCMkBokSSEEEJInKGQDBAKSUIIIYTEGQrJAKGQJIQQQkicoZAMEApJQgghhMQZCskAYfofQgghhMQZCskASfLsEkIIISTGUOoESIpzkBJCCCEkxlDpBEiCPpKEEEIIiTEUkgGSopAkhBBCSIyhkAwQBtsQQgghJM5QSAYILZKEEEIIiTMUkgFCH0lCCCGExBkKyQBh+h9CCCGExBlKnQBJ0keSEEIIITGGQlIymYya+51kHklCCCGExBgqHcmk1VYhmUrSIkkIIYSQ+EIhKZm0xiLJ9D+EEEIIiTMUkpJp1ghJpv8hhBBCSJyhkJRMOq2xSFJIEkIIISTGUEhKpjmTyf2mjiSEEEJInKGQlIw22EYBlSQhhBBC4guFpGS0wTaEEEIIIXGGQlIyWiGpgqKSEEIIIfGFQlIyKrUjIYQQQkoECknJaIUkRSUhhBBC4gyFpGQyVI+EEEIIKREoJCWjWvwmhBBCCIkbFJKSUWmRJIQQQkiJQCEpGV32H4pKQgghhMQYCknpUDwSQgghpDSgkJSM1iJJSUkIIYSQOEMhKRmOZhNCCCGkVCh5ISk7OEY7mw1FJSGEEELiTEkLyb9NXIaj7/kvVm/bJa3MTEZaUYQQQgghkaakheQf3l6A9Tv34O63F0grk/NrE0IIIaRUKGkhmSWdkSf+dFMkUlQSQgghJMZQSEqGfpGEEEIIKRUoJCWjnWubopIQQgghcYZCUjLUjoQQQggpFSgkASiKvLK06YQoKgkhhBASZygkJSMxbocQQgghJNJQSEK2LyN9JAkhhBBSGlBISoYWSUIIIYSUChSSkmEeSUIIIYSUChSSCC7YhhBCCCEkzlBISkY3tE1NSQghhJAYQyEpGQ5nE0IIIaRUoJCUjN5HkhBCCCEkvlBISoYukoQQQggpFSgkAXy6eAse+u9iKYEyqi6PJFUlIYQQQuILhSSA+sY07n//a7w3b4PvsphHkhBCCCGlAoWkhjXbd/sug1ZIQgghhJQKFJKS0QXbUFMSQgghJMZQSErGa/qftTt241cvz8bCDTWSW0QIIYQQEgwUkpLJZFp/u5GUP/y/GXh5xhqc+edPpbeJEEIIISQIKCQ1yBiK9lrEwvW1AIBmRusQQgghpEigkJSMNtiGPpKEEEIIiTMUkpLxalDk1IqEEEIIKTYoJKWjsUhSHBJCCCEkxlBIapAh/OjiSAghhJBSgUJSMswjSQghhJBSgUJSMhzOJoQQQkipQCEpGQ5tE0IIIaRUoJDUICWPJMezCSGEEFIiUEhKRu8jKS4qqT8JIYQQUmxQSGpoaM7gw/kbUd/Q7LkM+kgSQgghpFRIFboBUeJPH3wNABg7vDueuOIwT2XoLJIyGkUIIYQQElFokTThwwWbPO/LYBtCCCGElAoUkpLhXNuEEEIIKRUoJCVTLOJx1uoduP+9RdjTlC50UwghhBBSpNBHUjLFEmxzzsOfAQCSCQU/P3m/AreGEEIIIcUILZKSyeiCbaIvKhdvqi10EwghhBBSpFBISqZYhrYJIYQQQvxCISkZrRWSopIQQgghcYZC0oGVW+vxzlfrhWep8Zr+h5qTEEIIIcUGhaQD37hvAn743Jd4b95GsR206X8CahMhhBBCSBSgkBTky1XbhbZjQnJCCCGElAoUkoIogtsVW0JyRfjI4s+iDbW468352FbfWOimEEIIIUUB80hKpgi0I7Fg3AMTAQBrtu/GY5eNLnBrCCGEkOhDi6QogoY7/dA2ZWUx8tXanYVuAiGEEFIUUEhKRjS6mxBCCCGk2KGQFETEl7ChOY1Pvt6c+9uNpqQAJYQQQkixQSEpkVtem4dJi7cUuhmEEEIIIaHgS0jec889UBQFP/vZzyQ1J7ooAj6SL05frfubRsbihNZhQgghRAzPQvKLL77A448/jpEjR8psDyGEEEIIKRI8Ccm6ujpceuml+Nvf/oZOnTrJblMk8ZJtUWXUNiGEEEJijCched111+GMM87A2LFjHbdtaGhATU2N7h8hUYbynxBCCBHDdULyF154AV9++SW++OILoe3Hjx+P22+/3XXDooaIj6SRonC148Q2hBBCCPGIK4vk6tWrcf311+O5555DZWWl0D433ngjdu7cmfu3evVq550iCKcSLB2K4gOAEEIIiQCuLJIzZszApk2bMGrUqNyydDqNiRMn4qGHHkJDQwOSyaRun4qKClRUVMhpbQHxYpEkxQl9WwkhhBAxXAnJk046CV999ZVu2VVXXYVhw4bhN7/5TZ6ILHUoRwghhBASZ1wJyfbt2+PAAw/ULWvbti26dOmSt5wQQgghhMQbzmwjiKf0P26mSPRQPgkG+kgSQgghYriO2jYyYcIECc0ghBBCCCHFBi2SoniItmHQBiGEEELiDIVkicNg9Hwo/wkhhBAxKCQF8SS4qEgIIYQQEmMoJAkxwGAbQgghRAwKSUE8TZHoZluKF0IIIYQUGRSShBBCCCHEExSSgniZa1ulmbFI4XUjhBBCRKCQJIQQQgghnqCQFCRoH0kSHWhIJoQQQsSgkCxxFC8KmRBCCCEEFJLCBD3XNiGEEEJIsUEhSYgB6n9CCCFEDApJQWav2YF35653tQ8FSXHCaHtCCCFEDApJQT5csAnX/t+X+HpjbaGbQgghhBASCUpWSE5YtMlxGzPL1Kqtu4JoDiGEEEJI0VGyQvLKp79w3Cbjc4STQ6TFCa8aIYQQIkbJCkkRzIQgs+UQQgghhLRAIWmDb4uknGaQkKEhmRBCCBEjVegGBEldQzNWbKlHWTKBoT3bu95fpRQsSeiSQAghhIgRa4vklyu348y/fIqfvTjL0/5mesLV0HYR6BGO1BNCCCHEK7EWkmXJlsNrSmdc77toQy2HOCPE3LU78b1nv2D6JUIIISRCxFpIlqda7G1ehOQVT01DxqeS5NC4PM595DN8uGATLvnb1MDr4lUjhBBCxIi1kMxZJJvdC8kNNXtMBYXCweCC0JRuuRpb6hoK3BJCCCGEZCkJIdm41yJZs6cJz01diW31jUL7+7ZI0rRFCCGElCSrtu5Cs4cR0WIj1lHb5am9QnKvRfKmV+fijdnr8NL0NUL7mwpBGiTjDz8ACCGE+ODduRtw7f/NwHFDuuIfVx9R6OYESqwtkuV7LZI1e5qxaEMt3v5qPQBg9uodQvubpYGp2d0kXD8tkoQQQkjp8czk5QCASYu3FLglwRNrIZkd2gaAcQ9MRFnSnTnRTAhe/8IsPPPZcr9NIxGG+p8QQggRI+ZCUi8cy5PuDtfKR/K2N+YL7V8MUduc8jEfJiQnhBDih1IKzI23kEzpD6885e5wKScIIYQQQqyJtZA0WiDLXFgkFcXaIikqSGnYIoQQQkicibWQNApHV0ISsDRJVlfGOti95KH+J4QQQsSItZBMJvQ+Cm6CbTIq8H+frzRdV11ZJlQGBQkhhBBC4kyshaSR8lTS1fZ//u8S0+XtaZEkhBBCCCkxIeky/Y8V1W3ELJKkOMn6tjanM6jdI543lBBCCAFKKyNKSQlJNz6SdlSWiVk2GWxT3Jz5l08x4rb3salmT6GbQgghhEQSCkkPJGL0pRGjQ5FGNv/nwg21AICPFm4qZHMIIYSQyFJSQnLKsq1SyhFPNEqTZDFitCTTskwIIcQNHNomhBBCCCHEAQpJD4h+adCSRQghhJA4E3sh2bVdufQyS8lkXYpQ/xNCCCFixF5IfvSLMdLLFPWRpCAhhBBCSg/xWIriJ/ZCskMVcz4SQgghhARB7IVkIVHpJFmcGKO2aVsmhBBCTKGQ9ELpWKwJIYQQQiyhkPQAs0jGG1ogCSGE+KGUgnIpJD2gxOgOidOxyIIeCYQQQogYFJIeELZIUpAQQgghJMZQSBJCCCGEEE+UhJB86QdHSS1P1NBIg2RxwutGCCGEiFESQnLEPh2klse0PqUFLzchhBBiTkkIyfKU3MOkriCEEEIIKREhmUwoSCUkRicLKklaLgkhhBASZ0pCSAJyrZLMMxhv+AFACCGEiFEyQrJCppCMkc5gFsl8YnR5CSGEFIBSytFcMkKyb+cqaWXFSUgSQgghhHilZITkwX07SitLdGibgjMe8DISQggh5pSMkDykX0dpZWX2KosVW+rx8vTVSGcoNeIEPwAIIYQQMVKFbkBYHNy3k7SyVBWYsXIbznt0CgCgKa3ikiP65W9HWxYhhBBScpSOh2QJWSQHdKlC+0o5unnJptqciASAL1Zsk1IuIYQQQkgxUTJCUlEUfH7jSXj4klG+y1qxdZehbPPtPluyFcs21/mujxBCCCEkipSMkASAthUpHDpA3hB3loRNmP+tr8/L/Z6ydCtOuH8CJi/ZIr0NhBBCCCFhU1JCEgB6VFfiwH2qpZaZtBGSzelWP8mL//Y5lm+pxyVPTJVaPyGEEEJIISg5IQkAxwzqKrW8hMzpF0n0YBh3ybOtvhGffL0ZGWZoIIQIUEL5yEtTSMrOOF/UOrKY205ISJzyvxNxxVPT8NL01YVuCiGERIqSFJKyhV/SpsBS+iohJK5sqWsAAHwwf2OBW0IIIdGiRIWkXHVnVxqFJCGEEELiSokKSbnlNabpN0UIIYSQFkrJhlSSQlK2j2Rjc8a6rpK6nQiJN/xkJIQQPSUpJGUPbTc0p6WWR6IFxQMhhBBiTokKSbnlNdhZJGmQJIQQQkoK2SOfUaY0haRkJWknJAkhhBBC4kpJCknZHwoNTcU7tE0fTkIIIYR4pSSFpGwfycY0LZKEEEIIKT1KVEjKLa/ZJv1PqfhJfLFiGz5dvKXQzSCEEEIKTmn0/C2UqJCUe4nVgOJ6G5rTuP+9RZixcnsg5csik1FxwWNT8J0np2JbfWOhm0MIIYSQkChJISnbSqja6Eg/NT356XI89PESnPfoZB+ltFC7pwkvfbEaO3bJF3raw98eQPmFxu76EkIIIaVMSQpJ2UPbtkLSR11LNtZ539nAr16eg1//aw6u+ft0aWUSUmqo/KoghBAdJSokZQ9tB4PMct+dtwEA8MUK+cPk7FwJIYSQ0qREhWT+sjZlSc/lOQmpPU1pvDx9tefyCSGEEEKiSKrQDSgEZj6SKdnj3dm6APzvB1/j8YnLXO8bhqVPhnGW9khCCCGklRJJ2AKgRC2SZhc4mfR+1e19JBV8uGCjt3I9tidstMdfQs8OIYQQUvKUppA0kTt+LJJO6X9KJZekF+58cz7OfuhTNDRHd3Yg+oASQggh5pSmkDSzSPoRkgGl/ykW/eInj+aTny7H7DU78d48b1ZbQgghJHqUjgGpNIWkybJUwvupcJJRXm+nItGRUgRvOsNpJgkhhJBioySFpBl+LJJ2KEppOd0SQgghpHQoSSFpJux8+UjamuQUU59M/+USQgghhBSW0hSSZsE2fqK2AcxYuc1Hi6zLJYQQQkhxUUojkSUpJM0MhEkfPpJQgfMenWJdXcxvKBmGUxpfSTHA25QQQvSUppA0wV/6H2t8icgi6bW0UdtxTHVUJJeBEEIICZ2SFJJmUsdf+p9gpIaftDqixE/2EUIIISQsSlNISp4i0T7UJp5WOi0cliaEEEJaiXevr6c0haTm95H7dsY93xrhyyKZzlgrKUXxkUeySASajGYWy7ESQgghpJWSFJJanrnqcHz78H6WUdsXHtrHsYymtH0yba8GSYorQgghhESZkhSSWmGX/W0VtT2qXyfH8jbWNMhoVh5h+EjKgPkuCSGEkNKEQnLvwLOVj6Rf90YFSuwtktpmxnE6yGK5DoQQQkjYlKaQ1MidRM4iaSEkfbrMtvhIlpLbLSGEEFLaxDzGVkdJCkkt2YhqrUWyPKk5LQW8GYrFEEaLHSkVeK8TQoiekhSS+qHtFlIa8VieSuSt91NXlIe2pXw1sXMlhBBCSpKSFJJaskJKa5FsW5HUrPevtMxKeH7aKt/lxomoBexErT2EEEJIFKGQ3CsUtT6SHdqUta73Wz7MTZK3/meewN7FIWaKJbqcEEIICYNSio0oeSGZRWuR7NimPPfbKUekIz7uJRrFCgfPPSGEEOJMSQpJs+FqrUWyWmORrGto9lXXwvU1mL16R95yOyve3LU7fdVppN7nMTgRd9EV88MjhBBCPFOaQtJkmc4iWWUuJP9z3TGu61q6ud71Pmf+5VNkMvIGjA+54wNJJZmjyyPpNbBISkvkEbX2EEIIKR6Y/ifmmF1g7cw2Wh9JrTUvzBsjo6rSAj4a/Q7PE0IIIYSYUJpC0sQmqZ1r+5B+HXO/tRZJmc6zxTwcbPQbjWOEcxyPiRBCCJGNKyH56KOPYuTIkaiurkZ1dTWOOuoovPPOO0G1LVS0PpJDe7THc987Ah/8/HjU7imMRRIIbnhVK5LciuMnJi3DkJvewZSlW1vLk9IoGYUQQgghJExcCck+ffrgnnvuwYwZMzB9+nSceOKJOPvsszFvnkgqm+hgJgi1PpKKAhwzuCuG9GiP608agvJkAlcfOzDEFrboqigaxe56awEA4FevzDZdH8U2E0IIIWFSSj6SKTcbn3XWWbq///CHP+DRRx/F559/jgMOOEBqw4LE7Prq59pu/T2kR3t8dfspqEglMW+dvGhqJ72lqkFaJKNdXhTQHhKHuUkW3gmEEKLHlZDUkk6n8fLLL6O+vh5HHXWU5XYNDQ1oaGjI/V1TU+O1SmmYfSmUJa2NsxWplpluwkwwqkJesE0QaJumjS+PbosJIYQQIhvXwTZfffUV2rVrh4qKClx77bV49dVXsf/++1tuP378eHTo0CH3r2/fvr4aHBRJw9C2GWGaqoPUkFEUe1GbHSfCGp4QQgiJDK6F5NChQzFr1ixMnToVP/zhD3HFFVdg/vz5ltvfeOON2LlzZ+7f6tWrfTVYDiZR21ohabWXRCEZZWuja7TWyTgdFyGEEEJscT20XV5ejsGDBwMARo8ejS+++AIPPvggHn/8cdPtKyoqUFFR4a+VkjEThBWphGa9uWIMe+7MoDSZbLEXR+kYNQspIYQQEkV855HMZDI6H8hioCyZLwizfpBAOBZJJ6Ju2LMSo16bHfXjJYQQQkQJ2/BUSFxZJG+88Uacdtpp6NevH2pra/HPf/4TEyZMwHvvvRdU+wLhuCHdcNiATjigd4fcsooyZ00d5m2h7v1fMGVLLi+GIjCOx0QIIYTIxpWQ3LRpEy6//HKsX78eHTp0wMiRI/Hee+/h5JNPDqp9gVCWTODla4/WLStPaoe2zfeT6iPptF4tTjFTjG0mhBBCiDdcCcknn3wyqHYUHK1F0tokHWb6nwDL1hQuQxzTn5CQ4iCTUdGYzqCyLOm8MSHEO6Uzsl2ac22bofORjED6HyDa1j19wm6rNd7KIySqFHtWgm//7XMccOt72LGrsdBNIYTEBArJvWijtq1ISFSSTv2RqgbpI1ncnWEYFLleIMSUacu3IZ1R8d+FmwrdFEJITKCQ3Eu5gJAMN9gm2uhntjFfTgiJJnxOCSGyoJDcS9SGtoMMtpE/1zZ7JUIIISRLCblIUkhmiVxCcjX6VkkzirHNZujmD4/LQRFCCCGSoZDci3ZoOxIJySOkJNMZ+4ZQaBFSXPCRJYTIgkJyL1qLZNgv2ekrtoVcozg/fX4mDr3rA+zc3aRbbhWw41VURk2MRq09QbOlrgGvzVyLhuZ0oZtCCCGkiKCQ3EuFJq9aU3PGdJugLJLnPzYlb5mqBhdd7SaP5Ouz12H7ria8PntdIG3JtYk2koLyrUcm42cvzsKf3v+60E0hhJCix8pFLo5QSO5FO7NNY9pKSIabkDzKVjFd1LYugjvCjXZBPI5CnFXbdgEA3p23ocAtIWHAADlCiCwoJPdSlmwViQmrqO2Q2gIE+6L3JPZs2hMX8UgIIYQQd7iaIjHOKIqC608agk21DRjUrZ3FNuG1R0VxWsXiYuigxYYQQghxhkJSw89P3s92fZjpf1rySAbvIxnF8qIGLa6EEEKIORzadkEJ+c56Ji6iMiaHQQghpACUklygkHRBuFMkBmcH81KucR/VZp0X4iJACSkG+LgRQmRBIemGkCfbLhZxpR2C5zBwcVMs9xwhhJBoQCHpglB9JIMsW4JasCoiLkIkLsdBCCGEBAmFpAus0gIFgRqdGRIdKZZ2EkL2woeWkEAppZgKCkkXhJuQPLixbX2pXo9JM5ytKfDrjbUY/84C7NjV6KNNEcAi4TohhBBCWmH6HxeU0AeGZ37x0mwAwItfrMYZI3rh92fsjzblSYe9SFSgjyshhBA30CLpglATkgc4tC3fwpZf4I5dTXhu6io8MWmZ7MpCgYJKDnPW7MBlT07FgvU1hW4K0cD7m5BgKSXDE4WkC8IOtonSkKpdW+zWra/Z478CUrSc8/BnmLR4Cy752+eFbgohhJAA4NC2GyTryBkrt+OTRZtM1wU6RZ+EokWbV2YToRTlaQgj3LSiIrP3PG7f1VTYhhBCCAkECkkXyB7aPu/RyZbrWoa2ZaTpCV4R2dWQStLoXUxQQNvD80MIIXrYy7sgbJ+HoDotKQJV+9umuFTSziLpuxmBEeGmEUIIIZGBQtIFqUS4p0uG0JIl1rxaNssEz1mUhVuU20aIF6L8EUcIKS4oJF1QWRbe6ZIVtW1WhrYTkTFcb2fhtLVI+q86MKLsv0kIISTahJl3utBQSLog9ITkMaCMPpJFBfUzIYQQN7CXjyiqKmtO7PwypFg6NeXa+kgWadQ2IXGGTx4hRBYUkhElai96u/bY6cGk4ATlUdOUEWsOIYQQEkkoJCOKLGuduY+k3KhtO+IwtB01kUsIISTalI6HJIVkpImSgLF7KOIZbKP5HemWEkIIIYWDQtIlf7n4ELSvCD6PuwpZCcnNy3Zdjss6sgin/4mSaiaEEEKIEBSSLjnroN6YfespgdcTF11VvAnJxYKJCClGeE8TEjAlNLZNIemBhGAAiT9UOQnJTWyJYXYiosE2hBQDdHMghBA9FJIRRVpCcodCvIpKnQ+hTRkJm9ybke6UI9w0QgghJCpQSAbE/r2qfe0fpI7xIuCCtmJGWbfRf5PEjUh/xBFCigoKyYA4bEAn32WEI2C81aFLSG5ThtcgnUIT4aYFCkUzIYT4RykhJ0kKyYDwO52irKFt88IlFxdz7RH34yOEEEK8QiEZEH6n5ZY19BSUCBItVtTCFWWxFuGmEUIIIQWFQjIg/Jq11ZZEkjnmrNmB0x+chEmLN/trmL5YYQFnO0TtpzERJcrClhC/8P4mhMiCQjIg/Ga9MQ5tX/n0F5i/vgaXPTnNYT8Vf/rga7w2c23L37GUeeFSSp1uCR0qIYQEht9RyWIi+ClaShTZQ9vb6huF9pu1egf+/NFiAMA5h+xjPrONYOoeLbZTJHpUWlEWaBTghBBCiDO0SAaEXf5EUbwItLqGZt/1mpHXEtVmnXYzCUPnhYaikhBCCDGHQjIgChW1XVmWzP1uTmdMy4iKMIpKO8zwYrUlpFjgLU0IkQWFZEAUyj+iMtUqJPc0Zxy39yrmRAN2oiwWST4UzYQQQtxAIRkQfnWkqnrr1MtTrZd0T1PadHhcvliIh4+kVvzrhHLoLSGEEFLMaDVA3Cd6oJAMCBnBNn6tebsb0/4aocH4IMjODxn3B43Eg9jcprE5EEJIoaGQDAi/wTZeLZJa8bmnKW3hI6mvxy9ey4hyV6bSSZIQQogE4t6FUEgGhAwXSb833+4meRZJO4p1Pm0j2mt20h8/yf0uokMghBBCQoVCMij8WiS97qfZcU9TxiKPZOtCiqRWtJH2DQKBSoQQQogZVj73cYRCMiD8z2zjP4Bld1M6sDtYPGrbZl2RmCuLpJlSYJR9acCrTAiRBYVkQPj2kYQ3oaUVAlbBNrKFUbEIQq9QXBFCCPFK3PtICsmAkJH+x+9+LcE29gXJqMd+O+sN4/1oEUIIIfGHQjIg/Cck928Hi0KwjXAZEVCVVpcsCm2TQSajYmPNnkI3g0SAuNzThEQVRdOjxP1xo5AMCClTJPq8+9IZ1bGMoIdt4xLRHQeu++eXOOLuj/DfhRstt+E1IYQQ4gYKyYDwa5HcUtfoaT9d+kOBbWQQd/Hh9fCa0+4jvxubM1i5td5jjfa8M3cDAOCxT5YFUj4hhJB84t5HUkgGhN9gm2v/bwY2eBiG1FkYVXN7Y82eJu8Nc6ozf6W3dQXA6pJ5eQnMW7cTw25+Fw98+LWr/S594nN8474J+HjRJveVEkIIiRxxD9ikkAwIGQnJ/ZJRzYNdfvXKnNY/hINmDH8L7lhMD5Ai8ard+eZ8NGdUPPDhYlf7fbFiOwDg+amrpLWFyCPulgVCCHELhWRAaK1b+/eqRt/ObUKpVz+zn3mvt2B9jeRKve5WHL1yIdrpP1jLpmybdcVxRYhf4p6OhJBCo0tIHvPHjUIyILRD22VJBR3alIVSr2r4HdT9K+KLadzOtrwoSJgomJEJIYSQIoJCMgSSCUXqsKkdWktDRkCbRUC+AYj4F1sB2hbk/RKktZMQQkhpQSEZEFqLZCqZCK3z1k9d6Jz+R7xcm8TinqdI9N6eMClEMyn2SJAUyaNHCCkCKCQDQjvXdioRlj3SPaqqoimdQa2PSO5IDEtLwO4azV69A9f8fTqWbwkmNU9eWwL1kbQuvFjEPSGERJlS8pFMFboBcUWbkDyZUEIzMemDbcRE3pj7JmDtjt2YdcvJ6FhVbrqNUXyIPhderZVRQlVVnP3wZwCAZZvr8NEvxzjuE91PB0IIIUQetEh65P+uPgJnjOxluV4J0SL5xux1+NP7i/b6R2p9JMWk2toduwG0pp7JovW3VKFa5p+M+9eW9vhWbdsVSp30kSRBEvdnlpAoEZdROytokfTIsUO64tghXfHWnLdM12v76qB9JH/y/EwAwJGDuqA82fptoOb+zxrtaqcm/uDvM0x3tM85budbGa2HS+QaRazJhBBCIknpfLHTIimJK48egPsvOCj3t3ZoOywfya2GaRVbhrbFMQopo2iasmyrt4YJEAV9ZnWVjCmVQiFIH0nbsqNwJQghJD7E3QBBISmJVELBUYO65P7WdtbJhKITlkFhzBtpNbRt5QTsZlpHraXRzrIo6iMZ9weNkCjBx40QIgsKSUkYNZjWuhVm1HbeVIYmPUbSQjDmWSQdyokbInNthzUcXzqDIsVF3H2dCCHyiftbg0IyIHTpf8LMI6k6WwqTCSshaYjMFhRN9j6SNut0fpbRfdQKM0UipSQhhMSBqMUDyIZCMiASeT6S4QsDq/Q/KY2Q1A5/W+hLgYo87qctokieM9FmUgcSQggpBSgkg8LgIylTR575l0n45Uuz85arqmrwkTTfP2EhJN3kihQWfsWiEGF9iYy5OcMgSB3KHJelSdytIoRECV0sQuGaEQoUkgGhS/8j2Udy7toa/OvLNabrjMPFZn2H1iKZ1qhNOyua1xQ/dkR5OLtUodYoDSgqCSGyoJAMiIRuZpsw59rW+kiaC8C2Fa3pQ9OZ1uVO6X/09bjfzg52bHoCnSKRBklCAkFVVVz3zy/x8xdnFbopJELEvXujkJSEMThC+2dZMkQfSYEI48qyZO53OtOqJO3S/wTS+gIMGdthFeBSCJFLrUdkE4VnLO5srGnAW3PW49WZa1HX0Fzo5hASChSSkjB2/Pl5JENogzHqGuYiKKMZzk5rVhubqLNu2tRra5Esos4rSuKNUduEFB+i09KS+KN7g8f8tqCQDAhjHkk3yb69Ygy2sXqnpTUrtKIy4TFs2+sz4nXGmLlrd3qs0RsxfwcQQiTBdwUpRSgkA0JvkQwzj6Tmt0WwjTbARhdsY1OW9/YI5qJ0UdcP/jHDeSMvCCQkDwvaI4lsKHLChc8wyRL3wFIKSYlYiaZUMrxXivaGffjjpdjdlM7fRtPMtEeVJJL43LkMzW8XD9quxuj7HnFkOp7EZeQyLscRNRg0SLKUUh9AISkL4xSJxoTkAdxVIi+tBz9anLfMyiJpLO1PH3ytqcumHTb1B/FaDftVHbevSbt7MV5HSrRQ5IRLKQkJYk/cHz0KyYDQvkOSAc21bZZw3HjDLt1Ul7eN1gqpFZIZQ4F/nbjMsm4Zz4UxVZHwfgE9lJF670eqMYQQEeIuGIg34n5bUEgGhPZrNBVQ1LZZhKBxSaM2UWR2P614VK0tkqLYvTynLd+Gmj1NzmV4rDsMCuMjGZySjKJG/WD+Rlz818+xdsfuQjeFEEKICygkA0IrBJLJREAWSRMhaVi2bHN93jZWFskgBNM7czfgnIc/w7LNdfh6Y62UMsMeoiuEyC21YbFr/j4dU5Ztxe9f/arQTYktUf5YiyO0TpY2Wg0Qd7eSlPMmxC9lgflImiwT2M/SR9LmZrf3E7Svddnmepz4x08AAHNvH4d2e2fW0VXn4kGL9yMZPLZTYRb4hbdtl7P1mvgnbn6/hJDCQYukJIxDkXkJyQOoM23mJCmALmrbJthGtAw32mNLbYN5eS7qttr4zjfn45EJSwAAizbU4sUvVrkSRlZbMv1PiMT8y52UDryTSZa43wu0SAaEVgikkuH5SIrcsVrxqPWhtOvDzaTwmu27cOXTX6BXh0rnSk3waJA0PcTFG2vx5KfLAQA/GjMY4x6YCKBlOsizD97HU/vsayxeSlakljjU6MHDc0xKEVokJaEoQFV5qy5PJVtPbTKRQBDdt9EgqapiQ1ZaH8mVW3dpynM3tH3XmwuwZFMdJi3eItBaeZhZGa3mtXU1C06EOoFS85HMEqFLEGsoeIKn0G4ipLB4zUpSjNAiKZHObcsx/lsjkEooqCxrFZJlAUVtq6qa97ISuWGNaX5y+7qsf09zfrJzN+iSmvuUEFYiWM7sPP7LcEuQUdt2xPx95xueH0II0UOLpE+GdG8HADhzZC8AwMWH98MFh/bVCYHyVFBR295EjtVsNm6/oL0ckwpg8tItWLlVH03uV6w1p+WI46hQqhZJEhwMsAkXnm2SJe7PHi2SPnnzp8diW30jenVoo1uuFQLVbcoC85E03p4igsxqG7dizksk+rx1O/Hjf84EAEz69Qmu9wfMX9Bep3p0KhfwJnILZVEsduI+BBQVeJqDIe6CgYhTSu8yWiR9UpFK5olIQG+tq64sC0RYZDImQ9s+yrN7Ccp6KL6y8Fn0GjGexWsEu1B9MescgkhFRQjRU0pCgjgQ83uBFskQaF+ZcrRI7tOxjetZPTKqIfIZ+cLSDa4tkh7qsBLU7qK28ze20pFhvsxVVcUjE5ZieK/2vssqlNYrdOcXN9EeJQp9bUsOnm9SItAiGRANza1pdUSGtr2Ij4yq6jqHn784G7NW73BdTmt57rb3InashIIbAWHWIVoHELkp11/AzsTFW3Dfe4vw3Wemm65fvW0XttU3CrYmHlMk0vgZTSgqg4HnlWRRLX7HEVokA6JWk46mbXnScTjRy3Bji4+k/hZ9ZMJS1+VkcW/NlKcSVLVleDqZcC7TrJXNFjP0hPliX29jUd5UuwfH3fsxAGDFPWc4lhUXAaYg/i9RQsygdZ1kifsHBi2SAVG7p3WqN0Vx9pAU0E95qB6jti3Lc7m9X7GjbftfJy7DyX/6xLOvY7A+ku4xnpt562qktEUGcRGphESNmOsFQkyhkAwIo65xtEh6sO4Zh7b9UugEusu21GOdiJ+oQ7CN18MotSkS56zZgRPvn4AP528MoTZCSou4W6GIOHG3TnNoOyDOG7UP/jNzLU4c3h2AszBIeJD06Uz+0LYfwgi20TbXrO3NApZF82AbbXJzzbZ8m5vQcuWuevoLbK1vxPf+bu7TWQh4uYKD5zZ4+L4hWUrpVqCQDIiq8hRe+eHRub+dhhO9+UgWdmg7EcAYqdcharvpHf1SiK/JMIaf6xvzp5WU3REqilJab9QiIe4WkijAM0yyxP0VyKHtkHD2kXSvHFSThOR+cCvGPIkdzT5m1a3aVp+/0IDZftqZbVQL66SXcl0XYoHbUxVkQvPsdYv7yy0QeM6IDRwNIaUIhWRIOPtIuqfFIlnAoe0AtM53n5nueExuZrYp1nd5GBbJME6N28Mo1utVDNAKSUiYeDNoFCMUkiGh7VB/cfJ+eeu9RG2bTZHoh1BudtX0pw6n0W0zoakLttFV5/+o4v4SCBJGiEcTCvZgUAXeb4TEDQrJsNB0qCcM7Z632svQdphR22brghp+dfKTNLVISonaDu7V79YHNhT9ZXK4he78Cl2/GRyiJIT4Ie7vEArJkNCKLlNN4cUimYHUnjeUfOQC+3gJnLHaR8bzK+MlEKUXCY2EpUmEbsEYU5jJEEj00F7//8xaV7iGhACFZEg4GaSSkma28YPbsvwKEitx5WiRdMojKdk3Ja79AX3mxKAgIKLwXiFm3PfeIizbXFfoZgQGhWRIOIkukakBjcgf2na3zkvKIhFEckkasRradnN+guwEXA9th+BcKPN4v95Yi/veW4gazYxOgHv3hyhZbglxi2z/bBIfNtU2FLoJgeFKSI4fPx6HHXYY2rdvj+7du+Occ87BokWLgmpbrNDqAjON4DmPpI82mZVnRRCvRMtgGw9CMtA8kjHrD2xvNY/Hesr/TsTDHy/FXW/O91ZAhInL5Y/LcUQZVa8kCSkJXAnJTz75BNdddx0+//xzfPDBB2hqasIpp5yC+nrn3H+ljpNlxttc26rk9D/hDm1bkU3l8/DHS3D+o5OxuzHtuI8XK6ZsZLagWNP/zFmzU7+ADpmkhKAVkmSJmwHCDlcz27z77ru6v5955hl0794dM2bMwPHHHy+1YXFDZ5E06V29RW3LFQN2ZZkPbfurxOpBy1ok73uvxdr94herHIu1smI+P20V1mzfhaevPAyppP13U4D5yCOlp7L3XxDDyEbLepSOWwYUCsQOpv8hVsTtXajFl4/kzp0t1ofOnTtbbtPQ0ICamhrdv1LEeYpE92WmMyGm/zF5LXp5MB6fuMxxG6N1saE547hPWrOJ8TAmLd6CSYu3CLXPjEL47QU5s02QFGer7YmL36ZuxqeYHFPU4GklpYhnIZnJZPCzn/0MxxxzDA488EDL7caPH48OHTrk/vXt29drlUWOffofT8Y92VHboQfbeIvaNt1HNy1i/v6NaWcxKpMwgmX8YnaWZfeDRXAaCJGGLmMERWVJU0qjF56F5HXXXYe5c+fihRdesN3uxhtvxM6dO3P/Vq9e7bXKoiaoYBupeSRdbh+URvCUR9JBfAq11aKIQrwOKMCiQ1y6g7gcR5SheCSliCsfySw//vGP8eabb2LixIno06eP7bYVFRWoqKjw1Lg44SWYxgnZUyTaCTjTNT6Pyao6o0VS5BibJcxsY0kBOocgdWRWpAbR6fkVwH7aNPHrzXhn7gbcfOZwVJV7erWVDBQ8wVNKFiniTDGMUnnF1dtWVVX85Cc/wauvvooJEyZg4MCBQbUrduhmtjGRCV7usTDzSJqtLNQUiWYEmf5HNqqqOr5UivWdY2x3mL6elz81DQDQtV05fnnKUGnlFtGtRQqM1xy2JH6U0vV3JSSvu+46/POf/8R//vMftG/fHhs2bAAAdOjQAW3atAmkgXHBSRh4idpWVblfva6HtgPSCGkPT2Bz2v9sNlbnUsY51p4rVTU/d2EFQNhdtzgEYazdvrvQTYgkMbi0kYdWSFKKuPKRfPTRR7Fz506MGTMGvXr1yv178cUXg2pfbND23bKCbcKN2s4nmFCbfIukSD1ai6RsMSRanPB2PvePMkYLpNuPDRkdsXzPhhhcGBIKTP9DShHXQ9vEG0EMZcr2kXQftS2xcg0ZDwHWQQ5texlqt6PlOco/edpagvSnKdbUQqLwPUUIKTTGt1CxuiuJwLm2C4DZ/eQ5IXlIM9sEYZWxqq7ZoCRdB9tIbk9Yk+bEQQDl+0i6Q8YpCPJ6FfUlorUscLTnNQ7PMyEiUEiGhFP6Hy9GIlVysI3bDjgoy5YX62KQ72zpQ+UCy/2c2Wc+W44LH5+CuoZm8w0C/DKOwke39KHtAuuBpnQGN7w8G6/OXFPYhhBHKB6JFVF4NwYFhWRIOIkuL6KsxSLptUX5uAza9p/qxTIhuc9yJb/LZQyba6+vVXGqJCV52xvzMW35Njz96XLvhUgizikvwuJfM9bglRlr8PMXZxe6KcQBvUWyYM0gESTOtwOFZEjo+1OJ6X+kzmzjLtgmqBelb59Ej7tb7SZ7qFQkOlyGtXdXU9p0uV3JhX7Zyag/blahrfWNUsrhrCvBw/NKspTSvUAhGRJOssBLwvIw80iarfNrqRNNSC5Ymq+22OHlOG3FmohFUgKW9citJnLE/fhIlOHdR8yJ89gMp38ICaOPpKLoO3pvQ9uSo7ZdlubXUmcl0NzkkdzdmMZbX63HlrpWq43Xs2JlyZIh8EQszrr7oVjfOoaGuw+2kZD+p0Dpn4IibhbWOMOE5KQUoUUyJAJJ/5OR28n8deIyPDFpmek6M3Hmt27LKGkXCnX8Owtww8uz8cH8jb7aYodR8H7v2enYUtfgqgyRUxVWvsJAp1/U/E5nVNRaBfwQYWQ94hQ2wcNTTLKUUv5ZCsmQUCx+2y1zIqOq+GrtTo8tymdLXSPuemsB6s06/4gObb/91Qbhcr1iPM4PF2zE3W8v8FyeyNB2oQySxrbt3N2El6avxs7dTa7Luv/9RZJa5Q7pLgIl1CEQf+gTkvO+IaUBhWRY6Ia2zYJtvE2ReP0Ls3w0yhzRoeWg8vU1uyjYzCrqOY+kxXKz5myudWeR1E2RaBlsY769bNzcaz95fiZ+/cocXP/CTMGyW38/OmGp26ZJIW6WN1mHo4sopsgJBLohECuK1l1JAArJkMibOs6w/oRh3V2XGdRsLmb3u2nUts96rDozN8cluqWftpp1DmYJ5EU752xxE7/ejGcnrzCtJyqzz0z8ejMAYMKizQVuiTiynwtqAyIK0/+QHCV0/RlsExK6YBvDuneuPw7De1W7LtNNUIq2HV5ecGZiym+HbWV4NA5t20eTi/lu+rEUmLUz6SXMPtuWvf+9/KlpAIAD96nG6P6dQ5tBJywfSS8EcQpWbd2Ft+eux3eO7I92FXzlkeCgeCSlCN+qIWHsYBWNovMiIgFvaXISiuJJgJrt4j/Yxr9F0uwUmO0tcqqsp0jMX+EkmOymCjQe97odezC6P3QNX7tjN5rSGZQlvQ8aeBm+9DvkGYUE5MYjOO3BiahvTGPFlnrcc95IqWWHgbxgG+aRDBpdrs4CtoOQMOHQdkhoh0Jl9bX1DeYJp+3b4byN6Asw43MGGitx15zWr7A7X6Ji1k+Sc7Nd/Qgmq5ZoO6FXZ67FxX/93HMddkRA6wWK8Zaob2x5Tj5fttVbeX4b5BP6MxYRuvQ/vG5ES3xfvBSSIWFnofLKrkb3qVVEfO9MrY8m2/n3RRPLI7m70Vowm/puSo4wN+sQfBgKberR/z195XZX+zf5nVsyCkjpe+PVgVOPFA+8VCRLKd0LFJIhoU//I+fLpM5Djj4ha5TpMHb+Mv8JyS2WG1Y8+NFiyzJEO1k/nbH50LYPi6RV+h/PJQJPfrocQ256B58t2eKjFP9E4Zvb6r7yHM0fEyUXj6MoHni+SalAIRkWhqFtGcOLpvkexZthiWny8RATkrvx4TRP/5O/zNfQtomhL+HnybESOj7O551vzgcA3PDybM9ltLTB1+6+72t2vuHA8xwMMfnmIMQVFJIhkRdsI8F2U28z5GuFWdoaI6Kay3/UtsXQtps8koIL/bTVbF+n82g717Zl2iM3rRKo16K8KFgNg0T6FIlSS4tI/VQ8gaALtinBU7xscx1Wbq0vdDMigfE9FGffdArJkHBzE3VvXyGUXsaLRVJESJp1xE9/tiJvmV/hI2NmG1lD23biw3Ro2yyPpF0d2oTkFtu9Oy9/lh7inuzpXbihBlc/80VB2yIFSYqkFIVN2JTyOa5vaMaJf/wE37hvgq8RIFJ8MP1PSORZIO0ike1X5/A0tC2wjXDUdkDpf9xZJM2Gtk3KlJxH0kcaSdP2Ld5Yi5tfm+u90L0odglLzbaRjF9LuwxrYvZ6XfT4556mdjQSR3EQw0OKBKrNX3FnW31j7ndTOoNkIlnA1pAwoUUyJHT9uwI020TYqqqYBdNL+h8hH8kQAlgAuykJxQsOYxjebN+kn/Q/JuWt3r7Lc3nWFckvspiQISKjQBCXMY7iOAowVyfJUkqXn0IyJIyyY9wBPQEAI/bpYLG9wNC2h/Q/CQFTmmjeOv8WSfPlbubalhVhbj97Tv4y2Va9KCTydoup9TACwTZWVk0ZHXsxR3AzH2Xw8AwTK4rvDS8OhWRIaAWcoij4zanDcOtZ++Of1xxhsrWKqgrnYYHGZvd5A4VuZsG34ZJNda7r12IlRFVVvMM26xxnr9lhUqbsYBuXhaimP3P4sXBqESnGPhBIjK/W7MQRd3+EV2asES67aImhOqCoDAiH5zzOFPE3FvEJhWRIlCU1QhLAgK5tcdUxA9G+ssx0+8MGdHYs05Xlbi8yo7Y31Ta4rl+L5dB2RsVPnp8pVIZZW3/wjxl5y/zNbOM+atsOsxeun/LE6pT7lv/pCzOxqbbBd7qhILDO01mcPV0QHTQ7/WAo1nuMyKeUnjEKyZAodzEViqoCt3/zAHSqMheZWbyII695JIPAeq5t4M05632VYVambTl2+2byLZAvTl/tKiG8qvttIkwlPYlW11d3mgTugXU7duOav0+3XG9lDY/CCL3/GZf0FFocSKu/hDq2QqG99UpJSJDShkIyJMpSrafaqbNVAfTu2AbTf3+y7XZ2ATtWiPjihfUClBFsI7qlH4ucqqqm6Zge+XiJ5T7G86zqlWQe0oa2NSrRoUpbfvOvOfhg/kbL9VbnU9asTaSVQCyS8oskoHgk1hSjH7woFJIh4c4i2fI2csolWbPHSx5Jgfpdl+oNy6AIV2WIbedvaNv8JeB1aN+sJSJBUH7Qnuuv1uzEM58tt9iw5T9rt++2L09Ww4zlSgmI8V9GkOUVipgcRqRxGnkoFeLyzBAxKCRDolxrkSyg1Uak7rAiUy192QKo3zlq23qDjKqaWgzdNNNpxgtZOtJyaFvze/GmOtz2xnxf9US5oyjlDlyUKF+/YqaYo/qJXErpTqCQDImKlAuLZIDtELJIhvQEWIk72T5ufsvMqObWYTeCxal6WcE2VqVIt9JZHHsURm/kH2thCSSPZMGPKp7oLJIlfIp5f5UWFJIhUZYU95EMElEfyTC+rO2CbWST8VGoqqrm18xjkabBNkFHbbttrJMfb0C3h107VVXF+/M2YI1D8va4dWGyzrW2nG11jdYbEimUtJAs4WO3IgLf2IFBIRkS+qHtwiEatR3Gi0BGsI0ovqK2LYJtXPlyan+7SP/jRwB7QVRwWm0VpB5+d+4GfP8fM3Ds/3xsv6Gly4S3euM4XPnyjDXYuSseM/9EiRjeKsQjcXxvWEEhGRJu0/8Ehci0caFFbQc4A4kR30Pbpj6S+jLtatBNnWay3kqAuc0VamVxdnv4XvWg/7m2rdd9vmyrWBkxs0kGdTxfb6oNpNzSRvucx+s+dEPpHrk1cT4nFJIhoU3/4zxsGNwtVysQ6a0inJve6jCDsMLJnmsbAOauq/FUntn1tWqeU7T59BXbcNIfJ+T+1t5aQd5HUf7YDrJtET5sR4zCJuBEASVJlJ+LoNEHFJbwibAgzueEQjIkXFkkJdQ3sGtbz/uqqhrKTW8dbBNEXfaFOs21bbZ6yaY6fLlqu8U+1tZKs7qsrBfNGftcoRf99XMs3VzfukAkIbkUwn8piuZhk92ygr/+A2pAnPPaFQoG25izubYh1kLKjFI6WgrJkKgIOf2PH2tDxkI4ycZKPBXCR9KOdMZaWE/8erP3gjV4tUiK5scU9n0UPE9W2/nVJnLySJamJdYtQQd4lSJxuj/cYjUZwttfrcdhf/gQv3t1bviNihBxvjUoJEOizIVFUsYd56+TEG9A+8qU51qsNNAzk1d4LtO6LvPKbvz3Vzj3kc9sLX8Z1b23U972Hq+pax9Ji+VxS4ljh3SLZIEPVlb1xuPg0LZ8Stsv0vzY739vEQDg+WmrwmwOCRHvKoC4otzFFIkycJoVx46W9D9i2/oRrGEOdVj5XWZfbp8tsQ7ksDsfVtblCYv0lkqnhOSWEeyygm1cleJMlIep4uYjGdS5jpJFctGGWsxduxPfGrVPUQ+5c67tFkr52K2I8zmhkAyJco8JyRXF2w3oS+BB/Mvar2ANCyc9ZjecnpHsM2p2bq3Kd2uR1JfpXL4VTp15lN+JVm3znP4n0kcbD8Y9MBEA0LYihVMP7Fng1ninlO8U3Ud1KZ+ILCV0Dji0HRJlidZT3Zx2CvxoXZ/yKNQSPq6sO4ukj3pCfNL8BNt4Gdq2K9+NRdLtHOGWQ9uuSnHG2kcyOGuScNGBmiSLt3cwtjxKFsksc9fuLHQTfKFP81W894oXSu143RPf80OLZEiUpVpf2k1p+0hc7e2WSiTQlE67rs8s76Eobl4I/oa2Pe/qGuMQcX1DM+5/f5HYvi0mWlN0h29zQDoh6bBei/s8kua/wzrXUZAmMQhQ11cfUP1+RhOCgmIkHvA6lha0SIZERSqZ+11VnrTZUo9Xi6Qfy5BDxhkdRSMkDXU9+NFiPP3ZChf7mzfWy9GbpwYyLz/t5mLk1eNhH+GyC5D+R/Bsx85HUlY5hhMTQYNkIKm/woQ+ki2U8rFnMYrpOJ8TWiRDIplQ8Ox3D8fuxjS6tKsQ3y/p7W3vy3fRxRSJ/uoJD6MQXLqpztX+aZ9vAdXid26ZJIukUAMCLM5/+h//DaU1RIwIGiSLvrPlvUdKEVokQ+Qb+3UTciTXvky9WiR9DW27eBf6EQ5B5IsMqi4fhkEA9gJJgbUwc/KnzS/LKmq7+PP/iN5rQaY68lL2Rws24jtPTMX6nbu91R/YuY6ekix2IebkwlIqlPKxWxHnc0IhGXG8Wvx8W4ZCiNoO88lyFoL2jfFrkdTVZChKNVmWq9eHj6RdnY7lOKy3tEi6qyYQgh3adl/41c9Ox6dLtuCmAidkLgprXzG00QY/mRJIvCily8+h7Qii7axSHsOv/fgubt/ViAmLdgVeTyEtksZmOzXFStAJW8ls/mqpP4D0P7Y1Wuyztx1Ox2XVXjvfXFVVQ0kr5KYMoTZJuk231jV42i84K130errotYh4gSI6nzifElokI4huaLsAPpKXPTkNP3ruS6Fti8QgKdWi6AWjE772RetFWu3c3YS7317gon65x++ltLAugdWxGpf/d+FGjLrzA3y8cJOLsn20y/uugRDFjs1tAv6o4eXjLS5wWF+cDTv34MT7J+CJScsK3RQpUEhGEH36H695JMMZZPQjWMO0SEbpzWZsik12IcvO/g9vzcdfJ9q/hHSZiQTbJkpQEeF25cq+o7/7zHRs39WEq575wr5Nks6e54Tohv1ue30eLn3ic9duD8VAsR9RKVviSvfIxdDeG/e/vwjLttTjrrfEjQFRhkIy4ngd2vZoyHRNodL/nP7gJGzYuUd4+1BFqymt9ZslfLdqnlWr562rMV2uHab1GyRih5W4srsb/HayhQq2kVW2LEH6zOQV+GzJVkxZaj2tp1h7okfBH1OfBPnMFRO6Y4+C43QBsLv+jc0+ozcjBoVkFNHcgF4tfmElGy7UXNvz19dg/DviX3NBGW9E83WaBdjkyoC1yLAUmBbLrWe2ETsB4r6UFvXbnI6w+lXZPoWyBIFsYdHsM5VAFIVOsUdtF3vz/WA5q08JnxMtcT4NFJIRx6uPZJBT1WnxNRWjz7p3N4rP+BOlhzgvUW3u/0y2jWBvf8cb89Hg4Yta5FBkCImoJiSX3S63z3gEb6U8iqGNdujv3yI/GJeU1tH6I27nikIyguijtsPPIxlWPX4d691U7TjXtq+WOJdhHPIyn93Gf12W50Tim+upz5bLKywAovqS9tquoD4momj9i+KHkxs4s81eOLRdUjPbUEhGEO0NN7p/J09l+LEUusGP5dPvcyU6ZR5g1kEphvU+G+NYv/533tC2Sx9J6/Q77sqxws25Ne5phYhwsQ22EXYjCC5C3U/ZXveVVX9euRHs2CLYJFcUe/v9EMX7iYQDhWTE+cXJQz3t58d30Q2+pkj0+eJpTIsPr/qdmcYKL6fZbGjbrY+kUD0erCNRDVQBxA0bluLbX/UFQ5aVqxgsJFFskxtKOwWOavKLZNE+f8VueTdCIRlBtLdYm/Ik9unYxnUZYQnJQuaR/K+b/H8FfrXpXyJuorbdtVtrSZy5ejsO/8OHeHPOutCOP6Tbzh7ZEeoxe+lHmUI/p34p9vb7gcP6ekrpHFBIxpS4R227JciUe3PX7sRlT07FvLXmKXkAs6jt1gV2c2277Ze0l2Pmqh3YVNuAH/9zZmgvNfv0P+G0wdodQELZkizEnsvQleevwCiKnmLvfCmmiCUxvh8oJCOIXQfxu9OHAQDuPW+kbRnhWSS91/PevA0SW2JPUKJVgYLzHp2MSYu34MXpqz2VocK6fbJaHYV3WBSFixPvz9uAE+6fkPt7i8dpDgFvuUzrGpp1Oef8+Wja/x0Fij3HepE33xf6ePVSPhPOxO3scK7tCGJ3k33/+EG4/KgB2NOUxq//Zb1dMqRPBD+Wzy9WbJfYEnucOig/Lz6RVDj5UdtiZUvLI+lSNXj9PrDNIynQBNtNhBOSy3tNf/8fM3R/b61vxJa6BnRtVyGtDivqG5px4K3v6ZbFrQPKp8iPUJtLMYpKPUBojdVjPAVxPiW0SEYQp4ewsizpGMEalkUyEj5xAsia2ea0A3t62s8yWS/sh7YLPT2fW+yivcOK0g/aqrV4Y52n/dw2a8H6fFeJOM31bUYQ9+nGmj345OvNoQi7oGrYubsJD3z4NZZt9nbvERIkFJJFipMhUCRVilYU/fa0YZ7aEZYvpl+MfUgQFjevbVFz/+e8bes+VibJ6F6PsCw0QQ+r1Tc0e9pPxvHLPLYoWo2CaNOR4z/CFU9Nw3vzNsov3AaZh3L76/PwwIeLMe6BiRJLlYsuoLCA7YgqUXzeZEEhWaQ4WRzdDm1XpLzdCmFZPv3imJBc8CE3CnRv6X/sg2+M27qhEFejWHIbyhJhXq3bUetHoujHJmvkQEu2yM+WbJFetlVdxt9+mb6yxQ2oKR29a5ZFf+zRbWckiNnpoZAsUpwEjMiMMzKe9WIRkrLea16P1/iSzYvatrRIWghMl8cThfyQYb07g+7DPIsdyVHjbosrhs49+i20p5RmISL2iBoL4gCFZJFiJmiOHtSldb3gkPPFh/dDrw6VOG90H4/t8LRb6MiydMg4XDMnbNcz21gst57ZRpavZX452iV+g23spISohg9eSHrbz7XwM1sm8diiqCuj2CY3lHLkcpSDbWr2NPnKuCCDqJ0TmVBIFgFmHajZshtPG577LWI5U6Fi/LdG4LPfnIjqyjJPbStWH8m89YLlGA9XfLYVfQ2mfpISsI7allN+2kRFaUW67W0X0os0K3aDMpabnQMRZFirZA79RrFfC1J8NYeQWygoseB9ytLwCOra3fnmfJz7yGe6NFhuGXnb+zj0rg9R59G/WTZx+8igkCxSzISidpEbgSdqvZS9b5i46YAf+85oHLVvF9N1coa2DVY82IgMl0PelvW72tqatJlFUnho218r3E6RGNSdGZaPpFn7daLdZ/2RHOoOsEnPT1uFrQFbpVTLP+JPULfTk58ux8xVO/DBfP/BUiu31ktokSiqzV/xgkKyCLjmuH0BACfv3yO3zKwT0WocEb1jfPC9aMI4+kieOKw7UkmL45JyuPkvGOshbHevH6toffd5JM3LMbPG6f09bdL/CDQhitrGiGchKeHYgpozPioEEWyj5aXpawItnz6SLQRxGpo93vwZzTurWPqrYoMJyYuAy4/qj8MHdsbg7u1yy8weCO0yoWAbw9+Korh+A1jprajhpoNSFGshZTzvoqNleRZJQ3vcJh6X4W/nBVMhGbFgm6DFSDokMWd2FPEf2o4Pso5lT1Maq7ftllRa6aF9ZgopJHW5hON0o4MWyaJAURQM71WNMk1OH+PzcPOZ+7se2i4li6SxAza22ijsvnfsQNNy8vYT7C5Uw2/j0LZVt2P5wrFYXggfSaEGQMxaY7dFVG4170Pb/i+CnxKKYYrEKLbJDUEEnDz40WI5BQWMPqOAdxcM2aR1QjK8eu2uf7Hf50YoJIsUo8Xs6mMH6kSdSELyvDI9PPLF4iPp9rk9fr9umPq7k/D7M4brlhuFs5cXgnGKRLsizNat3rYLy7aY+/pYX3Y5by7HYBubfcN+dxqfAVkv74znYBt32zv5SMasLwJQ/McUxBD0l3tzSBYTftJUyUY7Iu6lX5RFoc9DkFBIxgitprN6XB6+ZJTl/l5egkWiI12lbMkeUo/qyjzLbsLwxIiKCv2whvjwsNm2J/5xglCdIuW7xc/QdtgEdWuaBRyJIMVHUurJjt6Fi2QAkAusrHKlQFSPN1Mgi2QpQSEZK7QWSfMtzhjZS/OXmJ+eHcWT/sfbS84kNl73l7CPpM0C+7m287Gb3SLoNCGmUduC+26vb/RVt+ixRTWPpFvMqvF3bP6f96CJYpvcEETzi+WUWFkhC91D6Ie2C9iaGH9kUEjGCL1F0v0D4+XWLuRQgRu8WnKMx2fUzV5eCKphPxXug20ssUxILqUYC4ukZmjb5n44+X/DnSc4qFvT+9C2/84j/sE2UWyVOIEk5S6SU+LSnTs0ChW1XejjDhMKyRihKM4WSS35zvfxHdrWHlrNnia8b5OTTHsejcfnJWpbNSSONDvN1h2onNeRa/88i+tq7iOp2c9dNXnY3YOifYD17D5yKORc237ESTFY+4qhjXYEIYSLUVxHyUVB+84qrEEyOudENhSSMcKvqPNymxdP1Hbr78c/WSq+o+H4jIfr5YWpQhWOoH3o4yW49T9zfb+YZb3ETKO2I/p+DGqY3/vMNu62dwq28YtIUR8v2oTLnpyKdTvCST8TIf3hiSCCTIrlnGjfUSf+8RN8tWYngMIPbYfliuKGYrmmolBIxghtxyny8BrvZS83d7EISe1LrmZ3/jRZVsduPDrj3yIde4tBUt/D5J17i33nrq3Bs1NWYuGGWsd6zNqXRVYia8eE5AW6HUyFdlBD2yH1Ao4+kj6bIfJxctXTX2DS4i343atf+atMkDhZbaJklQsD49Fe/8JM0+VhE9bzakQ0V3AcYELyGOF2ZhsZFIuQdHqZWOU9Mx6eMd2R0Gwthu0WbazFl6u2G8qxL6hecI5Yq8vhdVYII36mSBTBrijhKRJl+Zta4NXCIUMk+ekURT9ezNgS8NSCWYK2HhXJ60pHseoPr9kNZKP9+I1Ik2IHhWSM0IockSAYGV/MySKxaXs9VOPwqBcfyUUbajF/fU3u79vfmO+6Hc0ZFQs31ODtOetd7wvYR3q7aodJOaG9mzXnXlVV3T1udn1FZnfygtehbRkiKYrDdDIJuqMPvnx9EF0pITqqEzZen1fZxFnEUkjGiEI8sHGxSFqRH2yj/1tEjJ/+50mO2zgVk86oOPUB53Ks/AKbJM3rZ3YeRROSiyA83aKqty4Zd6tvaEYqoEgwr1HbbomCj2T4RLJRwgRxTqM0RF6zpwmfLNqMscN7oE150rA2Ou3UUqjcnnY1RfNMeYdCMkYUQtQVzcw2KvDp4i14+OMlSDlMEG7lImA2B/fjE5dJaJvq+IITFYKWQ9t2uScNU6zbdVzNDgnJw3pB5vv3ti7ZVNuAA259L7C6PQ9tu9zP0UfS5aOXF+DlcLWWbKpzV4EEIqSZPKFa/hEPvv/36fh82TacP7oP7r/gIN26qE7BqR1iL2SbInI6AqFIBiaJCG59JGXc2EENH8omo6r4zpNTMWXZVkxavCVvvfWwjDYVUNDpvq3xOzxjJ0TNOgCry2pmjdPlxHRo5o3//kqKv12eI7vvEsXx7vvlv5VhWqdOf9DZAi6bYu9sA7FIyi/SM58v2wYAeGXGmgK3RBydjySA56auxKcmfQDxDoVkjNB2/mlJPnFOFIlB0tGKZNkBCEw7KQOnDsjMEmiGlQCcvnKbcFvshk9N2+HiVnt+2ip879nplutt80h6q1I6XsWcnCkS/ZeRw6GsRknuEG7wO3Q/Z80OnPrARHzy9WbT9UF/9+onGpBzsaJi2XMiqs3U3lOzVm/HTa/OxXeenBp4vXZ5movlmopCIRkjurStyP2euXqH4/YybuaiGdp2eM3p/Py0id012yQUJRA1qaoCQlLww8DKZvrwx9a5M/NyY9qUb2aR1C4SaeXCDTXOG5mgbadRcIT5YvacR1J23a6HygtnxRXF73W84qlpWLihFlc8NS2Q8p2Iu0XSjqiKI+27Ys22cPKhlhoUkjEimVBw4aF9AADnjerjuL2M597olzm6fycJpcrHKfuN1bnQHl+Q1gynayErfY8IdlYh87m2tV/azndVv85VlutE70m3/n7uSrfH69C27CkS/Vrvotjx+21SzR6xNFlBoW1/FM9vKaL9+CpkAHecbwcKyZhx97kjMOGGMThjZC/HbeWk/2lVV789bRh+ecp+vssMAsdjtfKR1IjHFh9J+WqyZaYb+/aFmcLCrinGoe0ubctdd5hBBIWF2Wl7rUtGE/VBURIKFCQs7+AoRSh7IojhyyI5J/l+y9Fot/YbPCptipuspJCMGalkAgO6tg2tPq0m6N+5KrQOxy2OPpIWD7ZeSAZjlWyZ+cYeUR9JGaiqtXDIDm23KWtJ/aHCYIURLN8vhexbw5oi0bQMnQ+ev/qj06m24vccFfrtE70zGh5BH7vXe6NQUdvGqgr1ERgGFJIlxHePGSi9TGPUth+hVV0ZXDYqp05TNGo7MCT5SMpAhWozQ05LO7I5GlXVYE0VaKZXAaO9FoUUQV6HlOUMbcsrL4qdWdDXNfBgG+8urNZlSionaKJ4PwH65zWiTSx6KCRLhCHd2+HmM4dLL9foQ+jnPZ1IKPjNqcP8N8oEZ4ukObqOJ5hYm/y5uE1IB+gjaTwmuw4hK16Se3Nxqg7bm5fhbvss+mAbOWV6IayE5KZ1S+wU3ewflnD3bZF0eEADD7aRfJ5q9jRhzpqdUsssVrx+BGif18K6TsRXxlJIlgDHDemKN396bF4y7ahFbdvlL/Rftv3BZjtou/oTihLM0Dbkpf+RgZ3FLbsq+wFhjDgX6UhttxA8zEL6Y3kOtpFQt58UIlG1GGnxP7Rd2MFt0cT+ojz03yW+ywiLKLpKAIWba9vu+kfzTHmHQrIEGN6rGhUp43RWcjAMbAvN8W1FRlUD6wa86jCtxTXITEdOzQs12EZgXauQ1M/KI/KiltHBXv7UNHy8aJOremXh+VJIaKPcoW3x/UMLtvF7kgrtJCmZrXWNhW6COAGPEngtT/vMyJxi1C3F8CHnFQrJEiBIc74+/6K/soK0SDq9QCx9JMOI2ha4Pk1h+kjajKK3WiT3/g2jFUagfM8ta2Xmqh246ukvXO0j6zHwOrTtNzjGWLfr8oogj6Tf76VC60jV4rdXimTiMFv8GBdkEBUfySg+b7KgkCxhZAxFGAWan3eGqqqBWT4cs/9kh7YNy7XtUQo4tB2kj6QRu/sie72TGiXp+ivfzgfTZje7Uy/SAlkv8kIG2+iEShx7Jh/HtHrbLjQ02z8nQWsanWiRcH2KSUdaHW6hUzrp80hG46Ep9DmRDYVkCRDkPaudRU2BvxefigJaJC2Wu52/3CtOoj7s9D+W6/b+Nze0DePMNj59JD0S5ovZ68yBbltodr/prSs+jzlefRl++fLsQjdB+ru2mCySQT+CXs+Fzqe5gPd8zLSjDgrJCDFmaDcAwInDuodSn4wbW+YXXpAPmrNF0ny5forEgCwEqnP7RNP/yLIyWx1nznKrtG7rNgDE1gndZp1dRxLmO1p7z9c3iM+kIjt/pt9gmygGR/hp0/qdztPfyX7HrN62C7saW+8B/f3b+nvnribc/fYCLFjvbnrQQgcPibK1rgErt9WbrpM1tO15IgDNjlGxSMaN4BL3Edf8+eJD8P68jTjlgB5Syw3y0dEOG/gd+s2oamD+NE4vkNaobWNeTEMeySBmZRHoPkUtkl46HkVRdG9pkZq0UduFnHYsS7jBNi2VPTt5BW59fR7uPW8kLjysr+N+bkWSqY+kD3+v3736lWP5hSaKbbJi8cZanPy/E9GpqgwzbzkFgPWzcNsb8/DqzLX468RlWHHPGcJ1FItFcvRdH+Yty17LQg/jakcQCtmUKH64yYIWyQhRXVmG80f3QXVlmdRyrR4eGQ+VTkj6LEtFcJHRXgN5jME2geFwMYL0kTS+6EXS/2R9JFWorsVNsb9Os/f8ra/PAwD8+l9zQqvbT9R2MeQjLCaL0X8XtmQN2L6rKbfMymI8d623c+/lw1pVVYx/ZwFe+mK1pzqjhAwRWqi5tkVchOIChWQJUJYKTgDlv/j9RNsE51yeUdXcbCymVQsMbftNuG5XtyyLpKf6HRe0kr3euVgb1X2eNq8vWNuyfZ6eTEbFE5OW4ctV2x239T7E5m57Mw3hJ4+k3/YEheryQ8QPYQbbaPF6XF4+rKev3I7HP1kW6geOHdGK2i7cTR+V5y0IKCRjzG9OHYb9erTDtccPMl0v46Fq1g1t+yurZWq+4KK27YZ9c0PbhuXGoe2CJSQX9JH00j5j3XaatTX9j4+ZbWzuO88i0+e9/MacdbjrrQX41iOTHbf1PNe2p70MZejiBvyVGJV+zY/fZ5SRcShenuedGgtpFPBjVZTtxx+n+ytK0EcyxvxwzCD8cIy5iATkD20D/sRkxuPws1jZ1vNH22G0SAaF6Mw7zuVIaIud0Nv739wwv2psm3MD7MWiR5EpZAm13mjJpjrnAvbidWYbt8rCrBqZs3QU2ncti7u7x2ddAVcgP/1PkThJBoSMy5Uu0BSJdnlbI/LoSYMWyRJGxr2stZT5HfpVbaKF/eJkObNal9A8IcElJHe+FmG+d1QVlqrZGLWd5yPpc2jb1hoaERua187IfbBN/vaFsq6ENjpZYOuVX2T7eBZLsI0dfkaZZAg/7TWROTMUaYVCsogJcso+UbRBIH5FlgoE9ubMqPbduNVc2/qE5AE0DHujth3eaWFHJVum/9n736Citr2LNL/1im/reWhbyjCdpjyfZUWlG1UtOvqoY+7Dqvkt4QxH4BXvm0ILNm3UdiGHuQt9HoKEQrKISSUKf/mMQSD+vj6De3GqeUOwghinSIzDm90Be8tty8rWqG33wRLGF6r2bzuRJtKuMPAqdtzuZra9PtjGp4+ki92DPL1hdq/BB9uYL/d6rby8T+P0jtK7PXg7h1ZZJYLOEMCobVIUJP2aJGX7SEp4gRUqw47VqjASkosMbYteLBnnT8QFIHvrZVTVtQ+ScQvtLvZD297WieDmvIU1RaLZ5nH0syrW4zBvd+EsXlHFr3HBLxkLv2JeHnlQSBYxvoWkBGSnpSmUc3lr1LZDQvIAMFr1zBBNIxlWsI2iGdre1Zh2V77XNtrsGGanHVbUttl1kJvKJBpdaVR8X2Vg9Zx6PcI4WRe9oL03vPYNaQsrPoW+PCgkixi/QtLPC/y4IV0BAJce0S+3TIH/xNmFenFaBtsYfSaDMkk6EGaiZpH0P9p77yfPz2xdL1C+XTSjluemrsTDHy8RKNEfbq9pIZNm64MF/JXlZv+wnks/7yS/gV4y0LZfRlXFHLUdFZ9Aa4tkuEPb+jRX0Tg3smD6nyLGLsF20Dxz1eHYvqsRXdtV5JYpiiKc79CKqL02tS/yFh/J4PJc2q4PpFaLuuwsf3tbor316hq0cw3La8dNr84FAJx2YE/s262d7zySXppmmD0SgLh1OK9+CU6SMpN3R6Uri1MeSdlRwcVskZRxKWUIv4xFGcV+r0UJWiSLmIRfi6SPBymZUHQiMovZULebl2Gg0xDakPtqtbFAGmOb/nThQVLqVuH8khS1gknxkbQpJ3uarK6TkEUy70s9fy/tsq31jab7ua3Yan+7U2a2znMeSZeY1cLkyv4o1Mw2XomA91LRYzVFYuhR25H5dJMPLZJFTCEtkmYoAJrSPueEjtYhGYJt9I0rS8r5DlNVAYuk4DtIio+kRSGXPTkVCzfUArAWkhO/3oxdjc2oKrd+teQF25hso335797rgyniu+kVS5FpYpIMcrpKLWZt0lpDfc9sE5F+LSrtkILkYyn5qG2P53NPUxoVqQQURTHkkZTpY0yyuO4JJ06ciLPOOgu9e/eGoih47bXXAmgWEcG/j6Q3OlWVmS5XFPGp/Kwo1Dswa2XKq1+zwPhSlxns5HTWRIfJZLwcraqatHgLNtc2AMi3zmp58KPFnsrXorX67WlKO+4XlBgxtUj69AMWxexaapf49pGMSEcap+FG6QnJpZZW3Ij6i26ubcCwm9/F5U9NA2C4JiFaJPN8wWPkwmHEtZCsr6/HQQcdhIcffjiI9hAXFMIiOff2cfj8dyeZrhvWsxrNvoNtCvPqzApgo5VRl5DcsI+s06+qzgnJRY1gspNeW2HngrDUcbpB6xdsy9+qzvK2u8ldVLhbFJv8oGbL/X4s+UE/tB2z3igEghcPFnV5rLdQ70MZyM4gIfrh88bsdQBaPnwB6yT+hQyaixuuh7ZPO+00nHbaaUG0hbjEv4+k+wepXUX+LTPz5pNR39iMbu0rfA/7Feq1mW238ZRq/86P4JbTWpk+kkGn/8kSZG64jKq3SDY0Zfa2y6bMgKxrLcepL3vhhlq8/dX6QOrTYppHUmawTUT6UVntiIKwlu31UAw6UvZ5/2LFNnSqKsfg7u083RvGXayHtsMlArdnYATuI9nQ0ICGhobc3zU1NUFXWTJExUeyU9tydGpbDkDC0HaBDql5r29nymiRtMkjmZTYWFlR20EObWtJ2h66/XnJ95HUL0ln9PN37w54aNttsA0A/Oi5L71XKIh5sI1mve+h7cKyc3cTpq/YhtH9O4VWZ9DvF73Q93+Go/GGt0emQFq9bRcueGwKAGDFPWdIj/wOM9jGfgIM/chCMVuegRCitsePH48OHTrk/vXt2zfoKkuGZEBTJPoRqAO6tvVVd8GEZM4iaUxI3vrbuE7W6RfLfxeeRdJuru0sfvxD86dIzK9fm/stJyQDDLaxopDvd7NrLnNou9AWvMufmoarn52ORz9ZWtB2yET2KfV7/4VxjWXWsMTRLcYZuylYdetCvv2tqjvsDx/ilRlrQm2LbAIXkjfeeCN27tyZ+7d69eqgqywZ/AYNG2/sv11+KHp1qMRz3zvCc5kH9+2IB799MA7VWBncvAsLlYA3GyVsFNHav4wvdZlD21ms9FmoM9uozsdmt97ptDg1MZ1RTaO2/b74bfNjWqzSfjyELSodLZKhtSQYZq/eAQB4/JNluWXFfkxW6Zm8Hpff92EY3woyxWp+gIr/sq3cDaLiI7mlrhE3vDy70M3wReBD2xUVFaioyM83SPzj1yJpfI5O3r8HTt6/h68yAeDsg/fBnDU7MX3ldtf7FtoiabS02Vkk5Q5tq3vrM8mADfFhsrCGtv14VTiVn1ZVnY+kSLBNUJYX7WGWJRJo9JveyieFmuItrMey0FZSv8iOzPXyitGKzzDOplUdXt5FeYF37ptjW2aYBsk8F54iv7ftYELyIsbeT82ZIG/rYntmsj6S+ULS2iIlK3m6NmrbqsQwo7ZFOgA/x+48r7g+aruxWSDYxo+PpM2haK9/yu8D5xaTY8rokiuLH7R50ndfTQmMYu5wo5b+J5ShbYcq3ByD8T0ny1XH7Hch77MivsVNcS0k6+rqMGvWLMyaNQsAsHz5csyaNQurVq2S3TbigMw8hrKJSo46UUSGtoPykQRaO2orUSOeR9I/Qul/bO49t3el8dDSGb1FMnvshXjxa49FpgVaBLNnyGtUsNl+UXxGF26oxSF3foD/zFrret8oHI1q9dvjvevXfSYci6R9LW7aIOMZN0snlqWgUdsh1xcmrrvC6dOn45BDDsEhhxwCAPjFL36BQw45BLfccov0xhF7vArJscO7AwCuOW6g0Pa9OlS6rkP7MBdDRFpTNtimABbJrzfW5oSs1bkSfb/KeREHbJHM+1u/JG0Itsn+DC5q2zqPpFZJJkO2SJodk9cpEqPiD5al2cZFYMeuJlz/wqzwGiMR2R87/oNt5LQjrDryipLgKqDdTXvbRe2ZKGZc+0iOGTOmqIce4kTKo0nsse+Mxrode9CvS5XkFvmnULdWdrYSVxZJSULypemtEXvWQ9tiJ0ZGHjuRIuy+YRxPi0MFmUx0pjLTXuOwLZJm6H28xM+L2f1TyNf4jt1Nhas8QPT+eP5PsO9gmwjYwfwcgSc/S8M+2nv/wwUbtRsGS/4Xc+vPwl8WqdBHsojxmpA8lUxEUkQWkmz+S/v0P/p9JE21rcNKnAr7SEpoQ+gWSePQtqqP2s5ZJG3L9GWStF6lvf4hu5LItEiaJzcX31/2ke8JeLYiryzaUIvLn5qGWXsjyt0i3UeyiC2SqsBz61SWfmTLVbMsy8wt91acZ6Ig6oOCQrKIiUpCcpkU6mHL+Ugmra2O+SJT/vm39JEULUCSj5HTkdkJSScrilCwjYlg8ju07WV/7ZHIeN4mL9kivK1Zk6x88JyI2jBeEM2RUeZlT07FxK8345yHP/O0v+z0TMXwhpf7zpbvI2n1ER6xR6KooZAsYsIKtvHywGnFQjG8DJssEpJrCcpHUqTMjKBJUsbQtkgZTtZY25yNDn+35JHML8tvQnKrbeyuom5oW8LzdskTU4W3dU5ILl6vebCNOLL73ECEpIRWbqptcN5oL2aPqmr5hzc8vWI0+0RBLLk5BLv0P559JC12DNpokZ8TM9DqCgqFZBETlkUyTCthoR623BSJNnkkjRbIIHzmrIpMCypEOXkk/Q1tW6TC1JRvX59xaFvEIimCF5817WGGOQKweGMtfvB/M/KWaw/BjZXR7P4ppK971CyksrAWLd7wH7Ud/HkOMtjGyz3q9KGaRfa86G6I2zA3hWQRE7bPlhuK7TGxTEiu+Z42nu4gYi+sikwLvlDl5F1z3sapg/Pjz2gc2hYRHSIdjrdTI9ciKcp3npxq7iPpsfczzSNp+DudUfHxok3YXt/oqQ43xFdIan5LeAv6fceEIZakDmzbWCS9YvXMhP0hJcO6GlUoJIuYIwd2DqUeb0PbYtv9cMwg33XJIG0lJG1mtgliaNvq8IUtkhLOn1hCcut1LRZJm2Foh84iraqu872JbGMnXqwvZes+YQrJjTXmQ6w6HzwXF1vk9vn7lBW46ukvcLaJf6DsIy+kNShIvAjk3Y1prNq6y3Sd/ykSw7BIyrPCBmGpsyox6FMTN7FoR+BTJJLguOSI/ihPJXD4wC6Fbkoe2heC7cwhefsVhq11LR23USzYBdsEISysvp5FheT89TX+GyFkkbRfb9dcp+KNc21nO2cvc2W73cZuH79TkspA+1yJHM/jnyxF745tcNQgk3eEYf+3v1oPAFi1zVzUGPnzR4vRqW05Ljuyv9D2uqpD6mVDtzqpzr+NnHD/BGyo2YO3fnosDujdQbfOd9S2j313N6ZRWZbwNfrgFmP6JNFzaMe6Hbv9NUoSemt1vCj8m5F4JplQcNFh/TCwa9tCN8UWuxdABFLzAQBWWFkEtO3LC7aR3w4rARbmUGBGdb4udpYSBYq9ZSHPR9JQvyGPZHZ7+zPgfWi7OaPiL/9d4riPTB/JO96Yj588P9O10HETFTxv3U6Mf2chfvL8TPM8kj66s+Vb6vGnD77Gza/N9bR/WBbJsK1CXp7TDTV7AAAfzN+Yt85nrI3n499YswfDb3kXlz81zXHboHwkVdXw4SRahmbDd+euxztzN5huF1f3ikJAIUkc8TREIbiTUZAUOtn97kZ9fju7hORBpP+x8oXM5rkMAxWq45Ca06G7uoyGbVuCbVr/brVIuijToR7tfWZn7dVlH5B4uZ/6bDnemL0OizbWutpP2x6n87G9vjXpt0geSTfnt25Ps/jGJgTRiZseo8B+G3buyc3nLrMNMo7Q9z2ntrzTHvtkKZZurhPe7bWZLVNUTloskK5KppA0urR4dOXI8siEpTZ1uS7OFcbi4xZgo4VCkgSO7dB2RCySWYxCzj4heQA+khZvN9GhbRlkVOeXnu2RO4lMh7KNQ9tCPpICG3lJnRP0WXf7geAmCEl770bN+hJWe5zEx7x1O3Hk+I/wzYc+lVOfheuBiIgw+3jznakAKv70wSLc885CnPTHT/wVZlOHtLJ8CkdAvD0FfSKi9Tj6hkKSBILocxIxHWky5GbtIxnm0LZo1LYMjL5JZjj6TdmNbBstYYa7JaMao7bNt9OX4Yxq8dt2nwBOux+ruy6/psO22iskkkfS+8wh7o8nrNvZqZr/zFoHAFi4wZ1l2ArZ33u+jfAqMH3l9kDrtbqWnnySDUPZQT6zhfSfjRsUksSRQB+AiJkkjcEuWrFojEYOImrbylITpkXS+AI3wymAyo3oMx5ynkVSYGhbLNhGbDjbah9Z+ClS1Zts7DfWWiRN80i2/l68sRZfrHAvOESaYUahLKR5wX2S26EfmvVftn+LZPAEVYeqGo7fQ0V2b+i4Zg4oBBSSJBDEfSQN+0lviTuaDW8XxSZqOwgNbHXeQhWSqurYwTr5UNpGbRvKzgYb5Pa1miLRpj6RTlu7RZjplOza4WdfZ4tk6zVyEm4n/+9Ez23yIgoLFWwTdLXyLZL+Cixk+h9vZWl+7/2f9m+5BHtujOel0H1bkFBIkoBofWxsI3wDUJJV5UksuutUT/umM3qne5ug7VDzCoYrJJ23sfd7VexT9Rj+vuCxKbq/8/NIOlskRWixcLQU0pwRC67QCTdJl0BEeFndWhmdpda+DO01Mp3ZRlLX5qWUQIJtTJeFPHyp/S2ham/Dw+a/g0JmHXYGd9m3TCGHmuMWeEMhSWzp3LYcXl4V2of0zIN6yWuQAAqAilTS077G4IewE5JbEX6wjT0KgN+fMdxyva310LCyrkEfAWyca1tE84l2Ck9+uhyX/O1z1ApGHYcxtK2qKrbU6ROQW865brDY2KEtw2hpN2uHLTb3uhdRGJqPZGj1qLr/ekHa68SF94OU6iTWYRTiXqLgRa9B6FIuxk6SFJLElGu/MQjtKlL4+3cP933/33XOgbjiqP6m6/LS/xT4Sy1vaFsbbGN4WsJ07zQO/waJSLBNIqFY5i9V4O+dmVGNUdtq7pcVovXd9dYCTF66Fc9OXiG0vd66I8uCpy/nly/NxqF3fYgJizblllkLSW8WSbPocFlPmiermcBOhU4FpmVPUxrPTV2JNdvN881mm+p2RiYn/J6DllRewWL9XHj5wNA/90HeAUH76Ubn7g0eCkliym9PG4aZt5yMA/fp4Gl/7TNaVZ7CeaP7mG6nKNGKt2lOG4a2dcE2hpltotRwiYhaJG0P36GAdTt2Y+7anabr0oaE5LmobTu/S5ev7Rphi6SrYl2XqSjAv/fm7Hv44yW65eb75p8XK7RFiA7lW2LnquDhHIkY2GVY4Y1t8/rEPvDhYtz06lyM0/iSmlnL3EQwL91chycmLbOt13fks8dT6Go/F8fspiitK4qb8nTb2bykwv5OCcJNJipwikRiSVmy5TtDxj1v5SepAPjuMQPx5KfLMe6AHqYP2GvXHYNzTOb/tazLh8DLD7Zp/W30WwtzaDtcVOc3nWJ9nhXFWdgdfc9/AcDUqpkfta3/rwyspqJsqUfNHVsgc/9aFKn1ubW6tZZokko75vrUWiRN8/8EY2EVQcQa1JRW4cZDRShyX7w4HZ8u2QwAqDdMWNBatwpA0QtkhwaJ5HX00l6jn2HQmkVq+TZD2V7uM9tv3ZiJuUJCiyQJBONDb9UxKkqL9fO57x2BB799iOk2vTpUym6eJXZR20YxHFchKdL5KFBsj1/0Jb18S33eMmMeyT1NaTSl7S1qbjsFu7ycdg7/919wkFD5L0xbpbMw6sq3sBilNL4TVuf2syVbtQU5oPGRDHBo24vhUERINjpccxGcxIcnK5dpPdntzIe2wxQtxshnL7h5tcn1kdQ+G97m2hZtTtBuVEEHC0UJCkkSCBcf3g8AcPiAzrbbKYqCsmQCxwzuisqypCTrp3fyhrY1v/MskjF9ejIiBknF+jwr8Od/lDak//l0yRaccP8EqS9+W4uk9rdhs/NH98Ho/p0cy//tv7/Cfe8twjKTaelELJIiHymOYl9nkcwXZbKCbYx+fDt3N+GfU1dhe32jzT7OVb45Zx32NJlbAC1K9VSPDGRYzc3OsIzhYS/vQzf1urGMu6k33yIpl8JGbccLDm0TR7w4fB/SrxOm/e6kvVHf4tY7s6rCtPsZ/bLsfCSDmGs7CqzYWo+vLPwXsyQU+2vq50WpQj+DCwCs2b7b3kdSqkWyZZiypS351kM3WZ+MEemAXmRrT6HI0HZ+O63R+UgGOFe7UZP/8qXZ+HDBRrw2cy1euvYoi32c23PTq3Mxb10N7j53hOe2hdVh51JUSfBPBFrmxy5LKp4+nqysokFh6RfqpSyBch3LkGRllk2UgsdkE1ObCokC3asrkdrrZ2k3tO2IR732pwsPwsWH98PRg7oI72MbtR3CFImFpEObMgDAfe8tcty2ZWjbYp2i+HpJq6pqPhOL3T4uuy27QA6nzswpGbsTVjVnhaSqqmIWSReHbJpHUlbHZijmwwUbAQDTVmyz3EV0OPyVGWu8tgpA/jEGJU5ao7a9la+lrqEZw295Fyf96RMJFsngxYvMGnTNNY6MSD6WQmYIiZuopJAkoWApJAU6Zdcd997NvzWqD8Z/a0ROzIpQWab37teKJaNw8isoosTg7u1wQO9q4e0VBbYC389LWlXNLYZSg21EfSTNNnC47E6dhHb1rNU7cr/LkgrmrduJUXd+gJ27m+wrgbNo0a428zGV5yPpviTRffx+rOUPj6q26/2iT8/krfSZq7YDAFZu3SUl2CZopM5so7XoGhIAFcPQdnM605pT1LDu40WbXbpqFA8UksQROX6L1hG+jvv67EySLvZ/6spDDZW3/kwYerU4jWwrcBc8ZLe9Avi6aYx5JEVw2ynYZcMxOvwb63ASN45N16y/6dW5ud8JRcENL8/B9l3OItLYTtP1mrYHmdDem9gRFZJyHzLPFknRcv34SMpykswLdAz2RSU12MYggv2KYrtdZOeR3NOUxpHj/4uL/vq56fpPvt6M3736lWO7ihEKSRIK1hZJPWado9/XoGhn1LltOUb31wcHaQVwmUGRxilqW4VLYaw4RG37aEtGNX/J2wknu/oW3JE/XaZpOpxsWQ4WSed5xjU+kCbbWnVgqYRiGwRkRHS4FQCa/M5sY4Mni6RgQLbfJyxIi9zbX61vrWfvnSI9IbmXffLEWPHIFtXiNwDcL+By07KfmPqUfVamr9iOLXUNmLbc2qXj31+ulVxrNKCQJI7IeA9ZRvg69BS3f/MA0+V9OrXBoRbRs8YijZZEK8w207avPJm0XFfsaHMnipCwSySv+Pvat/SRtCnSrrNsU55EynBxjXOqW9VjJiqdovWdjt3aR1Lu61h7Cs2O189j7TeoRHQXNx9rpu2QpBaMrZi7didmr2kNSMtFbTu1xyW+fSSLzQ/Q4BqgLaG2oRkLN9S4K06sKinknesiEvB+oZAkjsj4ohX1kTRWdcXRA0wFTkJRhF+RojPQmNWjXZJK6oNIYmWRVIHKlPjrwC7YJluen7aYBRmLBsiYYbxWdoa/XASuxUE4WSS1u5ndIlblphLuonTtnktVVXHDy7NzfzeZ5ZH0cZF0c367LEdVVVzz9+lC2/p9xPJ8Ij0es3GvlVt3ma6XPVzqLWrb/HdQ9Uod2tb8zphYU3cIuH2IH38hRXbBqg4ECkkSCp3bVpguF/KRNFnmxglf1NBjXo/d0LZ4G6JORlXRtiI/G9iTVxxqsnV2aksrH0lxkW/VFjOLpJ9O2thUkWAbq02c7lmnZlqJ2KQbZ17Yd4Ortu3Cqm2tYke2j6SfFDNu5o33699nvBZBdeC5AAsHtwj35bqrv6Xe1t8ZlyMNXggqv6vZXNuufaft3GFkWyRjJg7dwDySJBQ6ty3HM1cdhvJkAqu378Jv/vWV6XZuhrxEX4+ilkOz7bSLUokEGpDRrIuPkkyrKqrK8+ejq7CYo84haNuXtaspncF78zaYlGm+/fqdu/GPKSttyzReWyfr5vf/Ph2z1+wwXe903Z2Hts3XJ12mTbKrx2iBNCba94v29Mm2wmmRH7XtsRxBdwU/p8Lr+2Txxlpc/Lep+PEJg3DlMQOliFk3GSncHnPNniaUJxN5GTIAw0eWSbl2vs1mu9m1TXb8max7rRihkCSOyHogxgztDgCYsbLVGdn48jxiYP5MOKbvVxfv3KQfH0mbYJs4kcnA1CJpl//TMmpb8deh/m3Scp01LddGi0LPfXiyo5XLeG1thaSq4v35Gy3XO90FjsLKyiKZcGfJdXOOzTpgN/vnH7N3H0k3IiXoqG0v9+mSTdazFVml/5E1pG7GLf+Zhy11Dbjtjfm48piBpu1yX6+LoW0X5e7c3YSDbn8fHavKMOuWU/LWG4OVjO238212i+wgpGIKapINh7ZJ6Gg7B2M3sV+P9vjGft10y9qZCBy7DsYoTkU7I1OLgGZRmYt8lMVGRlXzAlIAuyAp66htv12/mYgErMWfyFBpvo+ke3/LbEfhdDu5ye+oxez8eynHbK35XNvexYIfi6Qbbehqej2zZQF07mf8eZJl5YUYLrXPjRm8uLE6x2ZL5+y18lv5Oub7N+pLMfP1tS3EbjOhrcRx9REoue5CE9+ekUQWJwvhsF7tdX+nkgnMuuVkzLz55NwyBfIjP011pHZoO+ZC0o3lV7GJ2lZ8Rm1b4WcoythWe4ukfVlO95O2YzXb1OrcJJOKK+Hj1yLpBxkBHSI4DffWNzRj7tqdpufNGPULyPHna2g2i4BvKdfq2gY1pA7k349muU+DRGYVeoukamKRdOkjabO59HMTkj9uFOHQNnFG8gOhs0gKWhw6VpXr/rbbz+tomJlAsAu2iRPpjIWPqFUieSi25zmIl6gf65Lx48XOsOFmDmszHC2SFuvdWiTtHkxjHWY+kn6uUcaHWHHzkeF0Si766xTMXVuDRy8dhW31jbp1qhpmsM3e/0quS0Q4BZXPVRSZ51SfDSB/vWsfSbvtQoqwb1eRQl1Ds9S6okZ8TSxEGvt2ayu1PG2nbm4Ec+5Q3fhO7WkWm5bKYWQbKcl5/qKEVR5JLz6SQDAd2EcLN3neN29o26ZDcuqrnKxkThYhq+KTiYQ0H0njKtOE5LA/D6J1u7XyuanS6V0wd21LXsEfPvdl3joVTsO+ciOOAfniRORcGW/HsKzFmlqklZTnI2lY79ZH0jZFlquSROoyr7fcLK1azMyV8e0ZiTQeumQUzjqoN17/8TFSytMJSY/mQzf77RL8GnSM2o6ARbJndSWqypO4/Kj+UsvNqKqFqDdHgbWQbLEEhfOiFK3HeL/YWXqaHTorNz6SpkLSamjb5bMgOl84YG6R/NeMNRh5+/uYsnSrY13GlmmPwa0WdSNe/URtZ1Q1NGe0bDV+BLYZZtf4na/W459TV2HnXj/DvKFtbSBUKD6S8srSpf9R84e2zXx9vbYnSH/WAGckjSQc2iaO9O1chb9cfIi08pyGtof1bJ+/0IBd/2JcV9/oxyLZurDcxEdySPd2WGwSxRkUfTu3wfPXnIBUMoG/O6S8cUM6o2J4L+fzniWhWA9tm1kSgkK0M3ATte1k9HDSNnqRZea7Z7ezQ+Ea3GT0MTve7H37g3+IJQfXoh/CdWuRFN/eT4otUx0Z0AdO9hxY+kh6sCxalZe1vv7u1a+w4p4z8tw2ZFgk3exntamXuo0fYUYhLNPXN0iR3eKf21J+4c0PwUOLJAkdp6Htbx7UG7ecuT9euy7fAtq7QyUA4JQDegh3YLsaxSySphY5B4vkiz84Ck9cbp602ws/HDPIdn1CUQIJ+lFVYNwBPfOWWyYdV6ytRRkTS0JQiFaTl0fSpoFOFkkntwqniGarqt0PEYtvbxftKpoey6pu1xZJN0PbNk3b02T/gWg+X7s3jEXlDSfn6rTex20dgKiPpHU5qhq8kJHrI2lfmJiPpL1rSWtdws0SQluctmzT2a3kVl1wKCRJ6OiG8MwCXBIKvnvsQBzct2Peuv/8+Fj8+eJD8KMxgy3LN4qf+gYxi6STQDDzkezcthwj+3YQKl+EwwZ0sm9DQMPr2Rkwzjm4t265fVCTjUnS5FVZWSb/dSMqpoydra2PpJNF0nFo28EiadGNZFyOxNod+8MfL9H9bedbJjLHd55voWq31h43kbd2z6QxuMbItOXbPAfbuDWE5oJtJKgq7SiImI+kcWhb+9tbe9ylXZJoJTT4F+dFbQuY4UXdC4LMI6mvN/42SQpJEjrafsvtI9atfQW+eVBvcwdmC+p9+EiKRG27SbDshFNZIp2+KD/SWD+zFjrjObBqjZ2lI6Oqph1gEHOTiwrJPB9JDxbJ7C7uhKTZeosdVdVV52YlyJrSGbw+e51+mY0iMTNwb65tsK3bj0XSzTHaGUu377IXktc996VJsI3hb4um5PmEOmbtVPPKcytTFKXlA2fhhhpN+7xYJA1izGU7svuJtsGpiVbvNLNydRZdk/Q/IhbJDTtb88q6CUjzi97dQ/x9kWVjzZ68j6NMRsXctTvRaJJyKkpQSJLQ0QfbBF+fqJD0mkdS1pzbg7u3c1TW7lPEWPPrU4flflsZrKyNjqp1sA3MX+BBXOo9TWIvWOPwrZ3TvpVA67HXrcI5atv8d+uy1oXadrnt2IwiemtdA7737HR8tCB/Vp603dC2yfGc/9hk3d/5wkrz22XD7US8EbuPD6fOtSmT8Tzs6to3U9X9xzN3v70Ad721IPe3yIdSfrCN5rcEteRUhuN6Gwt8/jL9B4pbH8lNNXvw75lrTcszaZhUrJ570TvpiLs/wqg7P9Ate/LT5TjzL5/i5y/O8t2+IKGQJKGj7bhkWvNay9QjHmxjlkexlbKk+RR2XgIC9unYRvf3RYf2xVNXHOa4nxd/NhFyL9y84q0js606ebMOAAhmbnJR/1c3wTZmndVxQ7rinm+NACASbGNfj76j1w+Du+nbjGXf/fZCfLhgI679v/xUOE12Q9smlvaVW81nF8qhqdpt8nlXGVxsTrbTLCc9qysdhuStSSjAjl2N+O/CjULzlGeL9ZNfU4GCJz5drlsmYu1N5JkktT/NszG4wakJVnetsyXT3iI5a/V2PDJhqW69k1vEzNU7DG2zRv6kCVZD2yZb2ulbzcpHJrS4qLz11Xp/TQsYRm2T0ElIskiKvgaOG9IVkxZvwX492uHrjdYR1qZzbeuGts2/u9ymbQH0w/sA8D/njwQALNtiHwEu0yKpxbUYUK2vXZjBNrsFPxKMotfOsmHWWT115WG56+8cbKMVEyZCUifCzJeLYGzm5jrr4Wi7DtjL/RteQnLrtjkJvL6dq/LOv2jNChR865HJWLalHjedPtxx+1YfSW1d4sLCCpFUSbbpfzxUu6cpjfveW6QpQ4Wdovc6tG12aNrr9fMXZ+etb3K45m7cZ4JO/2M3tG3vu9m6j+iIS6GhRZKEjrbjCkNw/O9FB+OGU/bDP64+wnY7swdeq9v6dGqTvwHMLTpOeLXE5lkfJGHVX9kNbVu+s1Xz66rYlOeVRsEcOMZ67cSMmejS7m51DHua0lBV1dF/0G5uYjfPg1ZkTFq8GRO/3my5rd1Qvhcrt77TdPsR4kZIWq9zuvY9qyvzlglXrQDLttQDELMGWU2R2JzO4IlJy7Cxxt7n1AqxoW1DW3Ri1j3vzdugL89DGfr9re53M4ukfW1OeSSN3/rLNtfbtCs4dNOkunzXa9slOplGoaGQJKGj/WoMYk5mY0fftV0FfnziEPQw6Vis2tValoIvbz4Z0246CVXl5gZ8t1bCtuVJmxljnPzvWs/Xz8YOcVWvF+yCbayHtlX868s1rsrzSlOz2P2Tl/7HziJpck8qOneMfFZv24VhN7+Ln784yzn9j0W9bp8FbTsve3Ka7bZ2KY28zNgk0lZFAd6asx7nPvIZ1mxvHSqXNbPNUhuRALRYnfOitj3KB1E/QaOf3AtfrNb5PLpFIP+2g4+k++M1ziXu10fSzX5O94aTRdKN+4z8KRJbcUr/Y2dp9GPtLxQUkiR0tP1WWDOgiGD1Eurcthzd21uLULdC8vPfneRqey1av7ATh3X3XI4oVuekZUpF831UtcVJ3E15XrHz/dPiRkje8cb8vGXavc0E9DOTVwAAXpu1TndPm4lSy1tedSd03KTRkW6R1Py2EpUJRcF1//wSM1ftwK3/mZdb7qbdVrfL5toG3Plm/nXSImqtNkN7SkRuWTX3X/2xzV9fk7+xC5wEu91zmG2X20fOGMTk2AZLi2ML1kPb7i2STtfUzdC29DySOgVvv23dHmvf7iCMK0FDIUlCR9txBTOVlDexsq3e2/CTm474oL4d0b6yzHbqQTu0fmFBBCoZ8WqRdFueV5oE02IYm2pnoZtlcNjP29/kILTiyMkitHKruSXN9dC2i43tfEK9DW3rrSbpjIqtBh9NbalbNOvcpf8xb9vijbWO+zanTaK2BavWPlsiZ6d1ZhvjcrH6AIuZbRxekKqa7+PqlDXACaNF0jOmbh3mv+2WaXH2kRRoV2ttbjYWKE3V/c7+ZXYP19lkEilCHUkhScJH1tC29a7eyszOXesWN1a2h/ZONWm1T1V50nZ/rSDwY9y7d29wjxGjOLUOqLFfZ4VsH8kbX/1KaDvjy9xV5DCMQ9v5B6EVpjofSZN6vv+PGaZ1ZEyGYu1wcwx2wtmL363RR/Kix6dg9F0f6nIg6p9zmP52wup+EZndqTmt5lnL7Kqes2aHab0q4DgNauvQtlwV4GwNtA+2gUkuRgB4bupKS5/aBoNfnsyhbSdLttPxOlmz3QSOBR1sY0U6o9oKSVokCRFAawFxM8wlkyHd26GtQbQFMfWgkb6dq2zXj+zT0Xa90xe5KBce2ldoO8tkwlAt11m2MQADqp0zvZa8PJJulaQGM92lvY+1RbvpFNw+CW7yMdoNbXvJBKBPHA1MX7kdAPDK9FbfWO0wpPY8yJjZRmSGpyYzH0mbc/brV+bkfmtLn7lqh2NdWfRR+P7fbU6Pe0ZVda5Cxo8RsybMWbMDN706F5c/Ze5Xaxzadk7G7g2z/ZzuDaf1rnwkhbd0X57dta93SFlWhDqSQpKET/BR284vE0XJFxdBpdYxrd9ieXkqgfF78xWaoRUEYSRztyKjWg8j2aXkCWM43oy8KRJ93Hdm5117XbSi6XYTf0sr3OdjFN9+6vJtluu8pP/Rp5hxbodXgWV1j5ULWSTzVZho1W59eVU1/7ha/vT3gnM6V9mpTbOkVaM9Mp91O/aYLG3FfbCNt2NUTUSy0y3tlJDczStcvkVS+w5o/dt4K9Xa+Ee27JvfsDD7Ji9QSJLQSeh8JAvz+ZVQlDwhGaows6nr4sP7Wc65rRvaFhBlfhOYW6f4sZ7ZZneTTcqKAr0PZQb5mB23zkdSc0uv3bFbuFwzMWKHG4ukHSJB2/npk1p/a5thHYClsUj6dRy0qUdLy9C2Pdp2ta9szcrg9m5RYT4tqB1GcWBWp3OwjV48NadV3QVRbVxQrGhochtsI471fNT5681wHNp28b6T3fdkLI4tX0jau1CZHWJlmb3LU6GhkCQFRebI9uDu7QAAPz/ZOS2Ooih581aHqWm9yhrtkKxIB9Gpqlz392PfGY1pN4lHjVvnkcxfd/bBvQFYC0kFBdOR0qaxBCwskhn7DjIIMmrL9J9+5+H1G2wj8gynM2rOguou/Y85Ip4JLVMkivtIfrFiu3PFFqhqvjBRYf9OETnvTufKGGzTnMkIRdTb0Zg2+Eg6bG9Vhdlifdvy1zvmkZTYYch+SrUGcN3HleFmcrJImonpilS0pRpntiEFxVewjeHve741Aj2qKx39EIEWYWEcHRN5R4UlNq3qcTu03aFNShcxO6p/R9tURkascme2WDr0Dch2jLZD2xZtrq5M4ZIj+uOxT5aab+ATN2lBnLG3SHrt69xOkTh37U4cOf6jvOk23SJybvJ9DbW/nVu9eFMdDr/7I7xz/XGunnkrsSVi1TSzSIpW7XROJizapC8X+day9+dvwNy11ul/jMdm1jSnc5U3tO3gI+mULggw8ZF0OGe/+7dYwFtLYa0/zYNt7He3mzNeZH9dU4K0SNpY6ffYjdjA/BjKIy4ko906EntkPsyKogiJyJZtzdJmhGeS9DrU2ugy/U8bQ0CRcZ+jB3UBABy1bxfT/Y0BSVkyqppn5csO1dkNbVu1uW1FSqrV0IhMIWnWTquobTeoqrsPlXRGRe2eZizc4JwKxw4v96JWoumTL1uXtaWuAc9OXqHz7WxfmcJZB/W23MfqXIqc4yaT9D+1e5rwj89X5qUqMtvXjpem6xPuq2p+YI+diATy3z9PTFqet43bYeWWJOzW1vGlm+schV9e+h+HU71IIBWTWXvMDs0xaltAWBcKq/NufCKcrKpm/ZDcD2H50CJJCorMoW03z1pCUfLSnojO1iEDp2KsWuLWIlmZ0gtBowh6+JJReGPOOpw10rwzr6qwsEgi/+WWtbBsq2+0bI9Vm9vYzPYjA5llm5Wl95H0KCRDGhI34kXAa4eW3bQ7mVDy/Cvtqrd6P4gEGrV02PrtXp6xBi/PWIM3Zq3DS9ceZbmvex9J9yLGOLXqFhNx6zSEb6wznTFcDUOTLv7bVGy1eT6BfB9JmfelVp9/uWo7RvfvhK7tKlrrcrJIOlx3N9dAvo+k+W8jTrlvC5TIxBe0SJKC4udh/saQrrq/XfnwI9/ZPcyhba/CRp+Q3Bmjk7bRYtSpbTkuP2oAOrXV+1JmaWPh5G3mxC/i82W1RVV5MtCvbr9BR1rM80i23hheMzRl1MKISZEzM2v1Dp1407ZS7w9mT1lSyRPddpd99uoduukVs4ikEDKL2s4ybcU2/OPzlahrMLee9+kkNrKRxcxH0gmRaHknC5ya0ddrtMIa995c6zzpgtEaa3WqVVXNm5fbCW1bf/CPGTjsDx9arjfD0UfS1dC2+LYiGO/rbPnGd64Xi2TUoZAkBcXP19d1Jw7G7d88wNO+iknUdpjDIk7D0lYvk27tW7/ehSySZfpH3GkXUXGoIj9qW6RjtEzEXpaSPn2ilqCHtrWdiNcclYXqP0SfwVdnrs39Ns5sk8PhNKeSCcMQoPM9+auX5+QtE/GR3NWYxhVPfWG5/ubX5lrOCe9e0Kuuk9wLleo4tK3m5ei0EvnCdQq24a2v1uMHFsn1rfYzT5HUiqOPpMNJducjKb6tWHkWPpKG7ZzcJsyOIeIj2xSSpLD4+fqqSCV1ibXdPGxmeSRFmhLWA21synPfOwJHD+qC/73oYG1rHMupSBktku7bcuzgrnnLzC2S9q8TRbGWz23Kk4H6SMod2jaxSGpcDpocAgKsMPOzCwPRBOFa65O2nW4+wFKGoW2RfVebWCRFqtxU2+Aq/ZLb8o3bu/0QFdnaeShXLzycfCTz2iDQZqstpizd6rivEZEE63bYJdYX2V+L7EfNKiWW8aXn9H7YsbsRU5Zu1V0bCklCbPBrBXT7gO3brS0A4JsH9cZlR/bXrRN6qRZoaPuYwV3xz2uOxL7d2rkqo8JokfTwRnr8stF5y1RVzbNAisw2YkVVeTLQZOUyE/qancIpy1o7VdH5v40UyiLpxYKqn1e4FadpRstTCZ010Sz6P68uk/MS9IxY7u2RHoSkwPZOh5lR9cLReF5afFCtz6/IefR7X2ovr3NeTH9CsZA+krqhbZs7yMkieeoDk3Dx3z7XjQBEHQbbkIIStmPxqz88BrPX7MAxg7tCAXDzf+YVrC12CFlHBcqRYZFsaxJwk1HzLbpOfoiKTSLJqvJgo7at0hh5wUnwep3GMn9m6HAQFWXarbS71OxuFY8vfLHatoxUwv3QtlmHLysZu5s67VBV920SOe0iCcm13wFGi51TFUJ+4b5n59HW5yAEHR4dR/9C0Ua53tgZ7bFp/Z3zorYF3w9vf9U6AlCoGcFEoUWSFJSw0zV0qCrD8ft1QzLRErU9tEf7grXFDpGWiFgXRWbP8ELWkqQtXshH0mJ5VXkyL4o+m2BeBmU+rKVGHPPweRSSLdOqedrVF26SPKuqmhfQ8ZPnZwrvn0oquqE91SlsGxb5BoP+6nM7tA33bgliFklnC552m+aMIdjGsL8Xv3CZ96RZWX+fsgK/fGl2S9J6p/Q/EgNVrO4rz1kXLM678T3d6MH1hUPbhNggc9jEC02aT+AwO3EnEXjLmfs7lyFQT/40kHLeSNmv7ZTGL9LJInlI347WwTYm6X+u/cYgf43U4OS/6QYny6nXmWYK9R3jZpj48qem4dC7PkRdg/0QthVlSQU3vDw797eYVU5smUy8FB+Ej6Rz+h99Oc0Z1dLtAMifISVIIZndzWlo+5b/zMO/vlyD/y7c5N9H0sWjZ7yHmtIZnPLARHz3GesALfvy9BbJLF4tkjox6qlF4UEhSQqKX8uC1uTv5WHTvpjCjdq2Z3T/TpbJwHNlCBywcRvHqG3N73//6GjL7bKnSise7YTkeaP64P4LDrJsc0VZvo+kTCuiXIuk09C2R4tGy6R6nvb1w5w1O4W3nbR4C3bubsKERZul1K2qqqcMBoH7SHoZ2nbZJpEqnBOSm0Rt21gkjTOk+BnaFv0m1Q9tW29Xu6dJIGpbno+k0RVh5qodWLKpDh97vLf15WnT/+i3Ex0BiNIImRMUkqSgFNovUft1GKqQFHgJO02LJeI3Y0x74yYNzqh+nSzXZTuolKCQ/M2pQ9GlXYVliytSiTxLX5lxDksfyM0jaY9nH8mI9xva9pklzxahqVl/kBnV+VlIZ1Q0NmfwwrRVWL1t1979AhaSbrf34JYgI7jP6A5hnBbSuH950tkimTdnuMRT7WQ48DuzjZumGtviNW1Xrm4LwWx8T4uOWIjOGBUFKCRJQQk7attIk0WS5aARsyYKBK84kD+07byPCNlzpZ2dQyQy2uqYypJKnsg1DsP5QaYodbou3oe2xfzsotCnfL2xztN+Rv/RO885UCDYBnh28gr89t9fYcz9EwAEb5F0P0zt7N+Xv48zjlMCZpwsknqLovE5MBN2xkV+z7T2frU7HkURySMpz0fSWJbfe0qXsN+mKNEPTVokCRGk0A+L9qEO1UcyJK+XoGaLybi0SGax2qIsmcgTSMaIcz9ItUgGGGyzx2ae8ixlEv09w0b7vCkKcNmR/R3PZ0ZVMW3FNgCtnX3gFknX1kX3oysixyC2Tevvpkwmz0dSu95465i12SjG/LofiUZtt1h1HXwkJSYkN5teMsuepjTueWchPl8mnitTl9ZKcxW8Dm1rmxeBb0dbiveNRGKBb4ukz/rdBnQUaj5kM0Q0olE/SbNIuvSRdKq/PJXIs/QZc2D6QWYeSaeivA5tvz57HeobnYWkn3ydhUZ7brRzLNuhqtD5C3+0YGPkhCQQkI+kwFCwziKZzveR1AWBGG5NkaFtr5gVI5IX0460xITkdkLy2ckr8NgnS/Htv36eW+ac41Lz2+YVIDpikc4Uj5KkkCQFxe8IlV/fkR8cv6+/BnhEhqATOXbjXNmyLKHZy6aN2hYTa1ZD2/k+kkZ/Lj/IFF9B5ZEUJS+lU2idjH+Boe1Ec8EIDuczo6qo0uQyvfrZ6Z7nMw8KEWta3j4C2zgLL329RmuXCuu0NC37mwhJw7m1OizRd4n2/rQ7R4oiIY+ki0tgvIe0ZS/bXK9b971nv8AFj02xFfZWMwrlz7XNoW1CpDBinw4AgPNG7SOtTC+i0u0+soSYd8klvv7Q/p1wziH68yvLMJedNlFvkbR+nTi9EsuTiXwfSYkWSZnpf5xuGa9R26IY/dxk+n8GjTaHXrZDFQm2MWYwCDqPpHtRqLr+KBapQ8QnUKtL0sYpEg1C01icmaYx+jFKTUjuM9gmyJltdD6OmmNOZ1R8uGATpq/cjmVbrH2DdTPb2PlINou1sZiGtjmzDSkI//rh0dhc14B9OrbxVY72AfMzb7coxTS0/coPj8am2j2Gffy9ktpXpPDniw/BmKHdAOitY2LBNubLy02itmX6SH7zoF7480eLpZQVVLCNKEYXgvJkIvA6ZaG11mZTbzndkqqafy9EMWpbeIYgVYWiKELWM5HhVLuE5A3Naf0c0IYjMzuPxjqlRm07lOVUl1yLpH5jbdnaVaLX1TjXdvY8Gt9roiMWQc/eJJPi+ZQlsaI8lfAtIoFoRLB6QkLDRayjxm2c9jh+vxaBaKUJu1dX4IRh3XNiSitq2lU6f5faB9sEF7U9uHt7TP3dSbjo0L6+y3I6h16DbUQxWiDD8pn0agTs0KYsNwKhFbytHaqDME9n8NDHS3TLgu5kXfs7QlzcZjcT2dox3Y2qt4Qa0/9c/8Isy0TZLX+b+Uga6hBopx1u5tr26yPp5kM/zyJpsOSKtql1H+151g5t67drEhamFJKEhE4YubasxNvArm1dliOwjcNGnhKSO+xz5sheeOrKQzH5tyeZrh/Wq1r3t1ZIdqwqc92eLGVJJW+dUx7NLAf0rnbeCECP6kopossxajtg66DxGFIhRXH7SY/Sv0sVAINF0kd5QQ9tu59rW3yKxOxmbiOyrcrSCpibXvsqrx22PpImt6qdwHKDmaizK0uB4jyzjdPQuJuZbYwWSePUnXsRt0hqh8atafKQRzLqmpJCkhQ1YSdqtfrifeH7R+LWs5ynNcyibbaoYMorw8M2zrkpFZw4rAd6dqjULX/zJ8fiyqMH4K6zD9Qt1wrJzm3LLct1CqxoGdo2CiSxa3vXOQc6b7QXGfrDKaVS2ME2YblI+rGQZO8T7bnJdtBuH+Eh3dsFnkfSrchVIW4lzYoUOUPb+qjsPU0ZLFhfo9vGzndPZB7zMIe2HfNISvSRNJalO08u6mzdv/W3qmrS/xjeeaLBNlYWzihCIUmIBHpUV+KqYwZ62vfd64/zVmmIGvrAfTrgtm8egE4GsaizSLaxFpJZLH0kTaK2RT8S3ASbyPCjdWpV8EIyYft3UPjpzLLiu9Ek0OC7xwxwVVYqmQh8RiynIVQjqupiCBTi96Gj8Mrkb7OnWZ9CqmZP67zoItbG/DrlnWy/CcWd93fTFv3fjbqcwhoR58Eiefsb83OR38YRhImLtwi2z9olIWpQSBJSALSvln27tfNYhrPQCvoFpLXOWVlW+3WuQrf2LTkDtS2+5Ih+un2NwlHUUuUm2biML/uEQ31mYkkmZSmjRTKcLwo/VsDstTTzHx3cvT3m3zEu9/fx+3VDext/26Z0JnALTZPr6fJUYXHYEoghVqqjcDKZUWfyUn0S7W31jZbl+Zki0dHf0eQUOgpFASGpqip2NTbje89Ox0vTV+vLdyF6jW1p0k2X27p89bbdrsubtXpH7rdxBMHLFIlRCvI0g0KSkAJwzN70OfbDt07D0M71BB3Jrm1+0qJBH98wJid2tGJRH/GdyDtas6b/64dH5S1zk2zc2C//cMwgKQE4WsK2SIYlJP18lGTvDSv/sKryVuF40rDumHPrKZhy44mm2zalM4EPbXtJLi66iwoVuwVmMAKcxZqZKDUKlQZdgJNRSJrVaajDY9vMhnDtzpFIHkmg5do8/dkKfLhgI379yhzh8s3KAVotjrpZzjTbnfXQp0LlW4lgr8/n0k2tqYZ8TgMeOEz/Q2JDMQVwX3fCYPSorsQ39kZJe0HkeMO0SCoWn6VWL1KtIDKba7u6MoVjBnfBZ0taLCydqsowun9n4fLNML7rh/eqxjcP6o0XDZYNO2r3NNuuDzrYpixZGIuk1wAXRWm9T0REtqK0fHBYHVdjcybwqG1PPpKC++xqSOOQOz8QK9fRp9DdHN/G8y/iI2lVvtPxZlSgOZ3BVo1FVEYeyOaMip27m0zXufWRnLt2Jy7+2+f4xcn76Z7bN2avc12+1enwmufVaqg9itAiSUgBqCxL4jtH9kffzlWeyxDxIQx6SEQr/hKKgmeuOgyXHdnfcnttk7W+Q6lkIm8eYEVR8Nz3jnRsgx8fSS8abGtdg+36oC2SRoFlZQmWjR/xlr22IqmRskdjdVxN6YypwBp3QA+PrcvH7aH+zzsLddPp2TFn7U7hckV8At2IJ6NAFonatipeRGyf9+hkbK5tfV6cPkZEDsWLmDPdNqPi16/MQe2eZtz+xnxdsnwr7K6H1bHJ+NCjjyQhMeL80X0AtOZbLCQiryfR+YxlNCKhAGOGdsed5xyYZzUzI6kb2s63SIrix0fSS51anzMzgs4jWW5Izh11iyTQ+tGjtfrcfe4I232sgogam82Htv944cGe2+eX6Su3C2/r5mqJWPD8iAwRH8kajfVPe6+J3A+z1+hFs90uoqLYVsD6iNoW+QD0Moe2jGA4Rm0TEjBd9kYS79ejfeB19elUhfl3jMOzVx0WeF0y8kiWJROYd/s4HNq/E759mFxfQEBv0dMNc1t0l9o2ay1OqaRzDjkrC6wbH0ljDV402BYHi2TQQ9vlSQUXH94aqBRWQnKvFkkFGh/JvZ31wK5tdcFWZlj1v01p1VRIRsW1xc396ISzkPQ37Gk+s43+74s0llbtoXmZCdTemig2TG8XUe/KIqnq30ciz63dM2Dl9yrjfoi2jKSQJDFgyo0nYf4d49Cm3NuUeqce0BMActP+OVFVngolf+XRg7oAaPENNEN03u+2FSm88sOjcc95I6W1LYtOPGqbY9E0bZu1nUYqkRCeg9ZIysXQtrGj8XIdu7WvtF3vJ9G2CAlFwfFDuur+DgM/Dv/ZvjQb7GEXlZ3FytJqFbUdlVmunPLCurk7nG8lvxZJs2XWBWqf37SHG8KurRnBaPbmjGr55nM117ahMSIWSbuh7T0WQjIp4UMv6j6SDLYhRU95KoFyH99E9194EE6d3xMnDu8usVX5jOrX0dX2d55zIIb2bI+zRvY23yACHadWxCR1FklztJ19p6pyDO3RHhlVRZe25Y4pV6zK9DO07eUU3nH2AfhwwUbL9WHMbKNNQVQc6X/0Q9u2bTaZflNLc0Y1FeuiH1ZBU5FKYFejdVS26MwmgJhF0s+wp9k1tbO6aX2uvdwPdoIokxG0SNr5KXqI2s4i8tzatS9IiyR9JAmJOO0qUjjnkH1QXek8xZ9Xxg7vgccvO9TVPtWVZfjRmMGWATlRsMBo22BpndRur/mdTCh4+/rj8O7PjkcioeimKDtsQCfLOkf3169zNbQtwUeyd8c22Leb9ZSYwQfbJHTHHF76H69R20rezDYi18wuiMjM+hPk8/D37x4uvK2TRdKYMNwOx6jtjD+L5AMffo0x932MrXUNqN3ThJe+WI3t9eYR0cb2eLnN7UWg2LHYCl2XPpK6oW3fFsnS9ZGkRZKQEPjxiYNzSbllEQEdqUP7Urb2kdQLIK0I0gqwf1x9hGU9L//gKJz64ER8vbEuV462DXbvXOM6r+/4MpsdA7dIJvQWSZHAJhn4ito2JCS3E7+5qG2bbaw67aDYp1Mb4W2dsgg0SGy7Cn/DnpP2zrLy10nLsHLLLrw7b4NjfVlkD22nJfhIujkV+UPbzjvbBRhZDW1L8ZGMto6kRZKQMAjCxyXsecbNqNHkVFRcWiSN71fti7yyzNrfNZFQ0EazXttxHzu4K6bceKKluMob2t7b0Ae/fbBlfWbYBbhsdYjq9ksyoeisdeH5SPoRkkaLpHXXkz0cRVEsg6EaBBN6y8JNiqUKB4tkg8QPDbd5JK1oTquOIhLQv8ekB9tkVEEfSevz58pH0rBpo4Cl2O5jylJISvjQi7pFkkKSkBAI4jVQeBkJbLcQTZZt01otDZ1zs8PQknZz7fk0io1eHdpYiitj55Hd7uyD97Gt24ibAB/ZpBKKYVagaFskFbj0kdRgtZ2ZGAtST7txHzCmZzJiJTi8oKqFm/XEy4eFrY+koL+nNB9JVdWNnLw3z9rvOcvC9bWW66yHtmmRJIRIIIgXQQQMktZC0qJxRh9JLU0uegFth6Oty+k85w1tK+a/nSgLSbyZkUwUJtjGj2DJ6u6skBTtXK2OzdRHMsBPK3dCMjyLpNuE5HblCG2n+W1nGbTC7mMkLRhsYxZoNXPVdoz734n4dMlm4bZ4EcJ/eHuBzgVnT1M6l3DdMthGwkcnLZKEEARhkzR2nKmEgsuP6i+9HjtqG8ynC7Tqdu0siU4WSW2pVn1YNqrUSmTbBdtM/u1JDvW3ElbuRtO6DRbJqAfbANqh7ZYyhC2SFhfSrNOOikWywkE4yLRIZlSx4WBZaOtq9jC2bZ/+RxUK4Eln1LwXzHeemIpFG2tz06mKkM6onu6ZXQ2t1++E+yfgsD98iHU7dgfqI0khSQgJBONLMKOquPLoAQCAM0b2CqUNHdq0RLrn+SRavDuXba5v3cRwAG6ina1eqyJzE2vRNqFnh0rhXKJuIjGvP2mI8LYiKIp/i+Q1xw0EYJ2j1Ayv6X+yc2cDrcE2dkJc+4GUsDi2yUtbBIMu0MpT68Rw44dalrLfVqZFclNtQ8FEhpfsBPZD26qQL7nxPsxkVNTbpFuyrs/1LgCA+sbWj+f1O/cAACZ+vdnyulrdw26IuI6kkCQkLmRUYN9u7bDgjlPx0MWHhFLnM1cdhsMHdMa/fni0brnIq9NobTp9RIv47d/Fef5xqw4nu9hqmDMvIblH+eHGIvnzk/fzVIcVigKDRdL9a/zG04bjg58fj1+eMlR4Hz95JI19qV2be3bQZDdwqLLz3lmtgGCDz9xYlZxEZ4OL9D9ZDrHIQXvDy7PlDG0LjJgYnzmRuamN2I2Gi/pIGoe2vfruej1v9SajMFbD2oCcDxwKSUJKiKuOGWC6PAwfyfYVLdm82pQnQ4voPqRfJ7x07VEY2aejoW3O9Ru1xCH9OuHjG8bg3euPN91eW6RVJ+DUIRrXGvWB6HWSkRvOD1qx4mXoLJFQMKRHe1f7mp3zH44ZJLSv8aPBrN4nLj8U1580BCcMbZ0YwEkkaKf9DNQiKVFIekldlB1pMEPGu0WkDOM2XiySduItLZgT0/hB4/UDJ20zQ44ddW6FpIQbk0PbhJQQt551ABbeeWre8mCitvVvqL9fLZ40OWhEXp5mHe7Arm2Fprq0eq/mLJKiPpIGgaBdm50L2qwTDyt3oxkK9Pk3/fhIuhmyNfbXT191GC4Y3UdgTyXvPJu1eez+PfDzk/fTfYTYdaAdq8qEkuDLwM05dtrWi0XS7sPMaUYoGTQ0p/HJYn0gixch6ZRMXMwimdG9+7xOSerdItly/SYv2ZJbFnTu2KgLSSYkJ0QyZjkQg7ZIvvuz4zCsZ7X8Sjwi0u36yX9o9WJ1KvOCQ/vmkjC3bK9frxWat3/zAFwwug9G7NMhr5yw0/8kFL2Q0w1t+ziPdpa2yrKEznpmjHJtV5EyTb5dllTykjsbm+iUtLu1Tv3f7SpSOYtQpSHNTnSGtu3XexEddkXKTHBuxV8+WoKHPl6iW+Zmqscsdu/BdEbsPWm8J/xYJL2Qvf8ueWJqbpmd36vO51fx5psZbRlJiyQhodC+Uv43WwSy/1giNLTto+M3dji3nbU/urWvwJ3nHAAAGNKjvel+Z43shXvPHynUzrJkAof062QqGsPK3ahtSxZF0QtAKzE4ql9Hx9mU7PRcxzblur+N1qRkQjFNvl1uUqhR7Dol7c6iTTEz57ZTMEgzNWVlWXjdV0IxP1YztFNCmmGM7v3xCYMFyrS2uGZ99k4c1t18AwGembzCdv0/Pl+Zt8xNuq4szlMkilkkRcu0I6OqnszYZj6SdmJeW4XoB5QRVQ1mUgtZUEgSEgAPXHQwzh/dB3eecyB+etIQDO8l31oYhZltrBCzSHov0/hKvfKYgZj2u5MwuHuLgHz4kkNw7iH74M2fHKsvQ1Ewsk+rhdGrmC2okIT+XFi1pX+Xtvj5WPtAH7vj72iI6DZ22ElFMc2ZWFFmtBTm1+OUazGLtsrqyjLdfnazH2nZp6P49IZWJBMKvrz5ZLFtbWbkAYBdhghjbcCQFYpNWFg2YvmWM/fHc9+znlrUD21N3E28DG3b7SMqJNMZfXS3l3yWgPe8qLsa84VkY1rMXcGrkASiHXDDoW1CAuCcQ/bBOYe4my3FLTphFbGXjIg+8yOEzb7OteX16VSF/73oYNN9tYEynoNtQh7aNs4nbpyz3AyvfqpZOlXpBY5x+C6ZUEw7RjOLpLEas21E0NZXUZYUGvI7dEAnrJ2121N9WRIK0LZCrLtMJLLn1bx1xlQ1Ys/K3mtuc4O2rUjhmMFdhdroFrNj9zK0bRTRWlqEpPW+1ZUp1OxpRnNG1flFHjX+v67bAXiP9q5ryD8GO3cF3UefD9/qjKoiEdFxKFokCSlSImyQFMJtkIg+att7vdpAGaOQEkmDYiwjKLTWN+2pUhRFyCIpYm21uwZGi6RZ+eYWyfxlXi2SRrRCsjKVcLQMKwpw4aF9bbcRwc1HT8sc4dbbz169Q7+9SJkC2wQ51F9lJiQ9PIS7bYRki4+keZnHDO6S8wFPZ1Sdddzt0PbRg7rk9vPyFJsNbdtF4mtb58si6XnP4KGQJKRIifTQtkXbfja2NTm3n9b7iWK0syaKFqsVCtnk3rI5VzP/t1GYaP+0EoMJG7+6XDk264Z0b2e7b9Iww04Wo7VRQf7wowwh2aY8iSuOGoB9u7bFT0409zNMKgqOGdw1ENcSKxIOQ9t52wtsrCjOz0uFwxzffmhjIlK9DG3vajKfCQvIRm2br1u0oS53nxuFpFuuPrblec2o3ma2MUv/YzbcnUX7TvEzteo97yzExpo9nvcPEgpJQmJA5Ia2LZZfdUyr6PIjBv0cr/ZlLsNH0ktCcBG0YksrzI0tthIiIsdmdxovPKyvrZhMJsw/GMxEonHoz+vQdrlm1pjKVBIdqsrw3xvGWCZWz56D/p31Se6fueowT/WLsG7Hbld5J8W2VGxFj6LYW8n39ymkzYZuvTyD9hZJax/JLXUNuWHhdEb1PCwNtN4TfhKSG4Vs7R4bIal5yso8fkABwJOfLsd1z33pef8goZAkJAZ0be/ssB8mVp2eNvLVbf437YCunwhGu+Fc0WKTmk47qMAb7bky6i6tVTU7VGdExB/LqjO9YHQf9OlUhWu/YZ1w3EpAm0U35wlJHx1qlk21ztaZ7H3429OG6ZYH+d01c9V2dx8oAtu2WCRtUjWl7Cch2FcT7e4FO99GN9gKSYd5w7PntDmjIu1hVp1cOTnLpjcXmfrG5rz72e78aLc1pqxyy/SV233tHxQMtiGkiPnXD49GfUMzurevLHRTdFh1elpLlJehsSx+fCS1IswopER9JLXH4SchuB1aX0Nd+hwF6N2hEt88qDfalCVx3JBuuPSIfnhu6ird/u0qnOfRtjqPRw9uEad2Z8Mqf2XXdvqUQ4oCNKTlCMl6TaDDxpoGx+2z4mNA17aoKk+2dvgBKsmM6u6ekOEjaeaXqqXGxmImwsINtb72z7LLZgYYp+HqVE4AZnxZJLP3bSajotnDO6i+IZ0nJM38JrNsrWvM/Q4iDVwUoEWSkCJmdP9OOH6/boVuRh6/PKUl7cxFhkAH7ZCfHz8nUcFnhnYI0CgkTzuwZb5vp5QxXTQpW84c2bKPSBoXJ+6/4KDcb63PW3WbVlGoQIGiKPjzxYfgf/bmxDxi33yrZPvKlKMAcbLs2g3/WY3o9+yQ/1FjzLMnmpMxS9Xe9DNaX7R1O50jsbV6zm2dXjl/dB9XPpLCCcptynQ6tto9TeINChA7i6Qxib2RpMaS6Gve972nKq3m+1qKPMP1Dc1oMKT7qbURkpvrWj94gvroLDQUkoQQ6VxwaF989tsTcc95Iyy3afYxPOXHR1Kb/sfYkXznyP546spD8YYh/6SR7tWtYql7+0rMv2Mc3rn+OO+N2sv5mikHy1MJ/PWy0TigdzUeuuQQ2/2MSa6BlllgnNAKxXEH9Mj9zlmUbc6zVafYozpfSDYaLZIufSSzlhytRdJs2D37kTCsZ0s+Ue35tEpSL5Pj9+uGO88+0FUgnN08zVmMkfpGnHJq1uyOhpC0mwHm+WmrLNcBra4aj09c6k9Ianwkje41Ih8bdQ35Q9t2FsktGoukdgi8LKmgR7X9hAHFAoUkISQQ9unYRsocwdmo6JvOGJ5b5mtoO2FtkUwmFJw4rIejZaK7dsYYBagqT+nyLg7s2uKTdune+bq9UJFK4JQDeuKtnx6XS7QOmLvUba9vzFvWvjLl6H6nPfzHvjM6rw47y2+3duadYE+DkFSg5HW8btOgVFe2WGS1FkmzZOuzbz0FM28+GS/+4Cg8euko3Hh66z3zvxcdjFMP6ImXrz3Kl0XbyEF9O+Z+//27h6NNedLVh472I2DMUPPRhYpUwnZY1EkAmU3zWWys3dHiE7ty6y68Pnud53Kyz39zOt8iKSIk6xub8wSxnY/kFo1F8qu1O3O/7zrnQEz57Um2dWXfI1GHQpIQUhBELZK/O304vrz5ZJx1UO/cMj/BNvrhdW9lmE09qPX7+95xA/HO9cfhjrMP9FYBrK1MZtpw+658i1O7ihQGdbNP4aPtR81Ev5Vgv/YbgyzTKJkNbfsNtsmKdG2HbVZGVXkKndqWo0ObMpw2opfuHO7TsQ0eu2w0DhvQ2Vbo3XCK/WxAWn572jB0Nsm3qRUPTmiP6ZmrDjfdpnPbctuPG7vUPyfv3wO3nnWAcHuiym6bFDt2HDNY7/aR9eHdULMn774USaFk5iNpRXVlCifv32LpP3AffeT8nqaMY3R/2DNoeSWenp+EkMgjapVSFCWvE5VlT/I6RNarQ6sPZRsTwde3U5Vl7sI2ZUm0q0xhc22r2Hjx+0eiryFFjdXwr5mVsbuJsK0oS+LQAZ3xwEUHY2DXtrj77QWYunybbhunFChm69tXpPCbU83T7QDmIturkLz5zP3x6IQluPtbLYJcZBjYK306tcHlR/XHyfv3xP3vfy20T0IxF+BODOneDos31QEQO6au7SryZhrSYnYPdm9fgY9vGIOqcvuI7mKh3mRGGSt+efJ++O6xA7FzdxO6t6/A4Jveya3r06kNkokWK/naHXo/2+o2zpJoW30jfvL8TKF2dG1XgZtOH46D+3TE2P17YNSdH+TWiXxsFItPpSeL5MMPP4wBAwagsrISRxxxBKZNmya7XYSQmPKrcUNx+MDO+NYo71NI+slBCbT40LUpS+Kgvt6G/MpTCXxx01hMu+kknSj694+Oxp3nHIjjhlhPVXfj6cN0wRg/OXEwjti3C3obAnycInG1XHJEP3z/+H3xf1e3zrWcreOcQ/bRDb9qcbLsmq0uSyVshcmALm0xSJNuRlGAH3xjX902okLy6mMH4oubxuaG9h+6ZBTKkwmM/5a1761bskPGd587At8/flCeFejXNqI55ZBDtCKVwD+vOQLXn9SaiP/Mkb10Lg92AShZOrctR/tK6yj8A/bJ/2hJZ1S0rUjlrpUxBRIA3P7NA/IC4qJKjYuAoURCQduKFHp3bJNnOU8lE5ZDxqLzsi/Z+xHgRJd25WhbkcKFh/XN+xjWfkhaUZZMFMUMZq6F5Isvvohf/OIXuPXWW/Hll1/ioIMOwrhx47Bp06Yg2kcIiRnXnTAYL/3gKMcAATsyfpwkAbz10+Mw85aTUVXufVCmW/uKvLRLo/p1wmVH9rcVWpcc3k+XEsQqmbZIsEyWyrIkfnf6cByrEbB2wiOL1WnMtt9MaG4z+GMaRV15KoF3rj9et+yQfp3w8Q1jcn+7ybOoPZff2K8b5t4+Dhcf7t33FNAL5Im/OgH//tHRuewH2uHG1647Bj8aYz5rDgAM7dne1NXgwkNbgnx+d/pwHD2oK35+8n4Y2aflo+WXpwzViRujkHzAMEd8ZVkC5akE1mzfZdmOIzVR+0fu2xlAS8Cblh8cvy/e+1nrdenargJXHD0A23fl+9f6IagAEruk30aMt9cBvVuEdq+9bhcnDe9uul/ntq1tP/WAnrntvWJMhfW701vFvHZUw4pEIn+6zf/7fKWvNgWBayH5pz/9Cddccw2uuuoq7L///njsscdQVVWFp556Koj2EUJIHtkgiiuO6u9p/2RC8SVk3TJ2b8f18rVHIZVM5Kxc3zs2f3rF604YhLHDe+C4IeaBF/s5RB/fcub+uOqYATioj97aet0JLYLomxpf02xaHSPZrstMaP7mVL1l6+LD++mio4EWMZm19B0xsEXYaGeX2WUT5eqEjGTmfTq3duKd2pZjVL9Oub+7tatArw6VGNStLUYaglSqDQEvB/buYGox+sO5I/D2T4/D5Zr78/lrjsS0m07CwK5tdW4LVRX6a3DOIftg4Z2n5v7uslfcZANmzOZAz55jAHjiisPw9FWH4Rcn6309FUXB0J7ae6fl4hoj6o2cOKzl3i1LKnjl2qPw4LcPtt3+s9+ciH9ec4Ru2U2aoCcA6NCmDH8419p/WHuMyYSC1647xrZOI0bx9bfLD8V3jxmIF75/JADgt6cOM72PendsFY6PXTba1cecGVcePUD39/ePH4SXrz0KP/jGvviewNSqyzbVocrwnoricLers9TY2IgZM2bgxhtvzC1LJBIYO3YspkyZYrpPQ0MDGhpaTbg1NTUem0oIIS1cfHg/HL9fN/T2aTEIi79edii27WrMWSiuOW5fnDS8BwZ2yR9i+9W4/CFIoGXY/MuV23HWyN6m67N810ScAi2paWb8fqxuiO30Eb3w5pz1OSFy+oie+HLljpzF5oh9O+vKePnao3DYAP0yALj2G/virTnrcYlmyPatnxyHN+asw3eObBFTiYSCk4Z1x8INtRjVv1NeGWEyrGc1HrjoYFOLU3kqgY9vGINUQskLhvjnNUfittfnYe2O3TjtwF7oUFWGg/p0xIcL9CNyZckE9u+tH25uW5FC273CZNyBPXHnW/NxxMDO+M2pw7Bm+25cdmSr6KwsS+LJKw7FM5NX4MK9lsWbzhiObu0rcNlR/fHqzLW4991FAFrEbUeN/2S7ihROGGpucQNaXCn+8t8luUCw3542DHPX1uD7xw/E3W8vzG3XpW05ttY34uyDe+Oeb41AXUMz9u3WDkP3NKEsqaApreL4/brh6EFdsLWuAX+btBzfP35fpJIJjOrXCScO6449TWk8+p3R6NCmDFOXb8OHCzbizJG98NAlo7CnKY0HP1yMTYYh3n6dq/Dgtw/GuY9Mzm0LtPh8Grf9xcn74U8ffI1+natQWZbA1xtbhpyNH0i9O7bBLWftn/tbURTcd/5IvP3VeuzbrR0enbAU1xw3ECcM64673lqQ2+7HJw7G9S/MsjyXQMvMUj07VOLfX67VLf/b5Yea5nc9bEBn3TM0ql9HrNq2Cy/94Ch88vVm9O9ShX9OXY0PF2zE0YO74IiBXXDHm/MBtLjknDeqT16ZhUZRXYQ/rlu3Dvvssw8mT56Mo446Krf817/+NT755BNMnTo1b5/bbrsNt99+e97ynTt3orra3/yfhBBC5KGqat7MLPPW7UTb8hQa0xlba2hjc8bRWqjuTQJtFfFdjDQ0p/H0ZyswZmg3DOsp3qc1pTNIJRTPgTCNzRl8tXYH+naucj2z1c7dTeigSXKvqioURcF/Zq3FgvW1OG/UPujSrgIL1tfg6EFd8tqYyahoTGeQTCi5oLlNtXvQrV2F5fHs3NWEV75cg/NH98nVvXNXE9bX7MZ+3dtj/voadG9fgcryJKory7C1rgHVbcpy5S/cUIO/fLQE+/euxuEDO2Pfrm3RuW05FqyvxaDuLVbeZyavwObaBvzg+EHoYGK5tULb9jlrdqBru4qcz/LCDTV4f95GNDSncekR/fHRwk1oas7gO0f2x5rtu9CnUxUyqoqpy7dhaI/2mLNmB8YM7S5sOU9nVDQ0p3VuNumMigmLNmHEPh3QvboS2+obUV2ZQlpVhSLLZVFTU4MOHTo46rXAhaSZRbJv374UkoQQQgghEUVUSLoa2u7atSuSySQ2btyoW75x40b07NnTdJ+KigpUVMQjezshhBBCCGnF1fhCeXk5Ro8ejY8++ii3LJPJ4KOPPtJZKAkhhBBCSPxxHZL0i1/8AldccQUOPfRQHH744XjggQdQX1+Pq666Koj2EUIIIYSQiOJaSF500UXYvHkzbrnlFmzYsAEHH3ww3n33XfTo0SOI9hFCCCGEkIjiKthGBqLOm4QQQgghpDCI6rX45GAghBBCCCGhQiFJCCGEEEI8QSFJCCGEEEI8QSFJCCGEEEI8QSFJCCGEEEI8QSFJCCGEEEI8QSFJCCGEEEI8QSFJCCGEEEI8QSFJCCGEEEI8QSFJCCGEEEI8QSFJCCGEEEI8QSFJCCGEEEI8QSFJCCGEEEI8QSFJCCGEEEI8QSFJCCGEEEI8QSFJCCGEEEI8QSFJCCGEEEI8QSFJCCGEEEI8QSFJCCGEEEI8QSFJCCGEEEI8kQq7QlVVAQA1NTVhV00IIYQQQgTI6rSsbrMidCFZW1sLAOjbt2/YVRNCCCGEEBfU1taiQ4cOlusV1UlqSiaTyWDdunVo3749FEUJvL6amhr07dsXq1evRnV1deD1kRZ43gsDz3th4HkvDDzvhYHnvTCEfd5VVUVtbS169+6NRMLaEzJ0i2QikUCfPn3CrhbV1dW84QsAz3th4HkvDDzvhYHnvTDwvBeGMM+7nSUyC4NtCCGEEEKIJygkCSGEEEKIJ2IvJCsqKnDrrbeioqKi0E0pKXjeCwPPe2HgeS8MPO+Fgee9MET1vIcebEMIIYQQQuJB7C2ShBBCCCEkGCgkCSGEEEKIJygkCSGEEEKIJygkCSGEEEKIJ2ItJB9++GEMGDAAlZWVOOKIIzBt2rRCN6moGT9+PA477DC0b98e3bt3xznnnINFixbpttmzZw+uu+46dOnSBe3atcN5552HjRs36rZZtWoVzjjjDFRVVaF79+741a9+hebm5jAPpWi55557oCgKfvazn+WW8ZwHx9q1a/Gd73wHXbp0QZs2bTBixAhMnz49t15VVdxyyy3o1asX2rRpg7Fjx2Lx4sW6MrZt24ZLL70U1dXV6NixI66++mrU1dWFfShFQzqdxs0334yBAweiTZs2GDRoEO68807dfL887/6ZOHEizjrrLPTu3RuKouC1117TrZd1jufMmYPjjjsOlZWV6Nu3L+69996gDy3S2J33pqYm/OY3v8GIESPQtm1b9O7dG5dffjnWrVunKyNy512NKS+88IJaXl6uPvXUU+q8efPUa665Ru3YsaO6cePGQjetaBk3bpz69NNPq3PnzlVnzZqlnn766Wq/fv3Uurq63DbXXnut2rdvX/Wjjz5Sp0+frh555JHq0UcfnVvf3NysHnjggerYsWPVmTNnqm+//bbatWtX9cYbbyzEIRUV06ZNUwcMGKCOHDlSvf7663PLec6DYdu2bWr//v3VK6+8Up06daq6bNky9b333lOXLFmS2+aee+5RO3TooL722mvq7Nmz1W9+85vqwIED1d27d+e2OfXUU9WDDjpI/fzzz9VJkyapgwcPVi+++OJCHFJR8Ic//EHt0qWL+uabb6rLly9XX375ZbVdu3bqgw8+mNuG590/b7/9tnrTTTep//73v1UA6quvvqpbL+Mc79y5U+3Ro4d66aWXqnPnzlWff/55tU2bNurjjz8e1mFGDrvzvmPHDnXs2LHqiy++qC5cuFCdMmWKevjhh6ujR4/WlRG18x5bIXn44Yer1113Xe7vdDqt9u7dWx0/fnwBWxUvNm3apAJQP/nkE1VVWx6CsrIy9eWXX85ts2DBAhWAOmXKFFVVWx6iRCKhbtiwIbfNo48+qlZXV6sNDQ3hHkARUVtbqw4ZMkT94IMP1G984xs5IclzHhy/+c1v1GOPPdZyfSaTUXv27Kned999uWU7duxQKyoq1Oeff15VVVWdP3++CkD94osvctu88847qqIo6tq1a4NrfBFzxhlnqN/97nd1y771rW+pl156qaqqPO9BYBQ0ss7xI488onbq1En3nvnNb36jDh06NOAjKg7MBLyRadOmqQDUlStXqqoazfMey6HtxsZGzJgxA2PHjs0tSyQSGDt2LKZMmVLAlsWLnTt3AgA6d+4MAJgxYwaampp0533YsGHo169f7rxPmTIFI0aMQI8ePXLbjBs3DjU1NZg3b97/t3fvsS31fxzA392qtY5ZqbVD9iBk5ppRlppImGAkLnEJaZbyzzI25k5cgj+Gv0j4o0Jc/jAWxGUIMtuQCTOsswUjcU2ouc0219HP88eTnZ/zbPN7nj5ru8v7lZxkPd/vus/3Pev5pD3n8GP1LUtqaiomT56syhZg5r6UnZ0Nq9WKWbNmISIiArGxsdi7d68y/uTJE7jdblX2nTp1QlxcnCr78PBwWK1WZc64ceMQFBSEwsJC/y2mBRk5ciRyc3Px8OFDAEBJSQkKCgqQmJgIgLn7Q1NlfP36dYwePRo6nU6ZM2HCBJSXl+PDhw9+Wk3L9vHjR2g0GoSHhwNonrlrm/wZm4G3b9/i58+fqgMnAJjNZjx48CBAVbUuHo8HS5YsQXx8PAYOHAgAcLvd0Ol0yj/4OmazGW63W5nT0O+lbozqy8rKwp07d1BUVFRvjJn7zuPHj+F0OrFs2TKsXbsWRUVFWLx4MXQ6HRwOh5JdQ9n+mn1ERIRqXKvVonPnzsy+EWvWrEFVVRX69euH4OBg/Pz5ExkZGbDb7QDA3P2gqTJ2u93o1atXveeoGzMajT6pv7X4+vUrVq9ejblz5yIsLAxA88y9VTaS5HupqakoKytDQUFBoEtp1V68eIH09HTk5OSgffv2gS6nTfF4PLBardiyZQsAIDY2FmVlZdi9ezccDkeAq2u9jh49iszMTBw+fBgDBgyAy+XCkiVL0K1bN+ZObUZtbS1mz54NEYHT6Qx0Ob/VKj/aNplMCA4Ornfl6uvXr2GxWAJUVeuRlpaGs2fPIj8/Hz169FD2WywWfP/+HZWVlar5v+ZusVga/L3UjZHa7du3UVFRgaFDh0Kr1UKr1eLKlSvYuXMntFotzGYzM/eRyMhI9O/fX7UvJiYGz58/B/C/7H73OmOxWFBRUaEa//HjB96/f8/sG7Fy5UqsWbMGc+bMwaBBg5CUlISlS5di69atAJi7PzRVxnzt8U5dE/ns2TPk5OQo70YCzTP3VtlI6nQ6DBs2DLm5uco+j8eD3Nxc2Gy2AFbWsokI0tLScPLkSeTl5dV763zYsGFo166dKvfy8nI8f/5cyd1ms6G0tFT1h1D3h/L3gzYBCQkJKC0thcvlUjar1Qq73a58zcx9Iz4+vt7trR4+fIg//vgDANCrVy9YLBZV9lVVVSgsLFRlX1lZidu3bytz8vLy4PF4EBcX54dVtDyfP39GUJD60BQcHAyPxwOAuftDU2Vss9lw9epV1NbWKnNycnIQHR3Nj7UbUddEPnr0CJcuXUKXLl1U480yd59cwtMMZGVliV6vl4MHD8q9e/ckOTlZwsPDVVeu0r+zYMEC6dSpk1y+fFlevXqlbJ8/f1bmpKSkSFRUlOTl5cmtW7fEZrOJzWZTxutuRTN+/HhxuVxy4cIF6dq1K29F8y/8etW2CDP3lZs3b4pWq5WMjAx59OiRZGZmisFgkEOHDilztm3bJuHh4XL69Gm5e/euTJ06tcFbpMTGxkphYaEUFBRI3759eRua33A4HNK9e3fl9j8nTpwQk8kkq1atUuYw9/+uurpaiouLpbi4WADI9u3bpbi4WLk6uCkyrqysFLPZLElJSVJWViZZWVliMBja9O1/fpf79+/fZcqUKdKjRw9xuVyq4+yvV2A3t9xbbSMpIrJr1y6JiooSnU4nI0aMkBs3bgS6pBYNQIPbgQMHlDlfvnyRhQsXitFoFIPBINOnT5dXr16pnufp06eSmJgoISEhYjKZZPny5VJbW+vn1bRcf28kmbnvnDlzRgYOHCh6vV769esne/bsUY17PB7ZsGGDmM1m0ev1kpCQIOXl5ao57969k7lz50qHDh0kLCxM5s+fL9XV1f5cRotSVVUl6enpEhUVJe3bt5fevXvLunXrVAdS5v7f5efnN/h67nA4RKTpMi4pKZFRo0aJXq+X7t27y7Zt2/y1xGbpd7k/efKk0eNsfn6+8hzNLXeNyC//XQARERER0T/UKs+RJCIiIiLfYyNJRERERF5hI0lEREREXmEjSUREREReYSNJRERERF5hI0lEREREXmEjSUREREReYSNJRERERF5hI0lEFCAajQanTp0KdBlERF5jI0lEbdK8efOg0WjqbRMnTgx0aURELYY20AUQEQXKxIkTceDAAdU+vV4foGqIiFoeviNJRG2WXq+HxWJRbUajEcBfHzs7nU4kJiYiJCQEvXv3xvHjx1XfX1pairFjxyIkJARdunRBcnIyampqVHP279+PAQMGQK/XIzIyEmlpaarxt2/fYvr06TAYDOjbty+ys7N9u2gioibERpKIqBEbNmzAjBkzUFJSArvdjjlz5uD+/fsAgE+fPmHChAkwGo0oKirCsWPHcOnSJVWj6HQ6kZqaiuTkZJSWliI7Oxt9+vRR/YzNmzdj9uzZuHv3LiZNmgS73Y7379/7dZ1ERF4TIqI2yOFwSHBwsISGhqq2jIwMEREBICkpKarviYuLkwULFoiIyJ49e8RoNEpNTY0yfu7cOQkKChK32y0iIt26dZN169Y1WgMAWb9+vfK4pqZGAMj58+ebbJ1ERL7EcySJqM0aM2YMnE6nal/nzp2Vr202m2rMZrPB5XIBAO7fv48hQ4YgNDRUGY+Pj4fH40F5eTk0Gg1evnyJhISE39YwePBg5evQ0FCEhYWhoqLC2yUREfkVG0kiarNCQ0PrfdTcVEJCQv7RvHbt2qkeazQaeDweX5RERNTkeI4kEVEjbty4Ue9xTEwMACAmJgYlJSX49OmTMn7t2jUEBQUhOjoaHTt2RM+ePZGbm+vXmomI/InvSBJRm/Xt2ze43W7VPq1WC5PJBAA4duwYrFYrRo0ahczMTNy8eRP79u0DANjtdmzcuBEOhwObNm3CmzdvsGjRIiQlJcFsNgMANm3ahJSUFERERCAxMRHV1dW4du0aFi1a5N+FEhH5CBtJImqzLly4gMjISNW+6OhoPHjwAMBfV1RnZWVh4cKFiIyMxJEjR9C/f38AgMFgwMWLF5Geno7hw4fDYDBgxowZ2L59u/JcDocDX79+xY4dO7BixQqYTCbMnDnTfwskIvIxjYhIoIsgImpuNBoNTp48iWnTpgW6FCKiZovnSBIRERGRV9hIEhEREZFXeI4kEVEDeNYPEdH/x3ckiYiIiMgrbCSJiIiIyCtsJImIiIjIK2wkiYiIiMgrbCSJiIiIyCtsJImIiIjIK2wkiYiIiMgrbCSJiIiIyCt/AnFqr+bUpiybAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.title(\"Loss Curve\")\n",
    "plt.plot(loss_logger, label='train_loss')\n",
    "# plt.plot(accuracy_logger,label='accuracy')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save and Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.5103,  0.2428, -1.1660,  ...,  0.5757,  0.2179,  0.0081],\n",
       "         [ 0.5106,  0.2437, -1.2850,  ...,  0.5751,  0.2183,  0.0099],\n",
       "         [ 0.5112,  0.2443, -1.2880,  ...,  0.5748,  0.2189,  0.0108],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 0.5442,  0.2507, -1.4443,  ...,  0.6105,  0.2203,  0.0123],\n",
       "         [ 0.5444,  0.2508, -1.6574,  ...,  0.6094,  0.2198,  0.0134],\n",
       "         [ 0.5445,  0.2510, -1.7250,  ...,  0.6093,  0.2195,  0.0144],\n",
       "         ...,\n",
       "         [ 0.5324,  0.2577, -1.6704,  ...,  0.5975,  0.2210,  0.0145],\n",
       "         [ 0.5323,  0.2560, -1.6673,  ...,  0.5966,  0.2206,  0.0148],\n",
       "         [ 0.5323,  0.2553, -1.6826,  ...,  0.5957,  0.2201,  0.0146]],\n",
       "\n",
       "        [[ 0.5097,  0.2301, -1.2305,  ...,  0.5773,  0.2070,  0.0135],\n",
       "         [ 0.5098,  0.2316, -1.4094,  ...,  0.5762,  0.2072,  0.0130],\n",
       "         [ 0.5099,  0.2328, -1.4219,  ...,  0.5754,  0.2074,  0.0134],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 0.5122,  0.2456, -1.3706,  ...,  0.5836,  0.2207,  0.0125],\n",
       "         [ 0.5121,  0.2461, -1.4417,  ...,  0.5818,  0.2208,  0.0118],\n",
       "         [ 0.5121,  0.2465, -1.4591,  ...,  0.5815,  0.2204,  0.0118],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_sequence(sequences, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 23.3885,  -5.1714,   9.4786,  ...,   0.4380,  -5.5845,  -2.4367],\n",
       "        [ -6.4547,  17.6740, -14.5933,  ...,   0.2171,  -2.4432, -12.3853],\n",
       "        [ 10.7881, -17.0878,  24.3860,  ...,  -5.7952,   2.9442,  10.6538],\n",
       "        ...,\n",
       "        [ -5.5565,  -2.8786,  -5.4713,  ...,  17.9166, -16.9380,  -0.6132],\n",
       "        [ -8.8911,  -8.6660,   0.3260,  ...,  -6.2111,  19.6814,   9.5295],\n",
       "        [-13.8861, -20.9090,  13.0773,  ...,  -3.3227,  12.8458,  22.3281]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Put the model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# No need to track gradients during inference\n",
    "with torch.no_grad():\n",
    "    # Get the model's output (logits)\n",
    "    outputs = model(padded_sequences.to(device))\n",
    "\n",
    "# outputs = torch.softmax(outputs, dim=1)\n",
    "# outputs = torch.max(outputs,1)\n",
    "\n",
    "outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.5013,  0.2452, -1.2167,  ...,  0.5663,  0.2188,  0.0098],\n",
       "         [ 0.4997,  0.2482, -1.4690,  ...,  0.5652,  0.2181,  0.0106],\n",
       "         [ 0.4984,  0.2500, -1.4853,  ...,  0.5654,  0.2185,  0.0112],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 0.4922,  0.2382, -1.2850,  ...,  0.5578,  0.2124,  0.0094],\n",
       "         [ 0.4920,  0.2405, -1.4288,  ...,  0.5571,  0.2116,  0.0099],\n",
       "         [ 0.4920,  0.2409, -1.4093,  ...,  0.5567,  0.2122,  0.0098],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 0.5049,  0.2371, -1.2115,  ...,  0.5643,  0.2082,  0.0088],\n",
       "         [ 0.5045,  0.2381, -1.1896,  ...,  0.5643,  0.2081,  0.0085],\n",
       "         [ 0.5041,  0.2385, -1.1915,  ...,  0.5643,  0.2080,  0.0089],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.4849,  0.2484, -1.3873,  ...,  0.5543,  0.2162,  0.0108],\n",
       "         [ 0.4846,  0.2500, -1.4881,  ...,  0.5539,  0.2157,  0.0111],\n",
       "         [ 0.4844,  0.2507, -1.4935,  ...,  0.5537,  0.2161,  0.0111],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 0.5199,  0.2290, -1.2810,  ...,  0.5876,  0.2058,  0.0103],\n",
       "         [ 0.5195,  0.2307, -1.3821,  ...,  0.5873,  0.2059,  0.0106],\n",
       "         [ 0.5193,  0.2327, -1.4100,  ...,  0.5877,  0.2059,  0.0109],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 0.5005,  0.2439, -1.3629,  ...,  0.5694,  0.2139,  0.0154],\n",
       "         [ 0.5002,  0.2448, -1.5049,  ...,  0.5688,  0.2128,  0.0146],\n",
       "         [ 0.4999,  0.2457, -1.5287,  ...,  0.5683,  0.2134,  0.0148],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = [\"Data for different actions/เครียด.mp4/เครียด.npy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.5035,  0.2282, -1.1379,  ...,  0.5710,  0.1953,  0.0120],\n",
       "         [ 0.5033,  0.2284, -1.2771,  ...,  0.5713,  0.1954,  0.0123],\n",
       "         [ 0.5032,  0.2286, -1.2743,  ...,  0.5715,  0.1957,  0.0122],\n",
       "         ...,\n",
       "         [ 0.4810,  0.2296, -1.4129,  ...,  0.5542,  0.1897,  0.0133],\n",
       "         [ 0.4812,  0.2296, -1.4275,  ...,  0.5549,  0.1900,  0.0130],\n",
       "         [ 0.4816,  0.2299, -1.4337,  ...,  0.5557,  0.1903,  0.0133]]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the sequences\n",
    "import torch\n",
    "sequences = load_keypoint_sequences(file_paths)\n",
    "# Change list to numpy array \n",
    "sequences = np.array(sequences)\n",
    "# Change numpy array to tensor\n",
    "sequences = torch.FloatTensor(sequences)\n",
    "sequences = pad_sequence(sequences, batch_first=True)\n",
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -8.6970,  -2.5494,  -6.3655,   7.5547,  -1.4360,  -4.3802,  -9.5969,\n",
       "         -17.8804,  -1.2089,  -1.0139, -10.7713,   7.5410,  14.4671,  -5.2191,\n",
       "          -0.4342,   1.8465,  19.5008, -11.0712, -12.3366,  12.8540,  13.5287,\n",
       "         -11.2111, -20.0616, -17.6507,   9.9648,   5.6315,  -4.8823,  -2.0016,\n",
       "         -11.5639,   6.6041,   1.3654,   6.3328,   9.5502,  -7.9724,  -2.7140,\n",
       "           5.7531,   0.5599,  11.2523, -11.5018,   2.9860]], device='cuda:0',\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model(sequences.to(device))\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['กฎกระทรวง',\n",
       " 'กฎหมายรัฐธรรมนูญ',\n",
       " 'กรมอนามัย',\n",
       " 'กรรม',\n",
       " 'กรรมสิทธิ์',\n",
       " 'กระโดด',\n",
       " 'กล้วยบวชชี',\n",
       " 'กล้วยเชื่อม',\n",
       " 'กังวล',\n",
       " 'กีฬา',\n",
       " 'น้อง',\n",
       " 'เขิน',\n",
       " 'เขื่อนดิน',\n",
       " 'เขื่อนสิริกิติ์',\n",
       " 'เข้าใจผิด',\n",
       " 'เคย',\n",
       " 'เครียด',\n",
       " 'เครื่องปั่นดิน',\n",
       " 'เครื่องหมายการค้า',\n",
       " 'เจอ',\n",
       " 'เจ้าหนี้',\n",
       " 'เช่าซื้อ',\n",
       " 'เช่าทรัพย์',\n",
       " 'เซอร์เบีย',\n",
       " 'เซเนกัล',\n",
       " 'เซ็ง',\n",
       " 'เดิน',\n",
       " 'เดิมพัน',\n",
       " 'เพลีย',\n",
       " 'เมื่อย',\n",
       " 'เม็กซิโก',\n",
       " 'เฮโรอีน',\n",
       " 'แกมเบีย',\n",
       " 'แซมเบีย',\n",
       " 'โกหก',\n",
       " 'โจทก์',\n",
       " 'โชจู',\n",
       " 'ใกล้',\n",
       " 'ไดโนเสาร์',\n",
       " 'ไอซ์']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = [action.split(\".\")[0] for action in actions]\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change from tensor to numpy arrat\n",
    "outputs = outputs.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -8.696959 ,  -2.5494204,  -6.365549 ,   7.554743 ,  -1.4359683,\n",
       "         -4.3802094,  -9.596939 , -17.880407 ,  -1.2089441,  -1.0138978,\n",
       "        -10.771273 ,   7.540975 ,  14.467055 ,  -5.219145 ,  -0.4341802,\n",
       "          1.8465078,  19.50076  , -11.071238 , -12.3365755,  12.854005 ,\n",
       "         13.528713 , -11.211076 , -20.061558 , -17.650719 ,   9.9647665,\n",
       "          5.6315413,  -4.882337 ,  -2.001557 , -11.563926 ,   6.60413  ,\n",
       "          1.365406 ,   6.3327746,   9.550167 ,  -7.972444 ,  -2.713959 ,\n",
       "          5.7530932,   0.5599291,  11.252278 , -11.501826 ,   2.9860332]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -8.696959   -2.5494204  -6.365549    7.554743   -1.4359683  -4.3802094\n",
      "  -9.596939  -17.880407   -1.2089441  -1.0138978 -10.771273    7.540975\n",
      "  14.467055   -5.219145   -0.4341802   1.8465078  19.50076   -11.071238\n",
      " -12.3365755  12.854005   13.528713  -11.211076  -20.061558  -17.650719\n",
      "   9.9647665   5.6315413  -4.882337   -2.001557  -11.563926    6.60413\n",
      "   1.365406    6.3327746   9.550167   -7.972444   -2.713959    5.7530932\n",
      "   0.5599291  11.252278  -11.501826    2.9860332]\n"
     ]
    }
   ],
   "source": [
    "for idx, word in enumerate(outputs):\n",
    "    # max_value = torch.max(outputs)\n",
    "    list_outputs = max(outputs)\n",
    "    print(list_outputs)\n",
    "    # print(max_value)\n",
    "    # print(max_value.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_max = max(range(len(list_outputs)), key=list_outputs.__getitem__)\n",
    "index_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "เครียด\n"
     ]
    }
   ],
   "source": [
    "print(labels[index_max])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : กฎกระทรวง Predicted : กฎกระทรวง\n",
      "Input : กฎหมายรัฐธรรมนูญ Predicted : กฎหมายรัฐธรรมนูญ\n",
      "Input : กรมอนามัย Predicted : กรมอนามัย\n",
      "Input : กรรม Predicted : กรรม\n",
      "Input : กรรมสิทธิ์ Predicted : กรรมสิทธิ์\n",
      "Input : กระโดด Predicted : กระโดด\n",
      "Input : กล้วยบวชชี Predicted : กล้วยบวชชี\n",
      "Input : กล้วยเชื่อม Predicted : กล้วยเชื่อม\n",
      "Input : กังวล Predicted : กังวล\n",
      "Input : กีฬา Predicted : กีฬา\n",
      "Input : น้อง Predicted : น้อง\n",
      "Input : เขิน Predicted : เขิน\n",
      "Input : เขื่อนดิน Predicted : เขื่อนดิน\n",
      "Input : เขื่อนสิริกิติ์ Predicted : เขื่อนสิริกิติ์\n",
      "Input : เข้าใจผิด Predicted : เข้าใจผิด\n",
      "Input : เคย Predicted : เขื่อนดิน\n",
      "Input : เครียด Predicted : เครียด\n",
      "Input : เครื่องปั่นดิน Predicted : เครื่องปั่นดิน\n",
      "Input : เครื่องหมายการค้า Predicted : เครื่องหมายการค้า\n",
      "Input : เจอ Predicted : เครียด\n",
      "Input : เจ้าหนี้ Predicted : เจ้าหนี้\n",
      "Input : เช่าซื้อ Predicted : เช่าซื้อ\n",
      "Input : เช่าทรัพย์ Predicted : เช่าทรัพย์\n",
      "Input : เซอร์เบีย Predicted : เซอร์เบีย\n",
      "Input : เซเนกัล Predicted : เซเนกัล\n",
      "Input : เซ็ง Predicted : เขิน\n",
      "Input : เดิน Predicted : เดิน\n",
      "Input : เดิมพัน Predicted : เม็กซิโก\n",
      "Input : เพลีย Predicted : กฎกระทรวง\n",
      "Input : เมื่อย Predicted : เมื่อย\n",
      "Input : เม็กซิโก Predicted : เม็กซิโก\n",
      "Input : เฮโรอีน Predicted : เฮโรอีน\n",
      "Input : แกมเบีย Predicted : แกมเบีย\n",
      "Input : แซมเบีย Predicted : แซมเบีย\n",
      "Input : โกหก Predicted : โกหก\n",
      "Input : โจทก์ Predicted : เม็กซิโก\n",
      "Input : โชจู Predicted : โชจู\n",
      "Input : ใกล้ Predicted : เขื่อนดิน\n",
      "Input : ไดโนเสาร์ Predicted : ไดโนเสาร์\n",
      "Input : ไอซ์ Predicted : ไอซ์\n",
      "Correct Predicted on Training set : 33 Corrct percentage : 82.5%\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "num_clip = 0\n",
    "for clip in labels:\n",
    "    # print(clip)\n",
    "    file_paths = [f\"Data for different actions/{clip}.mp4/{clip}.npy\"]\n",
    "    # print(file_paths)\n",
    "\n",
    "    sequences = load_keypoint_sequences(file_paths)\n",
    "    # Change list to numpy array \n",
    "    sequences = np.array(sequences)\n",
    "    # Change numpy array to tensor\n",
    "    sequences = torch.FloatTensor(sequences)\n",
    "    sequences = pad_sequence(sequences, batch_first=True)\n",
    "\n",
    "    outputs = model(sequences.to(device))\n",
    "\n",
    "    for idx, word in enumerate(outputs):\n",
    "        # max_value = torch.max(outputs)\n",
    "        list_outputs = max(outputs)\n",
    "\n",
    "    index_max = max(range(len(list_outputs)), key=list_outputs.__getitem__)\n",
    "\n",
    "    print(f\"Input : {clip} Predicted : {labels[index_max]}\")\n",
    "\n",
    "    if clip == labels[index_max]:\n",
    "        correct = correct+1\n",
    "    \n",
    "    num_clip = num_clip + 1 \n",
    "\n",
    "print(f\"Correct Predicted on Training set : {correct} Corrct percentage : {correct*100/num_clip}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "FILE = \"model.pth\"\n",
    "torch.save(model.state_dict(), FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = LSTMModel(input_size=1662, hidden_size=256, num_layers=2, num_classes=40, dropout=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMModel(\n",
       "  (lstm): LSTM(1662, 256, num_layers=2, batch_first=True)\n",
       "  (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "  (fc2): Linear(in_features=256, out_features=40, bias=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model.load_state_dict(torch.load(FILE))\n",
    "loaded_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0454, -0.0832,  0.0849,  ...,  0.0172, -0.0027, -0.0738],\n",
      "        [-0.0043,  0.0195,  0.0432,  ..., -0.0926,  0.0158, -0.0690],\n",
      "        [ 0.0689,  0.0484, -0.0838,  ...,  0.0769,  0.0105, -0.0355],\n",
      "        ...,\n",
      "        [ 0.0845,  0.0110, -0.0230,  ...,  0.0400, -0.0170, -0.0040],\n",
      "        [-0.0085, -0.0809, -0.0333,  ..., -0.0100, -0.0178,  0.0294],\n",
      "        [-0.0166, -0.0403,  0.0752,  ..., -0.0377, -0.0322, -0.0732]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-3.8196e-02, -6.7619e-02, -1.8453e-01,  ...,  4.0731e-02,\n",
      "         -1.0970e-01,  1.7389e-03],\n",
      "        [-1.6284e-01,  3.5712e-02, -3.0034e-01,  ...,  7.8925e-02,\n",
      "         -2.8060e-01, -1.3662e-01],\n",
      "        [ 8.1770e-05, -7.9559e-02,  8.7201e-02,  ..., -5.8205e-02,\n",
      "          6.7485e-02, -7.3471e-02],\n",
      "        ...,\n",
      "        [-5.2455e-02, -2.4539e-02, -1.4663e-01,  ...,  1.3414e-01,\n",
      "         -1.0900e-01, -6.5919e-02],\n",
      "        [-1.4281e-01, -1.1980e-02, -5.6796e-02,  ...,  2.2399e-02,\n",
      "         -1.7692e-01, -1.0423e-01],\n",
      "        [-8.3271e-02, -3.1705e-02, -1.4520e-01,  ...,  5.6566e-02,\n",
      "         -2.3145e-01, -1.3825e-01]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.2491, -0.1567, -0.0217,  ..., -0.0640, -0.1338, -0.0924],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.2723, -0.0547, -0.0632,  ..., -0.0823, -0.0955, -0.1745],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-2.6528e-02,  2.4395e-03, -9.7770e-02,  ...,  1.7202e-01,\n",
      "         -1.2711e-01, -6.2873e-02],\n",
      "        [-1.2664e-01,  9.2037e-02, -2.2923e-01,  ...,  3.0904e-02,\n",
      "         -2.6102e-01, -7.2050e-02],\n",
      "        [-9.3912e-02, -7.3391e-02, -5.6187e-02,  ..., -6.2088e-03,\n",
      "         -1.1685e-01, -7.5722e-02],\n",
      "        ...,\n",
      "        [-8.6636e-02, -4.2857e-03, -1.4907e-01,  ...,  7.2883e-02,\n",
      "         -1.6568e-01, -1.3588e-01],\n",
      "        [-1.1577e-01, -4.5835e-02, -6.2416e-02,  ...,  1.3432e-02,\n",
      "         -1.9224e-02, -9.7254e-05],\n",
      "        [-1.4428e-01, -2.1675e-04, -1.0831e-01,  ..., -2.8239e-02,\n",
      "         -1.0344e-01, -5.1191e-02]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0655,  0.0155,  0.0216,  ..., -0.0250, -0.0866,  0.0713],\n",
      "        [ 0.0709,  0.0321,  0.0678,  ...,  0.0730, -0.1402,  0.1263],\n",
      "        [ 0.0249,  0.0955, -0.1106,  ...,  0.1723, -0.1344,  0.1628],\n",
      "        ...,\n",
      "        [ 0.0599, -0.0225,  0.1171,  ...,  0.0010, -0.1982,  0.0724],\n",
      "        [-0.0078,  0.0544,  0.0151,  ...,  0.0785, -0.0942,  0.2426],\n",
      "        [-0.0873,  0.1210,  0.0224,  ..., -0.0394,  0.0872,  0.1030]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0400, -0.0304, -0.1024,  ...,  0.0942, -0.1903, -0.1119],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0279,  0.0286,  0.0175,  ...,  0.1196, -0.1589, -0.1240],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0111, -0.0214,  0.0779,  ..., -0.0573,  0.0679,  0.0564],\n",
      "        [-0.0729, -0.0307, -0.0533,  ..., -0.0825, -0.0761, -0.0242],\n",
      "        [-0.0161,  0.0334,  0.0427,  ..., -0.0262,  0.0610,  0.0060],\n",
      "        ...,\n",
      "        [ 0.0110,  0.0139, -0.0113,  ..., -0.0130,  0.0803,  0.0235],\n",
      "        [ 0.0326, -0.0088, -0.0944,  ..., -0.0786, -0.0239,  0.0383],\n",
      "        [ 0.0484,  0.0032,  0.0905,  ..., -0.0790, -0.0046, -0.0517]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-3.4574e-02, -7.5441e-03, -8.0070e-02, -3.4012e-02, -4.2709e-02,\n",
      "         2.2317e-02, -6.5025e-02,  1.7660e-02,  1.1270e-02,  2.8715e-02,\n",
      "        -1.0065e-02, -4.9425e-02, -3.5450e-02,  7.8810e-02,  3.2130e-02,\n",
      "         9.3920e-03, -7.8406e-03, -6.0971e-03,  6.5147e-02,  6.2678e-02,\n",
      "         2.3195e-02, -7.7684e-03,  1.1669e-02, -9.6767e-02,  6.0400e-02,\n",
      "         4.0700e-02, -4.6435e-03, -5.1011e-03, -1.6692e-02,  1.4919e-02,\n",
      "        -4.7672e-02, -3.0273e-02,  1.2158e-02,  3.7110e-02, -6.1718e-02,\n",
      "        -2.5557e-03, -6.9231e-02,  8.9366e-03, -2.7915e-03, -3.1314e-02,\n",
      "        -1.7681e-02, -5.8437e-03, -4.0288e-02, -1.0352e-02,  3.6370e-02,\n",
      "        -1.1270e-03,  4.0106e-02, -7.5644e-02, -4.5901e-02, -1.2258e-02,\n",
      "        -2.3655e-02, -4.1966e-02,  9.1796e-02,  1.1747e-02, -8.2012e-03,\n",
      "         5.9375e-02,  6.8648e-02, -3.4903e-02, -6.2531e-02, -5.2697e-02,\n",
      "        -9.5226e-02, -1.4197e-02,  8.9350e-03, -1.0913e-01, -3.9740e-02,\n",
      "         1.1115e-05,  6.2981e-03,  1.8775e-02, -7.4774e-02, -6.1658e-02,\n",
      "        -2.7940e-02, -1.3120e-02, -2.4317e-02,  5.8487e-02,  2.7788e-02,\n",
      "         1.9668e-02, -1.4222e-02, -2.4206e-02,  4.7811e-02,  4.7106e-02,\n",
      "        -3.0664e-02,  6.2794e-02,  1.7189e-03,  3.7091e-02, -1.2742e-02,\n",
      "        -2.4593e-02,  7.5227e-02,  5.5454e-02, -2.7774e-02,  4.4833e-02,\n",
      "        -4.0477e-02, -1.1130e-02,  2.7711e-02,  2.1187e-02, -1.7917e-02,\n",
      "         9.5554e-03, -1.0621e-02, -9.0037e-02, -2.3957e-02, -5.0970e-02,\n",
      "         1.2353e-03, -4.0699e-02, -2.0105e-02, -1.9298e-02,  8.3670e-02,\n",
      "        -1.0855e-02,  1.5858e-02,  5.7019e-02, -7.3413e-02, -5.8234e-02,\n",
      "         1.1916e-03, -2.5985e-02,  4.6392e-02, -4.8957e-02,  6.7888e-02,\n",
      "         3.9164e-02, -2.7265e-02,  6.4450e-02,  7.0896e-02,  3.8694e-03,\n",
      "         7.8654e-02,  4.9626e-02,  6.0647e-02, -1.1176e-02,  3.1102e-02,\n",
      "        -1.3979e-02,  3.5472e-02,  2.5602e-02, -6.4245e-02,  1.3342e-02,\n",
      "         7.7677e-02, -5.5935e-02, -4.4800e-03,  2.6587e-02,  3.3527e-02,\n",
      "        -5.4833e-02,  4.5567e-02, -9.1367e-03,  2.7839e-02, -6.6854e-03,\n",
      "         5.5554e-02,  1.6483e-04, -1.3802e-02,  7.1617e-02, -3.8032e-02,\n",
      "        -6.7846e-02, -1.7028e-02, -1.0809e-02,  6.0870e-02, -6.1028e-02,\n",
      "         1.1712e-03, -4.4035e-02,  2.9310e-02, -2.0857e-03, -7.8184e-03,\n",
      "        -1.3887e-02, -3.4133e-02, -8.5426e-02,  8.5916e-02, -5.2908e-02,\n",
      "        -2.4893e-03, -1.4949e-02, -3.3741e-02, -9.6029e-02,  4.9869e-02,\n",
      "        -1.6416e-02, -8.5573e-04, -3.0947e-03, -4.3437e-02,  4.5480e-02,\n",
      "        -6.8883e-02,  1.2238e-02, -9.1837e-03,  1.3408e-02, -1.1305e-01,\n",
      "         1.2671e-01, -3.3206e-02, -7.6564e-03,  1.0138e-01,  6.4172e-04,\n",
      "        -1.3753e-02,  1.0786e-02,  9.5883e-03,  1.9478e-02, -1.0028e-02,\n",
      "         4.7250e-02, -1.2049e-02, -6.8203e-02,  1.4220e-02,  1.6864e-02,\n",
      "         3.4031e-02, -1.3561e-02,  2.5853e-02,  1.9034e-02, -4.2687e-02,\n",
      "         6.3737e-02,  7.1170e-02, -5.1677e-02,  2.4240e-02, -7.5582e-03,\n",
      "         2.6787e-02, -3.6843e-02,  1.7728e-02, -1.6466e-03, -5.6384e-02,\n",
      "         4.7742e-02, -1.0346e-02, -8.4314e-02, -2.1313e-02,  5.6552e-02,\n",
      "         7.8522e-02, -7.6503e-02, -8.7966e-02, -2.7776e-02, -3.1033e-02,\n",
      "         2.8013e-02, -8.0613e-03, -9.6433e-03, -8.8593e-02, -4.7548e-02,\n",
      "         3.0647e-02,  3.7387e-02, -6.7799e-03, -1.2656e-02,  4.8194e-02,\n",
      "         3.1528e-02, -4.8574e-02, -1.1072e-02,  3.1286e-02,  1.5454e-02,\n",
      "         9.6303e-02,  3.6493e-03,  1.0481e-01,  8.4897e-02, -2.2491e-02,\n",
      "        -2.3291e-02,  1.7215e-02,  1.7824e-02,  8.9043e-02,  3.2187e-02,\n",
      "         2.9800e-02, -1.8472e-04, -9.2980e-02,  4.2576e-02,  1.7258e-02,\n",
      "         2.8732e-02,  6.6261e-02, -5.7624e-02, -7.1779e-02,  2.6179e-02,\n",
      "         3.9608e-02,  4.5632e-03, -3.5911e-02, -9.6481e-02, -2.3191e-02,\n",
      "         1.8649e-02], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0110, -0.0324, -0.1010,  ..., -0.0408, -0.1102, -0.2243],\n",
      "        [ 0.0931, -0.1354, -0.1578,  ..., -0.1748, -0.2375, -0.1166],\n",
      "        [-0.2166,  0.1185,  0.1365,  ...,  0.0053, -0.0274, -0.0167],\n",
      "        ...,\n",
      "        [ 0.0756, -0.0727, -0.0147,  ...,  0.0191,  0.1004,  0.0217],\n",
      "        [-0.1277,  0.1344,  0.0736,  ...,  0.0863,  0.1024,  0.0302],\n",
      "        [-0.1616,  0.0910,  0.1198,  ...,  0.1159,  0.1430,  0.0921]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.1002, -0.0460, -0.0099, -0.0271, -0.0167,  0.0048, -0.0988,  0.0130,\n",
      "        -0.1249, -0.0624, -0.0578,  0.0129, -0.1017,  0.0701, -0.0216, -0.0376,\n",
      "        -0.0216, -0.0333, -0.0035, -0.1247, -0.0432, -0.0215, -0.1629, -0.0655,\n",
      "         0.0784,  0.0184, -0.0237, -0.0061, -0.0353,  0.1046, -0.0025,  0.0582,\n",
      "         0.0093, -0.0014,  0.0302, -0.0311, -0.0472,  0.0265, -0.0016,  0.0382],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for param in loaded_model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# FILE = \"model.pth\"\n",
    "# torch.load(FILE)\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0454, -0.0832,  0.0849,  ...,  0.0172, -0.0027, -0.0738],\n",
      "        [-0.0043,  0.0195,  0.0432,  ..., -0.0926,  0.0158, -0.0690],\n",
      "        [ 0.0689,  0.0484, -0.0838,  ...,  0.0769,  0.0105, -0.0355],\n",
      "        ...,\n",
      "        [ 0.0845,  0.0110, -0.0230,  ...,  0.0400, -0.0170, -0.0040],\n",
      "        [-0.0085, -0.0809, -0.0333,  ..., -0.0100, -0.0178,  0.0294],\n",
      "        [-0.0166, -0.0403,  0.0752,  ..., -0.0377, -0.0322, -0.0732]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-3.8196e-02, -6.7619e-02, -1.8453e-01,  ...,  4.0731e-02,\n",
      "         -1.0970e-01,  1.7389e-03],\n",
      "        [-1.6284e-01,  3.5712e-02, -3.0034e-01,  ...,  7.8925e-02,\n",
      "         -2.8060e-01, -1.3662e-01],\n",
      "        [ 8.1770e-05, -7.9559e-02,  8.7201e-02,  ..., -5.8205e-02,\n",
      "          6.7485e-02, -7.3471e-02],\n",
      "        ...,\n",
      "        [-5.2455e-02, -2.4539e-02, -1.4663e-01,  ...,  1.3414e-01,\n",
      "         -1.0900e-01, -6.5919e-02],\n",
      "        [-1.4281e-01, -1.1980e-02, -5.6796e-02,  ...,  2.2399e-02,\n",
      "         -1.7692e-01, -1.0423e-01],\n",
      "        [-8.3271e-02, -3.1705e-02, -1.4520e-01,  ...,  5.6566e-02,\n",
      "         -2.3145e-01, -1.3825e-01]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.2491, -0.1567, -0.0217,  ..., -0.0640, -0.1338, -0.0924],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.2723, -0.0547, -0.0632,  ..., -0.0823, -0.0955, -0.1745],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-2.6528e-02,  2.4395e-03, -9.7770e-02,  ...,  1.7202e-01,\n",
      "         -1.2711e-01, -6.2873e-02],\n",
      "        [-1.2664e-01,  9.2037e-02, -2.2923e-01,  ...,  3.0904e-02,\n",
      "         -2.6102e-01, -7.2050e-02],\n",
      "        [-9.3912e-02, -7.3391e-02, -5.6187e-02,  ..., -6.2088e-03,\n",
      "         -1.1685e-01, -7.5722e-02],\n",
      "        ...,\n",
      "        [-8.6636e-02, -4.2857e-03, -1.4907e-01,  ...,  7.2883e-02,\n",
      "         -1.6568e-01, -1.3588e-01],\n",
      "        [-1.1577e-01, -4.5835e-02, -6.2416e-02,  ...,  1.3432e-02,\n",
      "         -1.9224e-02, -9.7254e-05],\n",
      "        [-1.4428e-01, -2.1675e-04, -1.0831e-01,  ..., -2.8239e-02,\n",
      "         -1.0344e-01, -5.1191e-02]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0655,  0.0155,  0.0216,  ..., -0.0250, -0.0866,  0.0713],\n",
      "        [ 0.0709,  0.0321,  0.0678,  ...,  0.0730, -0.1402,  0.1263],\n",
      "        [ 0.0249,  0.0955, -0.1106,  ...,  0.1723, -0.1344,  0.1628],\n",
      "        ...,\n",
      "        [ 0.0599, -0.0225,  0.1171,  ...,  0.0010, -0.1982,  0.0724],\n",
      "        [-0.0078,  0.0544,  0.0151,  ...,  0.0785, -0.0942,  0.2426],\n",
      "        [-0.0873,  0.1210,  0.0224,  ..., -0.0394,  0.0872,  0.1030]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0400, -0.0304, -0.1024,  ...,  0.0942, -0.1903, -0.1119],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0279,  0.0286,  0.0175,  ...,  0.1196, -0.1589, -0.1240],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0111, -0.0214,  0.0779,  ..., -0.0573,  0.0679,  0.0564],\n",
      "        [-0.0729, -0.0307, -0.0533,  ..., -0.0825, -0.0761, -0.0242],\n",
      "        [-0.0161,  0.0334,  0.0427,  ..., -0.0262,  0.0610,  0.0060],\n",
      "        ...,\n",
      "        [ 0.0110,  0.0139, -0.0113,  ..., -0.0130,  0.0803,  0.0235],\n",
      "        [ 0.0326, -0.0088, -0.0944,  ..., -0.0786, -0.0239,  0.0383],\n",
      "        [ 0.0484,  0.0032,  0.0905,  ..., -0.0790, -0.0046, -0.0517]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-3.4574e-02, -7.5441e-03, -8.0070e-02, -3.4012e-02, -4.2709e-02,\n",
      "         2.2317e-02, -6.5025e-02,  1.7660e-02,  1.1270e-02,  2.8715e-02,\n",
      "        -1.0065e-02, -4.9425e-02, -3.5450e-02,  7.8810e-02,  3.2130e-02,\n",
      "         9.3920e-03, -7.8406e-03, -6.0971e-03,  6.5147e-02,  6.2678e-02,\n",
      "         2.3195e-02, -7.7684e-03,  1.1669e-02, -9.6767e-02,  6.0400e-02,\n",
      "         4.0700e-02, -4.6435e-03, -5.1011e-03, -1.6692e-02,  1.4919e-02,\n",
      "        -4.7672e-02, -3.0273e-02,  1.2158e-02,  3.7110e-02, -6.1718e-02,\n",
      "        -2.5557e-03, -6.9231e-02,  8.9366e-03, -2.7915e-03, -3.1314e-02,\n",
      "        -1.7681e-02, -5.8437e-03, -4.0288e-02, -1.0352e-02,  3.6370e-02,\n",
      "        -1.1270e-03,  4.0106e-02, -7.5644e-02, -4.5901e-02, -1.2258e-02,\n",
      "        -2.3655e-02, -4.1966e-02,  9.1796e-02,  1.1747e-02, -8.2012e-03,\n",
      "         5.9375e-02,  6.8648e-02, -3.4903e-02, -6.2531e-02, -5.2697e-02,\n",
      "        -9.5226e-02, -1.4197e-02,  8.9350e-03, -1.0913e-01, -3.9740e-02,\n",
      "         1.1115e-05,  6.2981e-03,  1.8775e-02, -7.4774e-02, -6.1658e-02,\n",
      "        -2.7940e-02, -1.3120e-02, -2.4317e-02,  5.8487e-02,  2.7788e-02,\n",
      "         1.9668e-02, -1.4222e-02, -2.4206e-02,  4.7811e-02,  4.7106e-02,\n",
      "        -3.0664e-02,  6.2794e-02,  1.7189e-03,  3.7091e-02, -1.2742e-02,\n",
      "        -2.4593e-02,  7.5227e-02,  5.5454e-02, -2.7774e-02,  4.4833e-02,\n",
      "        -4.0477e-02, -1.1130e-02,  2.7711e-02,  2.1187e-02, -1.7917e-02,\n",
      "         9.5554e-03, -1.0621e-02, -9.0037e-02, -2.3957e-02, -5.0970e-02,\n",
      "         1.2353e-03, -4.0699e-02, -2.0105e-02, -1.9298e-02,  8.3670e-02,\n",
      "        -1.0855e-02,  1.5858e-02,  5.7019e-02, -7.3413e-02, -5.8234e-02,\n",
      "         1.1916e-03, -2.5985e-02,  4.6392e-02, -4.8957e-02,  6.7888e-02,\n",
      "         3.9164e-02, -2.7265e-02,  6.4450e-02,  7.0896e-02,  3.8694e-03,\n",
      "         7.8654e-02,  4.9626e-02,  6.0647e-02, -1.1176e-02,  3.1102e-02,\n",
      "        -1.3979e-02,  3.5472e-02,  2.5602e-02, -6.4245e-02,  1.3342e-02,\n",
      "         7.7677e-02, -5.5935e-02, -4.4800e-03,  2.6587e-02,  3.3527e-02,\n",
      "        -5.4833e-02,  4.5567e-02, -9.1367e-03,  2.7839e-02, -6.6854e-03,\n",
      "         5.5554e-02,  1.6483e-04, -1.3802e-02,  7.1617e-02, -3.8032e-02,\n",
      "        -6.7846e-02, -1.7028e-02, -1.0809e-02,  6.0870e-02, -6.1028e-02,\n",
      "         1.1712e-03, -4.4035e-02,  2.9310e-02, -2.0857e-03, -7.8184e-03,\n",
      "        -1.3887e-02, -3.4133e-02, -8.5426e-02,  8.5916e-02, -5.2908e-02,\n",
      "        -2.4893e-03, -1.4949e-02, -3.3741e-02, -9.6029e-02,  4.9869e-02,\n",
      "        -1.6416e-02, -8.5573e-04, -3.0947e-03, -4.3437e-02,  4.5480e-02,\n",
      "        -6.8883e-02,  1.2238e-02, -9.1837e-03,  1.3408e-02, -1.1305e-01,\n",
      "         1.2671e-01, -3.3206e-02, -7.6564e-03,  1.0138e-01,  6.4172e-04,\n",
      "        -1.3753e-02,  1.0786e-02,  9.5883e-03,  1.9478e-02, -1.0028e-02,\n",
      "         4.7250e-02, -1.2049e-02, -6.8203e-02,  1.4220e-02,  1.6864e-02,\n",
      "         3.4031e-02, -1.3561e-02,  2.5853e-02,  1.9034e-02, -4.2687e-02,\n",
      "         6.3737e-02,  7.1170e-02, -5.1677e-02,  2.4240e-02, -7.5582e-03,\n",
      "         2.6787e-02, -3.6843e-02,  1.7728e-02, -1.6466e-03, -5.6384e-02,\n",
      "         4.7742e-02, -1.0346e-02, -8.4314e-02, -2.1313e-02,  5.6552e-02,\n",
      "         7.8522e-02, -7.6503e-02, -8.7966e-02, -2.7776e-02, -3.1033e-02,\n",
      "         2.8013e-02, -8.0613e-03, -9.6433e-03, -8.8593e-02, -4.7548e-02,\n",
      "         3.0647e-02,  3.7387e-02, -6.7799e-03, -1.2656e-02,  4.8194e-02,\n",
      "         3.1528e-02, -4.8574e-02, -1.1072e-02,  3.1286e-02,  1.5454e-02,\n",
      "         9.6303e-02,  3.6493e-03,  1.0481e-01,  8.4897e-02, -2.2491e-02,\n",
      "        -2.3291e-02,  1.7215e-02,  1.7824e-02,  8.9043e-02,  3.2187e-02,\n",
      "         2.9800e-02, -1.8472e-04, -9.2980e-02,  4.2576e-02,  1.7258e-02,\n",
      "         2.8732e-02,  6.6261e-02, -5.7624e-02, -7.1779e-02,  2.6179e-02,\n",
      "         3.9608e-02,  4.5632e-03, -3.5911e-02, -9.6481e-02, -2.3191e-02,\n",
      "         1.8649e-02], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0110, -0.0324, -0.1010,  ..., -0.0408, -0.1102, -0.2243],\n",
      "        [ 0.0931, -0.1354, -0.1578,  ..., -0.1748, -0.2375, -0.1166],\n",
      "        [-0.2166,  0.1185,  0.1365,  ...,  0.0053, -0.0274, -0.0167],\n",
      "        ...,\n",
      "        [ 0.0756, -0.0727, -0.0147,  ...,  0.0191,  0.1004,  0.0217],\n",
      "        [-0.1277,  0.1344,  0.0736,  ...,  0.0863,  0.1024,  0.0302],\n",
      "        [-0.1616,  0.0910,  0.1198,  ...,  0.1159,  0.1430,  0.0921]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.1002, -0.0460, -0.0099, -0.0271, -0.0167,  0.0048, -0.0988,  0.0130,\n",
      "        -0.1249, -0.0624, -0.0578,  0.0129, -0.1017,  0.0701, -0.0216, -0.0376,\n",
      "        -0.0216, -0.0333, -0.0035, -0.1247, -0.0432, -0.0215, -0.1629, -0.0655,\n",
      "         0.0784,  0.0184, -0.0237, -0.0061, -0.0353,  0.1046, -0.0025,  0.0582,\n",
      "         0.0093, -0.0014,  0.0302, -0.0311, -0.0472,  0.0265, -0.0016,  0.0382],\n",
      "       device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for param in model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -------------------------------------------------------------------------------------------------------------------------------------------- ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"word\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(video_list, columns=columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ชื่อไฟลล์ที่รวม Augment มาด้วย"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "onlyfiles = [f for f in listdir(\"Data for different actions\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onlyfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(onlyfiles, columns=columns)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('script.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.read_csv(\"script.csv\")\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[\"label\"] = y[\"label\"].astype(int)\n",
    "labels = y.label\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
