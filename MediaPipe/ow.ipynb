{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MediaPipe Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holist = mp.solutions.holistic \n",
    "mp_draw = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(img, model):\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img.flags.writeable = False                 \n",
    "    result = model.process(img)                 # Make prediction\n",
    "    img.flags.writeable = True                   \n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR) \n",
    "    return img, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks(img, result):\n",
    "    mp_draw.draw_landmarks(img, result.face_landmarks, mp_holist.FACEMESH_CONTOURS) # Draw face connections\n",
    "    mp_draw.draw_landmarks(img, result.pose_landmarks, mp_holist.POSE_CONNECTIONS) # Draw pose connections\n",
    "    mp_draw.draw_landmarks(img, result.left_hand_landmarks, mp_holist.HAND_CONNECTIONS) # Draw left hand connections\n",
    "    mp_draw.draw_landmarks(img, result.right_hand_landmarks, mp_holist.HAND_CONNECTIONS) # Draw right hand connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_styled_landmarks(img, result):\n",
    "    mp_draw.draw_landmarks(img, result.face_landmarks, mp_holist.FACEMESH_CONTOURS, \n",
    "                             mp_draw.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1), # color the joint \n",
    "                             mp_draw.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1) #color the connection\n",
    "                             ) \n",
    "    # mp_draw.draw_landmarks(img, result.face_landmarks, mp_holist.FACEMESH_CONTOURS, \n",
    "    #                          mp_draw.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1), # color the joint \n",
    "    #                          mp_draw.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1) #color the connection\n",
    "    #                          ) \n",
    "    \n",
    "    mp_draw.draw_landmarks(img, result.pose_landmarks, mp_holist.POSE_CONNECTIONS,\n",
    "                             mp_draw.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4), \n",
    "                             mp_draw.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    mp_draw.draw_landmarks(img, result.left_hand_landmarks, mp_holist.HAND_CONNECTIONS, \n",
    "                             mp_draw.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4), \n",
    "                             mp_draw.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    mp_draw.draw_landmarks(img, result.right_hand_landmarks, mp_holist.HAND_CONNECTIONS, \n",
    "                             mp_draw.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4), \n",
    "                             mp_draw.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                             ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holist.POSE_CONNECTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "# Set mediapipe model \n",
    "with mp_holist.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        print(results)\n",
    "        \n",
    "        draw_styled_landmarks(image, results)\n",
    "\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.pose_landmarks.landmark[0].visibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(results.pose_landmarks.landmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(results):\n",
    "    pose=np.array([[res.x,res.y] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*2)\n",
    "    left_hnd=np.array([[res.x,res.y] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*2)\n",
    "    right_hnd=np.array([[res.x,res.y] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*2)\n",
    "    face=np.array([[res.x,res.y] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*2)\n",
    "    return np.concatenate([pose,left_hnd,right_hnd,face])\n",
    "# concatenating for the model to detect the sign language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_keypoints(results).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "video_dir = \"C:/Users/araya/Desktop/keypoints/video_extract\"\n",
    "video_list = []\n",
    "video_list = os.listdir(video_dir)\n",
    "\n",
    "len(video_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for exported data, numpy arrays\n",
    "Model_Data=os.path.join('Data for different actions')\n",
    "\n",
    "actions = np.array(video_list)\n",
    "\n",
    "no_of_seqs = 1\n",
    "\n",
    "# 30 frames in length\n",
    "seq_length = 210"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just creating the folders and sub folders\n",
    "\n",
    "for action in actions: \n",
    "    try: \n",
    "        os.makedirs(os.path.join(Model_Data, action))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# augment_dir = \"C:/Users/araya/Desktop/augments\"\n",
    "\n",
    "# augment_list = []\n",
    "# augment_list = os.listdir(augment_dir)\n",
    "# augment_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actions = list(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x in augment_list:\n",
    "#     # print(x)\n",
    "#     actions.append(x)\n",
    "# actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actions = np.array(actions)\n",
    "# actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collecting keypoint values for Training nd Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory where your videos are stored\n",
    "directory = \"C:/Users/araya/Desktop/keypoints/video_extract\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in actions:\n",
    "    print(directory + '/' + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set mediapipe model \n",
    "# for action in actions:\n",
    "#     video_path = os.path.join(\"C:/Users/araya/Desktop/keypoints/video_extract\", action)\n",
    "#     cap = cv2.VideoCapture(video_path)\n",
    "#     cap.set(cv2.CAP_PROP_FPS, 60)\n",
    "#     length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "#     print(\"LENGTH:\" + str(length))\n",
    "#     # keypoints = []\n",
    "\n",
    "#     if not cap.isOpened():\n",
    "#         print(f\"Error opening video file: {video_path}\")\n",
    "#         continue\n",
    "\n",
    "#     with mp_holist.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "#         for seq in range(no_of_seqs):\n",
    "#             for frame_num in range(seq_length):\n",
    "\n",
    "#                 ret, frame = cap.read()\n",
    "#                 if not ret:\n",
    "#                     print(f\"End of video {video_path}\")\n",
    "#                     break\n",
    "                \n",
    "#                 img, results = mediapipe_detection(frame, holistic)\n",
    "#                 draw_styled_landmarks(img, results)\n",
    "\n",
    "#                 # print(frame_num)\n",
    "\n",
    "#                 if frame_num == 0: \n",
    "#                     cv2.putText(img, 'DATA COLLECTION STARTED', (120,200), \n",
    "#                                 cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255, 0), 4, cv2.LINE_AA)\n",
    "#                     cv2.putText(img, f'Collecting frames for - {action} Sequence Number - {seq}', (15,12), \n",
    "#                                 cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "#                     cv2.imshow('OpenCV Window', img)\n",
    "#                     cv2.waitKey(2000)  # 2 seconds delay for setup\n",
    "#                 else: \n",
    "#                     cv2.putText(img, f'Collecting frames for - {action} Sequence Number - {seq}', (15,12), \n",
    "#                                 cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "#                     cv2.imshow('OpenCV Window', img)\n",
    "\n",
    "#                 keypoints = extract_keypoints(results)\n",
    "#                 # keypoints.append(results)\n",
    "#                 npy_path = os.path.join(Model_Data, action, f\"frame_{frame_num}.npy\")\n",
    "#                 os.makedirs(os.path.dirname(npy_path), exist_ok=True)\n",
    "#                 np.save(npy_path, keypoints)\n",
    "\n",
    "#                 if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#                     break\n",
    "\n",
    "#             if not ret:\n",
    "#                 break\n",
    "\n",
    "#     cap.release()\n",
    "#     cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set mediapipe model \n",
    "for action in actions:\n",
    "    video_path = os.path.join(\"C:/Users/araya/Desktop/keypoints/video_extract\", action)\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    cap.set(cv2.CAP_PROP_FPS, 60)\n",
    "    length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    print(\"LENGTH:\" + str(length))\n",
    "    keypoints = []\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error opening video file: {video_path}\")\n",
    "        continue\n",
    "\n",
    "    with mp_holist.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "        for seq in range(no_of_seqs):\n",
    "            for frame_num in range(seq_length):\n",
    "\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    print(f\"End of video {video_path}\")\n",
    "                    break\n",
    "                \n",
    "                img, results = mediapipe_detection(frame, holistic)\n",
    "                draw_styled_landmarks(img, results)\n",
    "\n",
    "                # print(frame_num)\n",
    "\n",
    "                if frame_num == 0: \n",
    "                    cv2.putText(img, 'DATA COLLECTION STARTED', (120,200), \n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255, 0), 4, cv2.LINE_AA)\n",
    "                    cv2.putText(img, f'Collecting frames for - {action} Sequence Number - {seq}', (15,12), \n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                    cv2.imshow('OpenCV Window', img)\n",
    "                    cv2.waitKey(2000)  # 2 seconds delay for setup\n",
    "                else: \n",
    "                    cv2.putText(img, f'Collecting frames for - {action} Sequence Number - {seq}', (15,12), \n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                    cv2.imshow('OpenCV Window', img)\n",
    "\n",
    "                x = extract_keypoints(results)\n",
    "                keypoints.append(x)\n",
    "                npy_path = os.path.join(Model_Data, action, f\"{action.split(\".\")[0]}.npy\")\n",
    "                os.makedirs(os.path.dirname(npy_path), exist_ok=True)\n",
    "                np.save(npy_path, keypoints)\n",
    "\n",
    "                if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                    break\n",
    "\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
