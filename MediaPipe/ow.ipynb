{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holist = mp.solutions.holistic \n",
    "mp_draw = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(img, model):\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img.flags.writeable = False                 \n",
    "    result = model.process(img)                 # Make prediction\n",
    "    img.flags.writeable = True                   \n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR) \n",
    "    return img, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks(img, result):\n",
    "    mp_draw.draw_landmarks(img, result.face_landmarks, mp_holist.FACEMESH_CONTOURS) # Draw face connections\n",
    "    mp_draw.draw_landmarks(img, result.pose_landmarks, mp_holist.POSE_CONNECTIONS) # Draw pose connections\n",
    "    mp_draw.draw_landmarks(img, result.left_hand_landmarks, mp_holist.HAND_CONNECTIONS) # Draw left hand connections\n",
    "    mp_draw.draw_landmarks(img, result.right_hand_landmarks, mp_holist.HAND_CONNECTIONS) # Draw right hand connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_styled_landmarks(img, result):\n",
    "    mp_draw.draw_landmarks(img, result.face_landmarks, mp_holist.FACEMESH_CONTOURS, \n",
    "                             mp_draw.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1), # color the joint \n",
    "                             mp_draw.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1) #color the connection\n",
    "                             ) \n",
    "    # mp_draw.draw_landmarks(img, result.face_landmarks, mp_holist.FACEMESH_CONTOURS, \n",
    "    #                          mp_draw.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1), # color the joint \n",
    "    #                          mp_draw.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1) #color the connection\n",
    "    #                          ) \n",
    "    \n",
    "    mp_draw.draw_landmarks(img, result.pose_landmarks, mp_holist.POSE_CONNECTIONS,\n",
    "                             mp_draw.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4), \n",
    "                             mp_draw.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    mp_draw.draw_landmarks(img, result.left_hand_landmarks, mp_holist.HAND_CONNECTIONS, \n",
    "                             mp_draw.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4), \n",
    "                             mp_draw.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    mp_draw.draw_landmarks(img, result.right_hand_landmarks, mp_holist.HAND_CONNECTIONS, \n",
    "                             mp_draw.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4), \n",
    "                             mp_draw.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                             ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holist.POSE_CONNECTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "# Set mediapipe model \n",
    "with mp_holist.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        print(results)\n",
    "        \n",
    "        draw_styled_landmarks(image, results)\n",
    "\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.pose_landmarks.landmark[0].visibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(results.pose_landmarks.landmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(results):\n",
    "    pose=np.array([[res.x,res.y,res.z,res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    left_hnd=np.array([[res.x,res.y,res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    right_hnd=np.array([[res.x,res.y,res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    face=np.array([[res.x,res.y,res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "    return np.concatenate([pose,left_hnd,right_hnd,face])\n",
    "# concatenating for the model to detect the sign language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_keypoints(results).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "video_dir = \"C:/Users/araya/Desktop/keypoints/video_extract\"\n",
    "video_list = []\n",
    "video_list = os.listdir(video_dir)\n",
    "\n",
    "len(video_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for exported data, numpy arrays\n",
    "Model_Data=os.path.join('Data for different actions')\n",
    "\n",
    "actions = np.array(video_list)\n",
    "\n",
    "no_of_seqs = 1\n",
    "\n",
    "# 30 frames in length\n",
    "seq_length = 160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just creating the folders and sub folders\n",
    "\n",
    "for action in actions: \n",
    "    try: \n",
    "        os.makedirs(os.path.join(Model_Data, action))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collecting keypoint values for Training nd Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory where your videos are stored\n",
    "directory = \"C:/Users/araya/Desktop/keypoints/video_extract\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"hello, my name is Peter, I am 26 years old\"\n",
    "\n",
    "x = txt.split(\", \")\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in actions:\n",
    "    print(directory + '/' + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set mediapipe model \n",
    "# for action in actions:\n",
    "#     video_path = os.path.join(\"C:/Users/araya/Desktop/keypoints/video_extract\", action)\n",
    "#     cap = cv2.VideoCapture(video_path)\n",
    "#     cap.set(cv2.CAP_PROP_FPS, 60)\n",
    "#     length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "#     print(\"LENGTH:\" + str(length))\n",
    "#     # keypoints = []\n",
    "\n",
    "#     if not cap.isOpened():\n",
    "#         print(f\"Error opening video file: {video_path}\")\n",
    "#         continue\n",
    "\n",
    "#     with mp_holist.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "#         for seq in range(no_of_seqs):\n",
    "#             for frame_num in range(seq_length):\n",
    "\n",
    "#                 ret, frame = cap.read()\n",
    "#                 if not ret:\n",
    "#                     print(f\"End of video {video_path}\")\n",
    "#                     break\n",
    "                \n",
    "#                 img, results = mediapipe_detection(frame, holistic)\n",
    "#                 draw_styled_landmarks(img, results)\n",
    "\n",
    "#                 # print(frame_num)\n",
    "\n",
    "#                 if frame_num == 0: \n",
    "#                     cv2.putText(img, 'DATA COLLECTION STARTED', (120,200), \n",
    "#                                 cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255, 0), 4, cv2.LINE_AA)\n",
    "#                     cv2.putText(img, f'Collecting frames for - {action} Sequence Number - {seq}', (15,12), \n",
    "#                                 cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "#                     cv2.imshow('OpenCV Window', img)\n",
    "#                     cv2.waitKey(2000)  # 2 seconds delay for setup\n",
    "#                 else: \n",
    "#                     cv2.putText(img, f'Collecting frames for - {action} Sequence Number - {seq}', (15,12), \n",
    "#                                 cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "#                     cv2.imshow('OpenCV Window', img)\n",
    "\n",
    "#                 keypoints = extract_keypoints(results)\n",
    "#                 # keypoints.append(results)\n",
    "#                 npy_path = os.path.join(Model_Data, action, f\"frame_{frame_num}.npy\")\n",
    "#                 os.makedirs(os.path.dirname(npy_path), exist_ok=True)\n",
    "#                 np.save(npy_path, keypoints)\n",
    "\n",
    "#                 if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#                     break\n",
    "\n",
    "#             if not ret:\n",
    "#                 break\n",
    "\n",
    "#     cap.release()\n",
    "#     cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np \n",
    "\n",
    "# X = [[1,2,3]]\n",
    "# X.append([6,8,10])\n",
    "# X.append([20,9,4])\n",
    "# X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set mediapipe model \n",
    "for action in actions:\n",
    "    video_path = os.path.join(\"C:/Users/araya/Desktop/keypoints/video_extract\", action)\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    cap.set(cv2.CAP_PROP_FPS, 60)\n",
    "    length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    print(\"LENGTH:\" + str(length))\n",
    "    keypoints = []\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error opening video file: {video_path}\")\n",
    "        continue\n",
    "\n",
    "    with mp_holist.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "        for seq in range(no_of_seqs):\n",
    "            for frame_num in range(seq_length):\n",
    "\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    print(f\"End of video {video_path}\")\n",
    "                    break\n",
    "                \n",
    "                img, results = mediapipe_detection(frame, holistic)\n",
    "                draw_styled_landmarks(img, results)\n",
    "\n",
    "                # print(frame_num)\n",
    "\n",
    "                if frame_num == 0: \n",
    "                    cv2.putText(img, 'DATA COLLECTION STARTED', (120,200), \n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255, 0), 4, cv2.LINE_AA)\n",
    "                    cv2.putText(img, f'Collecting frames for - {action} Sequence Number - {seq}', (15,12), \n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                    cv2.imshow('OpenCV Window', img)\n",
    "                    cv2.waitKey(2000)  # 2 seconds delay for setup\n",
    "                else: \n",
    "                    cv2.putText(img, f'Collecting frames for - {action} Sequence Number - {seq}', (15,12), \n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                    cv2.imshow('OpenCV Window', img)\n",
    "\n",
    "                x = extract_keypoints(results)\n",
    "                keypoints.append(x)\n",
    "                npy_path = os.path.join(Model_Data, action, f\"{action.split(\".\")[0]}.npy\")\n",
    "                os.makedirs(os.path.dirname(npy_path), exist_ok=True)\n",
    "                np.save(npy_path, keypoints)\n",
    "\n",
    "                if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                    break\n",
    "\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set mediapipe model \n",
    "# for action in actions:\n",
    "#     video_path = os.path.join(video_dir, action)\n",
    "#     cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "#     if not cap.isOpened():\n",
    "#         print(f\"Error opening video file: {video_path}\")\n",
    "#         continue\n",
    "\n",
    "#     with mp_holist.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "#         frames = []\n",
    "\n",
    "#         while True:\n",
    "#             ret, frame = cap.read()\n",
    "#             if not ret:\n",
    "#                 break\n",
    "            \n",
    "#             img, results = mediapipe_detection(frame, holistic)\n",
    "#             keypoints = extract_keypoints(results)\n",
    "#             frames.append(keypoints)\n",
    "\n",
    "#         # Padding or trimming to fixed sequence length\n",
    "#         if len(frames) < seq_length:\n",
    "#             padding = [np.zeros_like(frames[0]) for _ in range(seq_length - len(frames))]\n",
    "#             frames.extend(padding)\n",
    "#         else:\n",
    "#             frames = frames[:seq_length]\n",
    "\n",
    "#         # Save sequences\n",
    "#         for i in range(len(frames)):\n",
    "#             npy_path = os.path.join(Model_Data, action, f\"frame_{i}.npy\")\n",
    "#             np.save(npy_path, frames[i])\n",
    "\n",
    "#     cap.release()\n",
    "\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Loop through all files in the directory\n",
    "# for filename in os.listdir(directory):\n",
    "#     # Check if the file is a video by checking its extension\n",
    "#     if filename.endswith(('.mp4', '.avi', '.mkv', '.mov')):\n",
    "#         print(f\"Processing {filename}...\")\n",
    "#         video_path = os.path.join(directory, filename)\n",
    "\n",
    "#         class_name = os.path.splitext(os.path.basename(video_path))[0]\n",
    "        \n",
    "#         cap = cv2.VideoCapture(video_path)\n",
    "#         with mp_holist.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "#             for action in actions:\n",
    "#                 for seq in range(no_of_seqs):\n",
    "#                     for frame_num in range(seq_length):\n",
    "#                         ret, frame = cap.read()\n",
    "#                         if not ret:\n",
    "#                             print(\"Error: Failed to read frame.\")\n",
    "#                             break  # Exit the loop if frame read fails\n",
    "                        \n",
    "#                         img, results = mediapipe_detection(frame, holistic)\n",
    "#                         draw_styled_landmarks(img, results)\n",
    "\n",
    "#                         # logic is for the formatting portion\n",
    "#                         if frame_num == 0: \n",
    "#                             cv2.putText(img, 'DATA COLLECTION STARTED', (120,200), \n",
    "#                                     cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255, 0), 4, cv2.LINE_AA)\n",
    "#                             cv2.putText(img, 'Collecting frames for - {} and Sequence Number - {}'.format(action, seq), (15,12), \n",
    "#                                     cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "#                             # Show to screen\n",
    "#                             cv2.imshow('OpenCV Window', img)\n",
    "#                             # providing the break for adjusting the posture\n",
    "#                             cv2.waitKey(2000) # 2 sec\n",
    "#                         else: \n",
    "#                             cv2.putText(img, 'Collecting frames for - {} and Sequence Number - {}'.format(action, seq), (15,12), \n",
    "#                                     cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "#                             # Show to screen\n",
    "#                             cv2.imshow('OpenCV Window', img)\n",
    "\n",
    "#                         keypoints = extract_keypoints(results)\n",
    "#                         npy_path = os.path.join(Model_Data, action, str(seq), str(frame_num))\n",
    "#                         np.save(npy_path, keypoints)\n",
    "\n",
    "#                         if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "#                             break\n",
    "\n",
    "#         cap.release()\n",
    "#         cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Data for different actions/กฎกระทรวง.mp4/กฎกระทรวง.npy', 'Data for different actions/กฎหมายรัฐธรรมนูญ.mp4/กฎหมายรัฐธรรมนูญ.npy', 'Data for different actions/กรมอนามัย.mp4/กรมอนามัย.npy', 'Data for different actions/กรรม.mp4/กรรม.npy', 'Data for different actions/กรรมสิทธิ์.mp4/กรรมสิทธิ์.npy', 'Data for different actions/กระโดด.mp4/กระโดด.npy', 'Data for different actions/กล้วยบวชชี.mp4/กล้วยบวชชี.npy', 'Data for different actions/กล้วยเชื่อม.mp4/กล้วยเชื่อม.npy']\n"
     ]
    }
   ],
   "source": [
    "file_paths = []\n",
    "for action in actions:\n",
    "    video_path = os.path.join('Data for different actions/', action)\n",
    "    # print(video_path)\n",
    "    # print(action)\n",
    "    file_paths.append(video_path + '/' + action.split(\".\")[0] + \".npy\")\n",
    "print(file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_keypoint_sequences(file_paths):\n",
    "    keypoint_sequences = []\n",
    "    for file_path in file_paths:\n",
    "        keypoints = np.load(file_path)\n",
    "        keypoint_sequences.append(torch.tensor(keypoints, dtype=torch.float32))\n",
    "    return keypoint_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 0.5013,  0.2452, -1.2167,  ...,  0.5663,  0.2188,  0.0098],\n",
       "         [ 0.4997,  0.2482, -1.4690,  ...,  0.5652,  0.2181,  0.0106],\n",
       "         [ 0.4984,  0.2500, -1.4853,  ...,  0.5654,  0.2185,  0.0112],\n",
       "         ...,\n",
       "         [ 0.4861,  0.2513, -1.3416,  ...,  0.5572,  0.2177,  0.0091],\n",
       "         [ 0.4873,  0.2514, -1.3574,  ...,  0.5575,  0.2172,  0.0097],\n",
       "         [ 0.4883,  0.2516, -1.3579,  ...,  0.5577,  0.2170,  0.0101]]),\n",
       " tensor([[ 0.4922,  0.2382, -1.2850,  ...,  0.5578,  0.2124,  0.0094],\n",
       "         [ 0.4920,  0.2405, -1.4288,  ...,  0.5571,  0.2116,  0.0099],\n",
       "         [ 0.4920,  0.2409, -1.4093,  ...,  0.5567,  0.2122,  0.0098],\n",
       "         ...,\n",
       "         [ 0.4814,  0.2260, -1.3318,  ...,  0.5503,  0.1923,  0.0123],\n",
       "         [ 0.4815,  0.2257, -1.3351,  ...,  0.5503,  0.1921,  0.0122],\n",
       "         [ 0.4815,  0.2255, -1.3497,  ...,  0.5501,  0.1919,  0.0124]]),\n",
       " tensor([[ 0.5049,  0.2371, -1.2115,  ...,  0.5643,  0.2082,  0.0088],\n",
       "         [ 0.5045,  0.2381, -1.1896,  ...,  0.5643,  0.2081,  0.0085],\n",
       "         [ 0.5041,  0.2385, -1.1915,  ...,  0.5643,  0.2080,  0.0089],\n",
       "         ...,\n",
       "         [ 0.4963,  0.2307, -1.3096,  ...,  0.5624,  0.2043,  0.0127],\n",
       "         [ 0.4965,  0.2308, -1.3085,  ...,  0.5625,  0.2045,  0.0126],\n",
       "         [ 0.4972,  0.2312, -1.3053,  ...,  0.5633,  0.2045,  0.0128]]),\n",
       " tensor([[ 0.5134,  0.2614, -1.4426,  ...,  0.5818,  0.2272,  0.0153],\n",
       "         [ 0.5130,  0.2604, -1.4262,  ...,  0.5810,  0.2273,  0.0147],\n",
       "         [ 0.5126,  0.2599, -1.4278,  ...,  0.5810,  0.2276,  0.0150],\n",
       "         ...,\n",
       "         [ 0.5079,  0.2693, -1.4999,  ...,  0.5780,  0.2349,  0.0115],\n",
       "         [ 0.5090,  0.2688, -1.4936,  ...,  0.5782,  0.2346,  0.0116],\n",
       "         [ 0.5092,  0.2683, -1.4518,  ...,  0.5786,  0.2341,  0.0116]]),\n",
       " tensor([[ 0.4883,  0.2402, -1.1024,  ...,  0.5482,  0.2132,  0.0081],\n",
       "         [ 0.4878,  0.2402, -1.1906,  ...,  0.5469,  0.2135,  0.0083],\n",
       "         [ 0.4863,  0.2402, -1.1774,  ...,  0.5477,  0.2141,  0.0087],\n",
       "         ...,\n",
       "         [ 0.4788,  0.3129, -1.6072,  ...,  0.5349,  0.2531,  0.0022],\n",
       "         [ 0.4782,  0.3129, -1.6350,  ...,  0.5344,  0.2525,  0.0022],\n",
       "         [ 0.4771,  0.3131, -1.6312,  ...,  0.5339,  0.2515,  0.0022]]),\n",
       " tensor([[ 0.4992,  0.1994, -1.1906,  ...,  0.5657,  0.1731,  0.0116],\n",
       "         [ 0.4988,  0.2047, -1.3590,  ...,  0.5663,  0.1723,  0.0123],\n",
       "         [ 0.4982,  0.2082, -1.3140,  ...,  0.5671,  0.1730,  0.0129],\n",
       "         ...,\n",
       "         [ 0.4743,  0.1974, -1.3230,  ...,  0.5475,  0.1620,  0.0155],\n",
       "         [ 0.4732,  0.1974, -1.3174,  ...,  0.5461,  0.1615,  0.0151],\n",
       "         [ 0.4720,  0.1974, -1.3149,  ...,  0.5453,  0.1614,  0.0149]]),\n",
       " tensor([[ 0.5023,  0.2809, -1.6242,  ...,  0.5847,  0.2321,  0.0112],\n",
       "         [ 0.5023,  0.2806, -1.6631,  ...,  0.5840,  0.2322,  0.0130],\n",
       "         [ 0.5022,  0.2805, -1.6912,  ...,  0.5837,  0.2322,  0.0122],\n",
       "         ...,\n",
       "         [ 0.5037,  0.2791, -1.5789,  ...,  0.5834,  0.2224,  0.0129],\n",
       "         [ 0.5044,  0.2774, -1.5686,  ...,  0.5831,  0.2220,  0.0130],\n",
       "         [ 0.5052,  0.2722, -1.5527,  ...,  0.5827,  0.2216,  0.0132]]),\n",
       " tensor([[ 0.4868,  0.2821, -1.4668,  ...,  0.5711,  0.2335,  0.0116],\n",
       "         [ 0.4861,  0.2786, -1.5812,  ...,  0.5702,  0.2334,  0.0129],\n",
       "         [ 0.4856,  0.2769, -1.6059,  ...,  0.5699,  0.2333,  0.0131],\n",
       "         ...,\n",
       "         [ 0.4826,  0.2583, -1.6140,  ...,  0.5606,  0.2261,  0.0154],\n",
       "         [ 0.4820,  0.2583, -1.5231,  ...,  0.5603,  0.2261,  0.0156],\n",
       "         [ 0.4818,  0.2580, -1.5255,  ...,  0.5600,  0.2263,  0.0156]])]"
      ]
     },
     "execution_count": 488,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the sequences\n",
    "sequences = load_keypoint_sequences(file_paths)\n",
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 160, 1662])\n"
     ]
    }
   ],
   "source": [
    "# Pad the sequences to the same length\n",
    "padded_sequences = pad_sequence(sequences, batch_first=True)\n",
    "pad_sequence\n",
    "print(padded_sequences.shape) # (batch_size, max_sequence_length, num_keypoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['กฎกระทรวง',\n",
       " 'กฎหมายรัฐธรรมนูญ',\n",
       " 'กรมอนามัย',\n",
       " 'กรรม',\n",
       " 'กรรมสิทธิ์',\n",
       " 'กระโดด',\n",
       " 'กล้วยบวชชี',\n",
       " 'กล้วยเชื่อม']"
      ]
     },
     "execution_count": 490,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = [action.split(\".\")[0] for action in actions]\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7], dtype=int64)"
      ]
     },
     "execution_count": 491,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "labels = le.fit_transform(labels)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Create a custom dataset\n",
    "class KeypointDataset(Dataset):\n",
    "    def __init__(self, file_paths, labels):\n",
    "        self.file_paths = file_paths\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        keypoints = np.load(self.file_paths[idx])\n",
    "        label = self.labels[idx]\n",
    "        return torch.tensor(keypoints, dtype=torch.float32), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset\n",
    "dataset = KeypointDataset(file_paths, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Data for different actions/กฎกระทรวง.mp4/กฎกระทรวง.npy', 'Data for different actions/กฎหมายรัฐธรรมนูญ.mp4/กฎหมายรัฐธรรมนูญ.npy', 'Data for different actions/กรมอนามัย.mp4/กรมอนามัย.npy', 'Data for different actions/กรรม.mp4/กรรม.npy', 'Data for different actions/กรรมสิทธิ์.mp4/กรรมสิทธิ์.npy', 'Data for different actions/กระโดด.mp4/กระโดด.npy', 'Data for different actions/กล้วยบวชชี.mp4/กล้วยบวชชี.npy', 'Data for different actions/กล้วยเชื่อม.mp4/กล้วยเชื่อม.npy']\n",
      "[0 1 2 3 4 5 6 7]\n"
     ]
    }
   ],
   "source": [
    "print(dataset.file_paths)\n",
    "print(dataset.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collate function for padding\n",
    "def collate_fn(batch):\n",
    "    sequences, labels = zip(*batch)\n",
    "    padded_sequences = pad_sequence(sequences, batch_first=True)\n",
    "    return padded_sequences, torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x1a81f1dbcb0>"
      ]
     },
     "execution_count": 495,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the DataLoader\n",
    "batch_size = 4\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        # Forward propagate the LSTM\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        # Use the last time step's output for classification\n",
    "        out = out[:, -1, :]\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 497,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set device (use GPU if available)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model, loss function, and optimizer\n",
    "model = LSTMModel(input_size=1662, hidden_size=128, num_layers=2, num_classes=8).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()  # For multi-class classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/300], Loss: 2.1161\n",
      "Epoch [2/300], Loss: 1.9582\n",
      "Epoch [3/300], Loss: 2.1770\n",
      "Epoch [4/300], Loss: 1.6733\n",
      "Epoch [5/300], Loss: 1.6795\n",
      "Epoch [6/300], Loss: 2.0610\n",
      "Epoch [7/300], Loss: 1.7889\n",
      "Epoch [8/300], Loss: 1.5011\n",
      "Epoch [9/300], Loss: 2.2988\n",
      "Epoch [10/300], Loss: 2.2428\n",
      "Epoch [11/300], Loss: 1.6378\n",
      "Epoch [12/300], Loss: 1.3712\n",
      "Epoch [13/300], Loss: 1.6537\n",
      "Epoch [14/300], Loss: 1.4455\n",
      "Epoch [15/300], Loss: 1.6070\n",
      "Epoch [16/300], Loss: 1.8169\n",
      "Epoch [17/300], Loss: 1.3233\n",
      "Epoch [18/300], Loss: 1.5108\n",
      "Epoch [19/300], Loss: 0.7051\n",
      "Epoch [20/300], Loss: 1.4103\n",
      "Epoch [21/300], Loss: 1.3084\n",
      "Epoch [22/300], Loss: 0.9015\n",
      "Epoch [23/300], Loss: 1.3995\n",
      "Epoch [24/300], Loss: 1.0576\n",
      "Epoch [25/300], Loss: 1.0629\n",
      "Epoch [26/300], Loss: 1.1937\n",
      "Epoch [27/300], Loss: 0.9754\n",
      "Epoch [28/300], Loss: 1.0815\n",
      "Epoch [29/300], Loss: 0.8562\n",
      "Epoch [30/300], Loss: 0.4761\n",
      "Epoch [31/300], Loss: 0.6833\n",
      "Epoch [32/300], Loss: 0.7307\n",
      "Epoch [33/300], Loss: 0.7525\n",
      "Epoch [34/300], Loss: 0.3035\n",
      "Epoch [35/300], Loss: 3.2796\n",
      "Epoch [36/300], Loss: 0.6567\n",
      "Epoch [37/300], Loss: 0.5066\n",
      "Epoch [38/300], Loss: 3.3266\n",
      "Epoch [39/300], Loss: 0.7090\n",
      "Epoch [40/300], Loss: 0.6454\n",
      "Epoch [41/300], Loss: 0.3945\n",
      "Epoch [42/300], Loss: 2.4173\n",
      "Epoch [43/300], Loss: 1.5410\n",
      "Epoch [44/300], Loss: 0.9014\n",
      "Epoch [45/300], Loss: 0.3878\n",
      "Epoch [46/300], Loss: 1.5740\n",
      "Epoch [47/300], Loss: 1.0098\n",
      "Epoch [48/300], Loss: 0.4289\n",
      "Epoch [49/300], Loss: 1.0105\n",
      "Epoch [50/300], Loss: 1.8900\n",
      "Epoch [51/300], Loss: 1.4915\n",
      "Epoch [52/300], Loss: 1.5356\n",
      "Epoch [53/300], Loss: 1.8058\n",
      "Epoch [54/300], Loss: 0.6872\n",
      "Epoch [55/300], Loss: 0.8215\n",
      "Epoch [56/300], Loss: 0.6478\n",
      "Epoch [57/300], Loss: 1.5133\n",
      "Epoch [58/300], Loss: 0.6835\n",
      "Epoch [59/300], Loss: 0.8706\n",
      "Epoch [60/300], Loss: 0.6764\n",
      "Epoch [61/300], Loss: 0.7041\n",
      "Epoch [62/300], Loss: 0.5278\n",
      "Epoch [63/300], Loss: 0.6440\n",
      "Epoch [64/300], Loss: 0.4859\n",
      "Epoch [65/300], Loss: 0.5486\n",
      "Epoch [66/300], Loss: 0.4998\n",
      "Epoch [67/300], Loss: 0.5012\n",
      "Epoch [68/300], Loss: 0.2442\n",
      "Epoch [69/300], Loss: 0.4510\n",
      "Epoch [70/300], Loss: 0.2564\n",
      "Epoch [71/300], Loss: 0.4506\n",
      "Epoch [72/300], Loss: 0.3806\n",
      "Epoch [73/300], Loss: 0.2056\n",
      "Epoch [74/300], Loss: 0.5714\n",
      "Epoch [75/300], Loss: 0.3043\n",
      "Epoch [76/300], Loss: 0.2797\n",
      "Epoch [77/300], Loss: 0.4330\n",
      "Epoch [78/300], Loss: 0.5204\n",
      "Epoch [79/300], Loss: 0.2758\n",
      "Epoch [80/300], Loss: 1.6617\n",
      "Epoch [81/300], Loss: 1.6033\n",
      "Epoch [82/300], Loss: 1.8878\n",
      "Epoch [83/300], Loss: 0.3344\n",
      "Epoch [84/300], Loss: 0.2521\n",
      "Epoch [85/300], Loss: 0.3032\n",
      "Epoch [86/300], Loss: 0.3728\n",
      "Epoch [87/300], Loss: 0.1859\n",
      "Epoch [88/300], Loss: 1.3044\n",
      "Epoch [89/300], Loss: 0.3672\n",
      "Epoch [90/300], Loss: 0.3664\n",
      "Epoch [91/300], Loss: 1.9576\n",
      "Epoch [92/300], Loss: 0.2862\n",
      "Epoch [93/300], Loss: 0.8294\n",
      "Epoch [94/300], Loss: 0.7624\n",
      "Epoch [95/300], Loss: 1.8355\n",
      "Epoch [96/300], Loss: 0.3601\n",
      "Epoch [97/300], Loss: 0.3549\n",
      "Epoch [98/300], Loss: 0.2561\n",
      "Epoch [99/300], Loss: 0.3613\n",
      "Epoch [100/300], Loss: 1.1748\n",
      "Epoch [101/300], Loss: 0.9334\n",
      "Epoch [102/300], Loss: 0.3456\n",
      "Epoch [103/300], Loss: 1.6627\n",
      "Epoch [104/300], Loss: 0.2540\n",
      "Epoch [105/300], Loss: 0.4145\n",
      "Epoch [106/300], Loss: 0.2486\n",
      "Epoch [107/300], Loss: 0.3621\n",
      "Epoch [108/300], Loss: 0.2102\n",
      "Epoch [109/300], Loss: 0.3637\n",
      "Epoch [110/300], Loss: 0.8325\n",
      "Epoch [111/300], Loss: 0.4164\n",
      "Epoch [112/300], Loss: 0.4124\n",
      "Epoch [113/300], Loss: 0.4404\n",
      "Epoch [114/300], Loss: 0.2604\n",
      "Epoch [115/300], Loss: 0.3259\n",
      "Epoch [116/300], Loss: 0.2012\n",
      "Epoch [117/300], Loss: 0.2796\n",
      "Epoch [118/300], Loss: 0.3238\n",
      "Epoch [119/300], Loss: 1.0023\n",
      "Epoch [120/300], Loss: 0.2368\n",
      "Epoch [121/300], Loss: 0.2698\n",
      "Epoch [122/300], Loss: 1.1653\n",
      "Epoch [123/300], Loss: 0.4151\n",
      "Epoch [124/300], Loss: 0.1125\n",
      "Epoch [125/300], Loss: 0.1720\n",
      "Epoch [126/300], Loss: 0.3600\n",
      "Epoch [127/300], Loss: 0.1497\n",
      "Epoch [128/300], Loss: 0.1979\n",
      "Epoch [129/300], Loss: 1.4714\n",
      "Epoch [130/300], Loss: 0.2726\n",
      "Epoch [131/300], Loss: 0.6553\n",
      "Epoch [132/300], Loss: 0.2502\n",
      "Epoch [133/300], Loss: 0.1040\n",
      "Epoch [134/300], Loss: 0.1018\n",
      "Epoch [135/300], Loss: 0.3255\n",
      "Epoch [136/300], Loss: 0.2952\n",
      "Epoch [137/300], Loss: 0.4176\n",
      "Epoch [138/300], Loss: 0.2630\n",
      "Epoch [139/300], Loss: 0.2521\n",
      "Epoch [140/300], Loss: 0.1929\n",
      "Epoch [141/300], Loss: 0.2489\n",
      "Epoch [142/300], Loss: 0.2813\n",
      "Epoch [143/300], Loss: 0.4068\n",
      "Epoch [144/300], Loss: 0.2766\n",
      "Epoch [145/300], Loss: 0.2494\n",
      "Epoch [146/300], Loss: 0.2441\n",
      "Epoch [147/300], Loss: 0.4974\n",
      "Epoch [148/300], Loss: 0.2182\n",
      "Epoch [149/300], Loss: 1.1523\n",
      "Epoch [150/300], Loss: 1.1144\n",
      "Epoch [151/300], Loss: 1.9005\n",
      "Epoch [152/300], Loss: 0.2336\n",
      "Epoch [153/300], Loss: 0.3832\n",
      "Epoch [154/300], Loss: 0.7645\n",
      "Epoch [155/300], Loss: 0.5821\n",
      "Epoch [156/300], Loss: 0.3646\n",
      "Epoch [157/300], Loss: 0.4589\n",
      "Epoch [158/300], Loss: 0.3708\n",
      "Epoch [159/300], Loss: 0.2682\n",
      "Epoch [160/300], Loss: 0.6672\n",
      "Epoch [161/300], Loss: 1.6715\n",
      "Epoch [162/300], Loss: 0.2316\n",
      "Epoch [163/300], Loss: 0.6879\n",
      "Epoch [164/300], Loss: 0.3844\n",
      "Epoch [165/300], Loss: 0.4165\n",
      "Epoch [166/300], Loss: 1.3406\n",
      "Epoch [167/300], Loss: 0.2409\n",
      "Epoch [168/300], Loss: 0.4638\n",
      "Epoch [169/300], Loss: 0.3543\n",
      "Epoch [170/300], Loss: 1.0282\n",
      "Epoch [171/300], Loss: 0.4803\n",
      "Epoch [172/300], Loss: 0.9181\n",
      "Epoch [173/300], Loss: 0.4372\n",
      "Epoch [174/300], Loss: 2.2551\n",
      "Epoch [175/300], Loss: 0.9700\n",
      "Epoch [176/300], Loss: 0.4193\n",
      "Epoch [177/300], Loss: 0.4242\n",
      "Epoch [178/300], Loss: 0.4981\n",
      "Epoch [179/300], Loss: 0.8360\n",
      "Epoch [180/300], Loss: 0.1906\n",
      "Epoch [181/300], Loss: 0.2981\n",
      "Epoch [182/300], Loss: 0.3970\n",
      "Epoch [183/300], Loss: 0.3496\n",
      "Epoch [184/300], Loss: 0.3212\n",
      "Epoch [185/300], Loss: 0.2328\n",
      "Epoch [186/300], Loss: 0.3446\n",
      "Epoch [187/300], Loss: 0.7481\n",
      "Epoch [188/300], Loss: 0.1160\n",
      "Epoch [189/300], Loss: 0.2042\n",
      "Epoch [190/300], Loss: 0.4053\n",
      "Epoch [191/300], Loss: 0.2663\n",
      "Epoch [192/300], Loss: 0.2305\n",
      "Epoch [193/300], Loss: 0.1047\n",
      "Epoch [194/300], Loss: 0.9494\n",
      "Epoch [195/300], Loss: 0.6525\n",
      "Epoch [196/300], Loss: 0.8231\n",
      "Epoch [197/300], Loss: 1.1331\n",
      "Epoch [198/300], Loss: 0.4105\n",
      "Epoch [199/300], Loss: 0.5597\n",
      "Epoch [200/300], Loss: 0.2825\n",
      "Epoch [201/300], Loss: 1.4254\n",
      "Epoch [202/300], Loss: 0.7118\n",
      "Epoch [203/300], Loss: 0.2995\n",
      "Epoch [204/300], Loss: 1.2682\n",
      "Epoch [205/300], Loss: 0.6849\n",
      "Epoch [206/300], Loss: 1.0724\n",
      "Epoch [207/300], Loss: 0.3666\n",
      "Epoch [208/300], Loss: 1.0097\n",
      "Epoch [209/300], Loss: 0.4592\n",
      "Epoch [210/300], Loss: 0.2256\n",
      "Epoch [211/300], Loss: 0.7992\n",
      "Epoch [212/300], Loss: 0.3491\n",
      "Epoch [213/300], Loss: 0.3225\n",
      "Epoch [214/300], Loss: 0.2660\n",
      "Epoch [215/300], Loss: 0.3440\n",
      "Epoch [216/300], Loss: 0.3897\n",
      "Epoch [217/300], Loss: 0.2409\n",
      "Epoch [218/300], Loss: 0.3101\n",
      "Epoch [219/300], Loss: 0.4010\n",
      "Epoch [220/300], Loss: 0.1999\n",
      "Epoch [221/300], Loss: 0.2830\n",
      "Epoch [222/300], Loss: 0.2162\n",
      "Epoch [223/300], Loss: 0.3116\n",
      "Epoch [224/300], Loss: 0.3210\n",
      "Epoch [225/300], Loss: 0.2601\n",
      "Epoch [226/300], Loss: 0.2227\n",
      "Epoch [227/300], Loss: 0.3497\n",
      "Epoch [228/300], Loss: 0.2507\n",
      "Epoch [229/300], Loss: 0.1913\n",
      "Epoch [230/300], Loss: 0.1173\n",
      "Epoch [231/300], Loss: 0.1282\n",
      "Epoch [232/300], Loss: 0.6180\n",
      "Epoch [233/300], Loss: 0.0941\n",
      "Epoch [234/300], Loss: 0.1052\n",
      "Epoch [235/300], Loss: 0.7182\n",
      "Epoch [236/300], Loss: 0.1809\n",
      "Epoch [237/300], Loss: 0.3213\n",
      "Epoch [238/300], Loss: 0.1968\n",
      "Epoch [239/300], Loss: 0.1908\n",
      "Epoch [240/300], Loss: 0.1678\n",
      "Epoch [241/300], Loss: 0.1648\n",
      "Epoch [242/300], Loss: 0.2011\n",
      "Epoch [243/300], Loss: 0.2283\n",
      "Epoch [244/300], Loss: 0.4062\n",
      "Epoch [245/300], Loss: 0.2338\n",
      "Epoch [246/300], Loss: 0.0997\n",
      "Epoch [247/300], Loss: 0.3113\n",
      "Epoch [248/300], Loss: 0.3136\n",
      "Epoch [249/300], Loss: 0.2240\n",
      "Epoch [250/300], Loss: 0.3667\n",
      "Epoch [251/300], Loss: 0.2004\n",
      "Epoch [252/300], Loss: 0.2304\n",
      "Epoch [253/300], Loss: 0.0781\n",
      "Epoch [254/300], Loss: 0.4888\n",
      "Epoch [255/300], Loss: 0.1882\n",
      "Epoch [256/300], Loss: 0.8083\n",
      "Epoch [257/300], Loss: 0.2075\n",
      "Epoch [258/300], Loss: 1.3196\n",
      "Epoch [259/300], Loss: 0.3587\n",
      "Epoch [260/300], Loss: 0.2532\n",
      "Epoch [261/300], Loss: 0.2060\n",
      "Epoch [262/300], Loss: 0.6082\n",
      "Epoch [263/300], Loss: 0.3677\n",
      "Epoch [264/300], Loss: 0.1862\n",
      "Epoch [265/300], Loss: 0.2819\n",
      "Epoch [266/300], Loss: 0.2134\n",
      "Epoch [267/300], Loss: 1.2061\n",
      "Epoch [268/300], Loss: 1.9280\n",
      "Epoch [269/300], Loss: 0.2899\n",
      "Epoch [270/300], Loss: 0.2219\n",
      "Epoch [271/300], Loss: 0.4021\n",
      "Epoch [272/300], Loss: 0.5414\n",
      "Epoch [273/300], Loss: 0.4099\n",
      "Epoch [274/300], Loss: 0.2846\n",
      "Epoch [275/300], Loss: 0.4817\n",
      "Epoch [276/300], Loss: 0.2706\n",
      "Epoch [277/300], Loss: 0.2089\n",
      "Epoch [278/300], Loss: 0.2589\n",
      "Epoch [279/300], Loss: 0.2290\n",
      "Epoch [280/300], Loss: 0.2279\n",
      "Epoch [281/300], Loss: 1.8308\n",
      "Epoch [282/300], Loss: 0.2308\n",
      "Epoch [283/300], Loss: 0.2828\n",
      "Epoch [284/300], Loss: 0.2242\n",
      "Epoch [285/300], Loss: 0.9702\n",
      "Epoch [286/300], Loss: 0.1052\n",
      "Epoch [287/300], Loss: 0.3858\n",
      "Epoch [288/300], Loss: 0.0854\n",
      "Epoch [289/300], Loss: 0.1905\n",
      "Epoch [290/300], Loss: 0.5037\n",
      "Epoch [291/300], Loss: 0.2207\n",
      "Epoch [292/300], Loss: 0.2985\n",
      "Epoch [293/300], Loss: 0.6535\n",
      "Epoch [294/300], Loss: 0.3864\n",
      "Epoch [295/300], Loss: 0.8473\n",
      "Epoch [296/300], Loss: 0.4387\n",
      "Epoch [297/300], Loss: 0.2761\n",
      "Epoch [298/300], Loss: 0.3085\n",
      "Epoch [299/300], Loss: 0.0579\n",
      "Epoch [300/300], Loss: 0.3009\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 300\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for i, (sequences, labels) in enumerate(data_loader):\n",
    "        # Move data to the device\n",
    "        sequences = sequences.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(sequences)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.5134,  0.2614, -1.4426,  ...,  0.5818,  0.2272,  0.0153],\n",
       "         [ 0.5130,  0.2604, -1.4262,  ...,  0.5810,  0.2273,  0.0147],\n",
       "         [ 0.5126,  0.2599, -1.4278,  ...,  0.5810,  0.2276,  0.0150],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 0.5023,  0.2809, -1.6242,  ...,  0.5847,  0.2321,  0.0112],\n",
       "         [ 0.5023,  0.2806, -1.6631,  ...,  0.5840,  0.2322,  0.0130],\n",
       "         [ 0.5022,  0.2805, -1.6912,  ...,  0.5837,  0.2322,  0.0122],\n",
       "         ...,\n",
       "         [ 0.5037,  0.2791, -1.5789,  ...,  0.5834,  0.2224,  0.0129],\n",
       "         [ 0.5044,  0.2774, -1.5686,  ...,  0.5831,  0.2220,  0.0130],\n",
       "         [ 0.5052,  0.2722, -1.5527,  ...,  0.5827,  0.2216,  0.0132]],\n",
       "\n",
       "        [[ 0.4868,  0.2821, -1.4668,  ...,  0.5711,  0.2335,  0.0116],\n",
       "         [ 0.4861,  0.2786, -1.5812,  ...,  0.5702,  0.2334,  0.0129],\n",
       "         [ 0.4856,  0.2769, -1.6059,  ...,  0.5699,  0.2333,  0.0131],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 0.4992,  0.1994, -1.1906,  ...,  0.5657,  0.1731,  0.0116],\n",
       "         [ 0.4988,  0.2047, -1.3590,  ...,  0.5663,  0.1723,  0.0123],\n",
       "         [ 0.4982,  0.2082, -1.3140,  ...,  0.5671,  0.1730,  0.0129],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]])"
      ]
     },
     "execution_count": 501,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_sequence(sequences, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.2644,  3.4531, -1.2713, -2.3117, -3.7239, -3.0003, -2.7968,  2.8237],\n",
       "        [-0.1658,  5.7221, -2.5147,  0.0424, -4.1849, -1.1155, -3.5870,  3.3477],\n",
       "        [ 0.8228,  1.0697,  4.4864, -3.4782, -1.4098, -3.5199, -1.6029,  1.1104],\n",
       "        [-2.3772, -0.7847, -2.9117,  4.4399, -1.3170,  3.7237, -1.7338,  1.3067],\n",
       "        [-0.6764, -1.5457, -0.2841, -0.9745,  6.4210, -0.7854,  1.4269, -1.8412],\n",
       "        [-2.6242, -1.3845, -3.0832,  4.3615, -0.6597,  5.0905, -1.1641,  0.5335],\n",
       "        [ 0.7772, -1.0372, -0.8932, -2.2138,  0.1314, -2.0715,  5.5156, -2.6485],\n",
       "        [-1.6479,  2.1180, -2.6393,  1.4775, -3.6063,  0.2648, -3.4498,  4.5402]])"
      ]
     },
     "execution_count": 502,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Put the model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# No need to track gradients during inference\n",
    "with torch.no_grad():\n",
    "    # Get the model's output (logits)\n",
    "    outputs = model(padded_sequences)\n",
    "\n",
    "# outputs = torch.softmax(outputs, dim=1)\n",
    "# outputs = torch.max(outputs,1)\n",
    "\n",
    "outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -------------------------------------------------------------------------------------------------------------------------------------------- ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "# Load the sequences\n",
    "import numpy as np \n",
    "import os\n",
    "Model_Data=os.path.join('Data for different actions')\n",
    "action = \"กฎหมายรัฐธรรมนูญ.mp4\"\n",
    "a = np.load(os.path.join(Model_Data, action, \"กฎหมายรัฐธรรมนูญ.npy\"))\n",
    "a = torch.from_numpy(a)\n",
    "\n",
    "# Pad the sequences to the same length\n",
    "padded_sequences = pad_sequence(a, batch_first=True)\n",
    "len(padded_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 803,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 804,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelMap = {label:num for num, label in enumerate(actions)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 806,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs, labels = [], []\n",
    "\n",
    "for action in actions:\n",
    "    for seq in range(no_of_seqs):\n",
    "        window = []\n",
    "        for frame_num in range(seq_length):\n",
    "            res = np.load(os.path.join(Model_Data, action, f\"frame_{frame_num}.npy\")) \n",
    "            window.append(res)\n",
    "        seqs.append(window)\n",
    "\n",
    "        labels.append(labelMap[action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(seqs).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 809,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = np.array(seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing the labels from 0,1,2 to categorical data for easier accessebility\n",
    "Y_label = to_categorical(labels).astype(int)\n",
    "Y_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_data, Y_label, test_size=0.3)\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 815,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 816,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding the logs folder\n",
    "log_dir = os.path.join('Logs')\n",
    "tb_callback = TensorBoard(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural network\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, return_sequences=True, activation='relu', input_shape=(30,1662)))\n",
    "model.add(LSTM(128, return_sequences=True, activation='relu'))\n",
    "model.add(LSTM(64, return_sequences=False, activation='relu'))\n",
    "\n",
    "# adding 64 units for dense layer\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(actions.shape[0], activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eg\n",
    "eg_res = [.7, 0.2, 0.1]\n",
    "actions[np.argmax(eg_res)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 819,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, Y_train, epochs=300, callbacks=[tb_callback])\n",
    "# tensorboard --logdir=."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res=model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# again the actions with the max value provided by softmax is returned\n",
    "actions[np.argmax(res[4])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions[np.argmax(Y_test[4])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 836,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_hat = model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 838,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_true = np.argmax(Y_train, axis=1).tolist()\n",
    "# one hot encoding\n",
    "Y_hat = np.argmax(Y_hat, axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confution matrix\n",
    "multilabel_confusion_matrix(Y_true, Y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(Y_true, Y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
