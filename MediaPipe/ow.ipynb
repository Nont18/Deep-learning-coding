{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holist = mp.solutions.holistic \n",
    "mp_draw = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(img, model):\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img.flags.writeable = False                 \n",
    "    result = model.process(img)                 # Make prediction\n",
    "    img.flags.writeable = True                   \n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR) \n",
    "    return img, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks(img, result):\n",
    "    mp_draw.draw_landmarks(img, result.face_landmarks, mp_holist.FACEMESH_CONTOURS) # Draw face connections\n",
    "    mp_draw.draw_landmarks(img, result.pose_landmarks, mp_holist.POSE_CONNECTIONS) # Draw pose connections\n",
    "    mp_draw.draw_landmarks(img, result.left_hand_landmarks, mp_holist.HAND_CONNECTIONS) # Draw left hand connections\n",
    "    mp_draw.draw_landmarks(img, result.right_hand_landmarks, mp_holist.HAND_CONNECTIONS) # Draw right hand connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_styled_landmarks(img, result):\n",
    "    mp_draw.draw_landmarks(img, result.face_landmarks, mp_holist.FACEMESH_CONTOURS, \n",
    "                             mp_draw.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1), # color the joint \n",
    "                             mp_draw.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1) #color the connection\n",
    "                             ) \n",
    "    # mp_draw.draw_landmarks(img, result.face_landmarks, mp_holist.FACEMESH_CONTOURS, \n",
    "    #                          mp_draw.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1), # color the joint \n",
    "    #                          mp_draw.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1) #color the connection\n",
    "    #                          ) \n",
    "    \n",
    "    mp_draw.draw_landmarks(img, result.pose_landmarks, mp_holist.POSE_CONNECTIONS,\n",
    "                             mp_draw.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4), \n",
    "                             mp_draw.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    mp_draw.draw_landmarks(img, result.left_hand_landmarks, mp_holist.HAND_CONNECTIONS, \n",
    "                             mp_draw.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4), \n",
    "                             mp_draw.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    mp_draw.draw_landmarks(img, result.right_hand_landmarks, mp_holist.HAND_CONNECTIONS, \n",
    "                             mp_draw.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4), \n",
    "                             mp_draw.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                             ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "frozenset({(0, 1),\n",
       "           (0, 4),\n",
       "           (1, 2),\n",
       "           (2, 3),\n",
       "           (3, 7),\n",
       "           (4, 5),\n",
       "           (5, 6),\n",
       "           (6, 8),\n",
       "           (9, 10),\n",
       "           (11, 12),\n",
       "           (11, 13),\n",
       "           (11, 23),\n",
       "           (12, 14),\n",
       "           (12, 24),\n",
       "           (13, 15),\n",
       "           (14, 16),\n",
       "           (15, 17),\n",
       "           (15, 19),\n",
       "           (15, 21),\n",
       "           (16, 18),\n",
       "           (16, 20),\n",
       "           (16, 22),\n",
       "           (17, 19),\n",
       "           (18, 20),\n",
       "           (23, 24),\n",
       "           (23, 25),\n",
       "           (24, 26),\n",
       "           (25, 27),\n",
       "           (26, 28),\n",
       "           (27, 29),\n",
       "           (27, 31),\n",
       "           (28, 30),\n",
       "           (28, 32),\n",
       "           (29, 31),\n",
       "           (30, 32)})"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mp_holist.POSE_CONNECTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\araya\\AppData\\Roaming\\Python\\Python312\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "# Set mediapipe model \n",
    "with mp_holist.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        print(results)\n",
    "        \n",
    "        draw_styled_landmarks(image, results)\n",
    "\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9995303153991699"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.pose_landmarks.landmark[0].visibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results.pose_landmarks.landmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(results):\n",
    "    pose=np.array([[res.x,res.y,res.z,res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    left_hnd=np.array([[res.x,res.y,res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    right_hnd=np.array([[res.x,res.y,res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    face=np.array([[res.x,res.y,res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "    return np.concatenate([pose,left_hnd,right_hnd,face])\n",
    "# concatenating for the model to detect the sign language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1662,)"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_keypoints(results).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "video_dir = \"C:/Users/araya/Desktop/keypoints/video_extract\"\n",
    "video_list = []\n",
    "video_list = os.listdir(video_dir)\n",
    "\n",
    "len(video_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['กฎกระทรวง.mp4',\n",
       " 'กฎหมายรัฐธรรมนูญ.mp4',\n",
       " 'กรมอนามัย.mp4',\n",
       " 'กรรม.mp4',\n",
       " 'กรรมสิทธิ์.mp4',\n",
       " 'กระโดด.mp4',\n",
       " 'กล้วยบวชชี.mp4',\n",
       " 'กล้วยเชื่อม.mp4',\n",
       " 'เขิน.mp4',\n",
       " 'เขื่อนดิน.mp4',\n",
       " 'เขื่อนสิริกิติ์.mp4',\n",
       " 'เข้าใจผิด.mp4',\n",
       " 'เคย.mp4',\n",
       " 'เครียด.mp4',\n",
       " 'เครื่องปั่นดิน.mp4',\n",
       " 'เครื่องหมายการค้า.mp4',\n",
       " 'เจอ.mp4',\n",
       " 'เจ้าหนี้.mp4',\n",
       " 'เช่าซื้อ.mp4',\n",
       " 'เช่าทรัพย์.mp4',\n",
       " 'โจทก์.mp4']"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for exported data, numpy arrays\n",
    "Model_Data=os.path.join('Data for different actions')\n",
    "\n",
    "actions = np.array(video_list)\n",
    "\n",
    "no_of_seqs = 1\n",
    "\n",
    "# 30 frames in length\n",
    "seq_length = 160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['กฎกระทรวง.mp4', 'กฎหมายรัฐธรรมนูญ.mp4', 'กรมอนามัย.mp4',\n",
       "       'กรรม.mp4', 'กรรมสิทธิ์.mp4', 'กระโดด.mp4', 'กล้วยบวชชี.mp4',\n",
       "       'กล้วยเชื่อม.mp4', 'เขิน.mp4', 'เขื่อนดิน.mp4',\n",
       "       'เขื่อนสิริกิติ์.mp4', 'เข้าใจผิด.mp4', 'เคย.mp4', 'เครียด.mp4',\n",
       "       'เครื่องปั่นดิน.mp4', 'เครื่องหมายการค้า.mp4', 'เจอ.mp4',\n",
       "       'เจ้าหนี้.mp4', 'เช่าซื้อ.mp4', 'เช่าทรัพย์.mp4', 'โจทก์.mp4'],\n",
       "      dtype='<U21')"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just creating the folders and sub folders\n",
    "\n",
    "for action in actions: \n",
    "    try: \n",
    "        os.makedirs(os.path.join(Model_Data, action))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# augment_dir = \"C:/Users/araya/Desktop/augments\"\n",
    "\n",
    "# augment_list = []\n",
    "# augment_list = os.listdir(augment_dir)\n",
    "# augment_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actions = list(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x in augment_list:\n",
    "#     # print(x)\n",
    "#     actions.append(x)\n",
    "# actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actions = np.array(actions)\n",
    "# actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collecting keypoint values for Training nd Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory where your videos are stored\n",
    "directory = \"C:/Users/araya/Desktop/keypoints/video_extract\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:/Users/araya/Desktop/keypoints/video_extract'"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'my name is Peter', 'I am 26 years old']\n"
     ]
    }
   ],
   "source": [
    "txt = \"hello, my name is Peter, I am 26 years old\"\n",
    "\n",
    "x = txt.split(\", \")\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/araya/Desktop/keypoints/video_extract/กฎกระทรวง.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/กฎหมายรัฐธรรมนูญ.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/กรมอนามัย.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/กรรม.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/กรรมสิทธิ์.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/กระโดด.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/กล้วยบวชชี.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/กล้วยเชื่อม.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/เขิน.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/เขื่อนดิน.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/เขื่อนสิริกิติ์.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/เข้าใจผิด.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/เคย.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/เครียด.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/เครื่องปั่นดิน.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/เครื่องหมายการค้า.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/เจอ.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/เจ้าหนี้.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/เช่าซื้อ.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/เช่าทรัพย์.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/โจทก์.mp4\n"
     ]
    }
   ],
   "source": [
    "for filename in actions:\n",
    "    print(directory + '/' + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set mediapipe model \n",
    "# for action in actions:\n",
    "#     video_path = os.path.join(\"C:/Users/araya/Desktop/keypoints/video_extract\", action)\n",
    "#     cap = cv2.VideoCapture(video_path)\n",
    "#     cap.set(cv2.CAP_PROP_FPS, 60)\n",
    "#     length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "#     print(\"LENGTH:\" + str(length))\n",
    "#     # keypoints = []\n",
    "\n",
    "#     if not cap.isOpened():\n",
    "#         print(f\"Error opening video file: {video_path}\")\n",
    "#         continue\n",
    "\n",
    "#     with mp_holist.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "#         for seq in range(no_of_seqs):\n",
    "#             for frame_num in range(seq_length):\n",
    "\n",
    "#                 ret, frame = cap.read()\n",
    "#                 if not ret:\n",
    "#                     print(f\"End of video {video_path}\")\n",
    "#                     break\n",
    "                \n",
    "#                 img, results = mediapipe_detection(frame, holistic)\n",
    "#                 draw_styled_landmarks(img, results)\n",
    "\n",
    "#                 # print(frame_num)\n",
    "\n",
    "#                 if frame_num == 0: \n",
    "#                     cv2.putText(img, 'DATA COLLECTION STARTED', (120,200), \n",
    "#                                 cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255, 0), 4, cv2.LINE_AA)\n",
    "#                     cv2.putText(img, f'Collecting frames for - {action} Sequence Number - {seq}', (15,12), \n",
    "#                                 cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "#                     cv2.imshow('OpenCV Window', img)\n",
    "#                     cv2.waitKey(2000)  # 2 seconds delay for setup\n",
    "#                 else: \n",
    "#                     cv2.putText(img, f'Collecting frames for - {action} Sequence Number - {seq}', (15,12), \n",
    "#                                 cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "#                     cv2.imshow('OpenCV Window', img)\n",
    "\n",
    "#                 keypoints = extract_keypoints(results)\n",
    "#                 # keypoints.append(results)\n",
    "#                 npy_path = os.path.join(Model_Data, action, f\"frame_{frame_num}.npy\")\n",
    "#                 os.makedirs(os.path.dirname(npy_path), exist_ok=True)\n",
    "#                 np.save(npy_path, keypoints)\n",
    "\n",
    "#                 if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#                     break\n",
    "\n",
    "#             if not ret:\n",
    "#                 break\n",
    "\n",
    "#     cap.release()\n",
    "#     cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np \n",
    "\n",
    "# X = [[1,2,3]]\n",
    "# X.append([6,8,10])\n",
    "# X.append([20,9,4])\n",
    "# X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LENGTH:142\n",
      "End of video C:/Users/araya/Desktop/keypoints/video_extract\\กฎกระทรวง.mp4\n",
      "LENGTH:134\n",
      "End of video C:/Users/araya/Desktop/keypoints/video_extract\\กฎหมายรัฐธรรมนูญ.mp4\n",
      "LENGTH:151\n",
      "End of video C:/Users/araya/Desktop/keypoints/video_extract\\กรมอนามัย.mp4\n",
      "LENGTH:100\n",
      "End of video C:/Users/araya/Desktop/keypoints/video_extract\\กรรม.mp4\n",
      "LENGTH:184\n",
      "LENGTH:94\n",
      "End of video C:/Users/araya/Desktop/keypoints/video_extract\\กระโดด.mp4\n",
      "LENGTH:182\n",
      "LENGTH:128\n",
      "End of video C:/Users/araya/Desktop/keypoints/video_extract\\กล้วยเชื่อม.mp4\n",
      "LENGTH:100\n",
      "End of video C:/Users/araya/Desktop/keypoints/video_extract\\เขิน.mp4\n",
      "LENGTH:161\n",
      "LENGTH:208\n",
      "LENGTH:105\n",
      "End of video C:/Users/araya/Desktop/keypoints/video_extract\\เข้าใจผิด.mp4\n",
      "LENGTH:82\n",
      "End of video C:/Users/araya/Desktop/keypoints/video_extract\\เคย.mp4\n",
      "LENGTH:118\n",
      "End of video C:/Users/araya/Desktop/keypoints/video_extract\\เครียด.mp4\n",
      "LENGTH:234\n",
      "LENGTH:126\n",
      "End of video C:/Users/araya/Desktop/keypoints/video_extract\\เครื่องหมายการค้า.mp4\n",
      "LENGTH:92\n",
      "End of video C:/Users/araya/Desktop/keypoints/video_extract\\เจอ.mp4\n",
      "LENGTH:117\n",
      "End of video C:/Users/araya/Desktop/keypoints/video_extract\\เจ้าหนี้.mp4\n",
      "LENGTH:138\n",
      "End of video C:/Users/araya/Desktop/keypoints/video_extract\\เช่าซื้อ.mp4\n",
      "LENGTH:111\n",
      "End of video C:/Users/araya/Desktop/keypoints/video_extract\\เช่าทรัพย์.mp4\n",
      "LENGTH:108\n",
      "End of video C:/Users/araya/Desktop/keypoints/video_extract\\โจทก์.mp4\n"
     ]
    }
   ],
   "source": [
    "# Set mediapipe model \n",
    "for action in actions:\n",
    "    video_path = os.path.join(\"C:/Users/araya/Desktop/keypoints/video_extract\", action)\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    cap.set(cv2.CAP_PROP_FPS, 60)\n",
    "    length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    print(\"LENGTH:\" + str(length))\n",
    "    keypoints = []\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error opening video file: {video_path}\")\n",
    "        continue\n",
    "\n",
    "    with mp_holist.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "        for seq in range(no_of_seqs):\n",
    "            for frame_num in range(seq_length):\n",
    "\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    print(f\"End of video {video_path}\")\n",
    "                    break\n",
    "                \n",
    "                img, results = mediapipe_detection(frame, holistic)\n",
    "                draw_styled_landmarks(img, results)\n",
    "\n",
    "                # print(frame_num)\n",
    "\n",
    "                if frame_num == 0: \n",
    "                    cv2.putText(img, 'DATA COLLECTION STARTED', (120,200), \n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255, 0), 4, cv2.LINE_AA)\n",
    "                    cv2.putText(img, f'Collecting frames for - {action} Sequence Number - {seq}', (15,12), \n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                    cv2.imshow('OpenCV Window', img)\n",
    "                    cv2.waitKey(2000)  # 2 seconds delay for setup\n",
    "                else: \n",
    "                    cv2.putText(img, f'Collecting frames for - {action} Sequence Number - {seq}', (15,12), \n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                    cv2.imshow('OpenCV Window', img)\n",
    "\n",
    "                x = extract_keypoints(results)\n",
    "                keypoints.append(x)\n",
    "                npy_path = os.path.join(Model_Data, action, f\"{action.split(\".\")[0]}.npy\")\n",
    "                os.makedirs(os.path.dirname(npy_path), exist_ok=True)\n",
    "                np.save(npy_path, keypoints)\n",
    "\n",
    "                if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                    break\n",
    "\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Data for different actions/กฎกระทรวง.mp4/กฎกระทรวง.npy', 'Data for different actions/กฎหมายรัฐธรรมนูญ.mp4/กฎหมายรัฐธรรมนูญ.npy', 'Data for different actions/กรมอนามัย.mp4/กรมอนามัย.npy', 'Data for different actions/กรรม.mp4/กรรม.npy', 'Data for different actions/กรรมสิทธิ์.mp4/กรรมสิทธิ์.npy', 'Data for different actions/กระโดด.mp4/กระโดด.npy', 'Data for different actions/กล้วยบวชชี.mp4/กล้วยบวชชี.npy', 'Data for different actions/กล้วยเชื่อม.mp4/กล้วยเชื่อม.npy', 'Data for different actions/เขิน.mp4/เขิน.npy', 'Data for different actions/เขื่อนดิน.mp4/เขื่อนดิน.npy', 'Data for different actions/เขื่อนสิริกิติ์.mp4/เขื่อนสิริกิติ์.npy', 'Data for different actions/เข้าใจผิด.mp4/เข้าใจผิด.npy', 'Data for different actions/เคย.mp4/เคย.npy', 'Data for different actions/เครียด.mp4/เครียด.npy', 'Data for different actions/เครื่องปั่นดิน.mp4/เครื่องปั่นดิน.npy', 'Data for different actions/เครื่องหมายการค้า.mp4/เครื่องหมายการค้า.npy', 'Data for different actions/เจอ.mp4/เจอ.npy', 'Data for different actions/เจ้าหนี้.mp4/เจ้าหนี้.npy', 'Data for different actions/เช่าซื้อ.mp4/เช่าซื้อ.npy', 'Data for different actions/เช่าทรัพย์.mp4/เช่าทรัพย์.npy', 'Data for different actions/โจทก์.mp4/โจทก์.npy']\n"
     ]
    }
   ],
   "source": [
    "file_paths = []\n",
    "for action in actions:\n",
    "    video_path = os.path.join('Data for different actions/', action)\n",
    "    # print(video_path)\n",
    "    # print(action)\n",
    "    file_paths.append(video_path + '/' + action.split(\".\")[0] + \".npy\")\n",
    "print(file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_keypoint_sequences(file_paths):\n",
    "    keypoint_sequences = []\n",
    "    for file_path in file_paths:\n",
    "        keypoints = np.load(file_path)\n",
    "        keypoint_sequences.append(torch.tensor(keypoints, dtype=torch.float32))\n",
    "    return keypoint_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 0.5013,  0.2452, -1.2167,  ...,  0.5663,  0.2188,  0.0098],\n",
       "         [ 0.4997,  0.2482, -1.4690,  ...,  0.5652,  0.2181,  0.0106],\n",
       "         [ 0.4984,  0.2500, -1.4853,  ...,  0.5654,  0.2185,  0.0112],\n",
       "         ...,\n",
       "         [ 0.4861,  0.2513, -1.3416,  ...,  0.5572,  0.2177,  0.0091],\n",
       "         [ 0.4873,  0.2514, -1.3574,  ...,  0.5575,  0.2172,  0.0097],\n",
       "         [ 0.4883,  0.2516, -1.3579,  ...,  0.5577,  0.2170,  0.0101]]),\n",
       " tensor([[ 0.4922,  0.2382, -1.2850,  ...,  0.5578,  0.2124,  0.0094],\n",
       "         [ 0.4920,  0.2405, -1.4288,  ...,  0.5571,  0.2116,  0.0099],\n",
       "         [ 0.4920,  0.2409, -1.4093,  ...,  0.5567,  0.2122,  0.0098],\n",
       "         ...,\n",
       "         [ 0.4814,  0.2260, -1.3318,  ...,  0.5503,  0.1923,  0.0123],\n",
       "         [ 0.4815,  0.2257, -1.3351,  ...,  0.5503,  0.1921,  0.0122],\n",
       "         [ 0.4815,  0.2255, -1.3497,  ...,  0.5501,  0.1919,  0.0124]]),\n",
       " tensor([[ 0.5049,  0.2371, -1.2115,  ...,  0.5643,  0.2082,  0.0088],\n",
       "         [ 0.5045,  0.2381, -1.1896,  ...,  0.5643,  0.2081,  0.0085],\n",
       "         [ 0.5041,  0.2385, -1.1915,  ...,  0.5643,  0.2080,  0.0089],\n",
       "         ...,\n",
       "         [ 0.4963,  0.2307, -1.3096,  ...,  0.5624,  0.2043,  0.0127],\n",
       "         [ 0.4965,  0.2308, -1.3085,  ...,  0.5625,  0.2045,  0.0126],\n",
       "         [ 0.4972,  0.2312, -1.3053,  ...,  0.5633,  0.2045,  0.0128]]),\n",
       " tensor([[ 0.5134,  0.2614, -1.4426,  ...,  0.5818,  0.2272,  0.0153],\n",
       "         [ 0.5130,  0.2604, -1.4262,  ...,  0.5810,  0.2273,  0.0147],\n",
       "         [ 0.5126,  0.2599, -1.4278,  ...,  0.5810,  0.2276,  0.0150],\n",
       "         ...,\n",
       "         [ 0.5079,  0.2693, -1.4999,  ...,  0.5780,  0.2349,  0.0115],\n",
       "         [ 0.5090,  0.2688, -1.4936,  ...,  0.5782,  0.2346,  0.0116],\n",
       "         [ 0.5092,  0.2683, -1.4518,  ...,  0.5786,  0.2341,  0.0116]]),\n",
       " tensor([[ 0.4883,  0.2402, -1.1024,  ...,  0.5482,  0.2132,  0.0081],\n",
       "         [ 0.4878,  0.2402, -1.1906,  ...,  0.5469,  0.2135,  0.0083],\n",
       "         [ 0.4863,  0.2402, -1.1774,  ...,  0.5477,  0.2141,  0.0087],\n",
       "         ...,\n",
       "         [ 0.4788,  0.3129, -1.6072,  ...,  0.5349,  0.2531,  0.0022],\n",
       "         [ 0.4782,  0.3129, -1.6350,  ...,  0.5344,  0.2525,  0.0022],\n",
       "         [ 0.4771,  0.3131, -1.6312,  ...,  0.5339,  0.2515,  0.0022]]),\n",
       " tensor([[ 0.4992,  0.1994, -1.1906,  ...,  0.5657,  0.1731,  0.0116],\n",
       "         [ 0.4988,  0.2047, -1.3590,  ...,  0.5663,  0.1723,  0.0123],\n",
       "         [ 0.4982,  0.2082, -1.3140,  ...,  0.5671,  0.1730,  0.0129],\n",
       "         ...,\n",
       "         [ 0.4743,  0.1974, -1.3230,  ...,  0.5475,  0.1620,  0.0155],\n",
       "         [ 0.4732,  0.1974, -1.3174,  ...,  0.5461,  0.1615,  0.0151],\n",
       "         [ 0.4720,  0.1974, -1.3149,  ...,  0.5453,  0.1614,  0.0149]]),\n",
       " tensor([[ 0.5023,  0.2809, -1.6242,  ...,  0.5847,  0.2321,  0.0112],\n",
       "         [ 0.5023,  0.2806, -1.6631,  ...,  0.5840,  0.2322,  0.0130],\n",
       "         [ 0.5022,  0.2805, -1.6912,  ...,  0.5837,  0.2322,  0.0122],\n",
       "         ...,\n",
       "         [ 0.5037,  0.2791, -1.5789,  ...,  0.5834,  0.2224,  0.0129],\n",
       "         [ 0.5044,  0.2774, -1.5686,  ...,  0.5831,  0.2220,  0.0130],\n",
       "         [ 0.5052,  0.2722, -1.5527,  ...,  0.5827,  0.2216,  0.0132]]),\n",
       " tensor([[ 0.4868,  0.2821, -1.4668,  ...,  0.5711,  0.2335,  0.0116],\n",
       "         [ 0.4861,  0.2786, -1.5812,  ...,  0.5702,  0.2334,  0.0129],\n",
       "         [ 0.4856,  0.2769, -1.6059,  ...,  0.5699,  0.2333,  0.0131],\n",
       "         ...,\n",
       "         [ 0.4826,  0.2583, -1.6140,  ...,  0.5606,  0.2261,  0.0154],\n",
       "         [ 0.4820,  0.2583, -1.5231,  ...,  0.5603,  0.2261,  0.0156],\n",
       "         [ 0.4818,  0.2580, -1.5255,  ...,  0.5600,  0.2263,  0.0156]]),\n",
       " tensor([[ 0.5032,  0.2228, -1.1998,  ...,  0.5663,  0.1971,  0.0106],\n",
       "         [ 0.5035,  0.2228, -1.2072,  ...,  0.5662,  0.1969,  0.0106],\n",
       "         [ 0.5035,  0.2230, -1.2082,  ...,  0.5664,  0.1963,  0.0108],\n",
       "         ...,\n",
       "         [ 0.4982,  0.2206, -1.2743,  ...,  0.5578,  0.1892,  0.0092],\n",
       "         [ 0.4983,  0.2178, -1.2380,  ...,  0.5581,  0.1883,  0.0091],\n",
       "         [ 0.4979,  0.2173, -1.2264,  ...,  0.5578,  0.1877,  0.0096]]),\n",
       " tensor([[ 0.5442,  0.2507, -1.4443,  ...,  0.6105,  0.2203,  0.0123],\n",
       "         [ 0.5444,  0.2508, -1.6574,  ...,  0.6094,  0.2198,  0.0134],\n",
       "         [ 0.5445,  0.2510, -1.7250,  ...,  0.6093,  0.2195,  0.0144],\n",
       "         ...,\n",
       "         [ 0.5324,  0.2577, -1.6704,  ...,  0.5975,  0.2210,  0.0145],\n",
       "         [ 0.5323,  0.2560, -1.6673,  ...,  0.5966,  0.2206,  0.0148],\n",
       "         [ 0.5323,  0.2553, -1.6826,  ...,  0.5957,  0.2201,  0.0146]]),\n",
       " tensor([[ 0.5402,  0.2562, -1.5458,  ...,  0.6041,  0.2257,  0.0125],\n",
       "         [ 0.5389,  0.2596, -1.7020,  ...,  0.6031,  0.2264,  0.0130],\n",
       "         [ 0.5379,  0.2616, -1.7134,  ...,  0.6027,  0.2258,  0.0139],\n",
       "         ...,\n",
       "         [ 0.5153,  0.2631, -1.6030,  ...,  0.5956,  0.2125,  0.0142],\n",
       "         [ 0.5176,  0.2626, -1.6101,  ...,  0.5985,  0.2126,  0.0139],\n",
       "         [ 0.5197,  0.2624, -1.5662,  ...,  0.6011,  0.2119,  0.0145]]),\n",
       " tensor([[ 0.5030,  0.2553, -1.1988,  ...,  0.5699,  0.2265,  0.0097],\n",
       "         [ 0.5028,  0.2582, -1.2761,  ...,  0.5689,  0.2266,  0.0109],\n",
       "         [ 0.5028,  0.2605, -1.3315,  ...,  0.5690,  0.2269,  0.0105],\n",
       "         ...,\n",
       "         [ 0.5028,  0.2675, -1.4499,  ...,  0.5672,  0.2323,  0.0110],\n",
       "         [ 0.5007,  0.2672, -1.4234,  ...,  0.5668,  0.2317,  0.0115],\n",
       "         [ 0.4988,  0.2671, -1.4308,  ...,  0.5670,  0.2312,  0.0118]]),\n",
       " tensor([[ 0.5069,  0.2355, -1.3384,  ...,  0.5700,  0.2051,  0.0053],\n",
       "         [ 0.5045,  0.2395, -1.5097,  ...,  0.5699,  0.2043,  0.0064],\n",
       "         [ 0.5028,  0.2420, -1.5081,  ...,  0.5699,  0.2045,  0.0069],\n",
       "         ...,\n",
       "         [ 0.4952,  0.2462, -1.4319,  ...,  0.5658,  0.2015,  0.0122],\n",
       "         [ 0.4952,  0.2448, -1.4781,  ...,  0.5661,  0.2014,  0.0123],\n",
       "         [ 0.4953,  0.2436, -1.4689,  ...,  0.5662,  0.2013,  0.0124]]),\n",
       " tensor([[ 0.5035,  0.2282, -1.1379,  ...,  0.5710,  0.1953,  0.0120],\n",
       "         [ 0.5033,  0.2284, -1.2771,  ...,  0.5713,  0.1954,  0.0123],\n",
       "         [ 0.5032,  0.2286, -1.2743,  ...,  0.5715,  0.1957,  0.0122],\n",
       "         ...,\n",
       "         [ 0.4810,  0.2296, -1.4129,  ...,  0.5542,  0.1897,  0.0133],\n",
       "         [ 0.4812,  0.2296, -1.4275,  ...,  0.5549,  0.1900,  0.0130],\n",
       "         [ 0.4816,  0.2299, -1.4337,  ...,  0.5557,  0.1903,  0.0133]]),\n",
       " tensor([[ 0.5108,  0.2425, -1.1053,  ...,  0.5724,  0.2176,  0.0127],\n",
       "         [ 0.5091,  0.2430, -1.3007,  ...,  0.5713,  0.2177,  0.0129],\n",
       "         [ 0.5080,  0.2432, -1.3035,  ...,  0.5714,  0.2179,  0.0130],\n",
       "         ...,\n",
       "         [ 0.4966,  0.2620, -1.5367,  ...,  0.5653,  0.2259,  0.0082],\n",
       "         [ 0.4968,  0.2621, -1.5419,  ...,  0.5656,  0.2262,  0.0083],\n",
       "         [ 0.4971,  0.2623, -1.5482,  ...,  0.5658,  0.2263,  0.0086]]),\n",
       " tensor([[ 0.4878,  0.2235, -1.2515,  ...,  0.5511,  0.1981,  0.0114],\n",
       "         [ 0.4870,  0.2286, -1.4279,  ...,  0.5506,  0.1977,  0.0105],\n",
       "         [ 0.4865,  0.2315, -1.4431,  ...,  0.5508,  0.1984,  0.0110],\n",
       "         ...,\n",
       "         [ 0.4901,  0.2217, -1.3449,  ...,  0.5576,  0.1888,  0.0156],\n",
       "         [ 0.4898,  0.2224, -1.3780,  ...,  0.5575,  0.1892,  0.0157],\n",
       "         [ 0.4896,  0.2232, -1.4140,  ...,  0.5572,  0.1897,  0.0156]]),\n",
       " tensor([[ 0.5194,  0.2227, -1.3410,  ...,  0.5911,  0.1970,  0.0142],\n",
       "         [ 0.5200,  0.2228, -1.3298,  ...,  0.5912,  0.1965,  0.0131],\n",
       "         [ 0.5202,  0.2230, -1.3074,  ...,  0.5909,  0.1970,  0.0149],\n",
       "         ...,\n",
       "         [ 0.5146,  0.2198, -1.3267,  ...,  0.5821,  0.1907,  0.0159],\n",
       "         [ 0.5141,  0.2205, -1.3207,  ...,  0.5816,  0.1911,  0.0154],\n",
       "         [ 0.5137,  0.2209, -1.3223,  ...,  0.5812,  0.1917,  0.0161]]),\n",
       " tensor([[ 0.5103,  0.2428, -1.1660,  ...,  0.5757,  0.2179,  0.0081],\n",
       "         [ 0.5106,  0.2437, -1.2850,  ...,  0.5751,  0.2183,  0.0099],\n",
       "         [ 0.5112,  0.2443, -1.2880,  ...,  0.5748,  0.2189,  0.0108],\n",
       "         ...,\n",
       "         [ 0.4800,  0.2603, -1.3086,  ...,  0.5532,  0.2237,  0.0069],\n",
       "         [ 0.4807,  0.2604, -1.2544,  ...,  0.5539,  0.2238,  0.0067],\n",
       "         [ 0.4813,  0.2604, -1.2161,  ...,  0.5548,  0.2238,  0.0065]]),\n",
       " tensor([[ 0.4945,  0.2362, -1.1227,  ...,  0.5529,  0.2086,  0.0088],\n",
       "         [ 0.4947,  0.2363, -1.1264,  ...,  0.5528,  0.2094,  0.0088],\n",
       "         [ 0.4951,  0.2364, -1.1371,  ...,  0.5531,  0.2099,  0.0089],\n",
       "         ...,\n",
       "         [ 0.4933,  0.2480, -1.2424,  ...,  0.5563,  0.2184,  0.0091],\n",
       "         [ 0.4933,  0.2480, -1.2443,  ...,  0.5565,  0.2185,  0.0093],\n",
       "         [ 0.4932,  0.2480, -1.2944,  ...,  0.5564,  0.2185,  0.0095]]),\n",
       " tensor([[ 0.5038,  0.2425, -1.1550,  ...,  0.5665,  0.2136,  0.0080],\n",
       "         [ 0.5027,  0.2423, -1.2423,  ...,  0.5653,  0.2133,  0.0092],\n",
       "         [ 0.5018,  0.2423, -1.2565,  ...,  0.5649,  0.2130,  0.0094],\n",
       "         ...,\n",
       "         [ 0.4950,  0.2412, -1.1727,  ...,  0.5624,  0.2175,  0.0087],\n",
       "         [ 0.4951,  0.2412, -1.1690,  ...,  0.5625,  0.2177,  0.0082],\n",
       "         [ 0.4951,  0.2412, -1.1910,  ...,  0.5626,  0.2180,  0.0079]]),\n",
       " tensor([[ 0.4828,  0.2604, -1.2334,  ...,  0.5483,  0.2258,  0.0073],\n",
       "         [ 0.4816,  0.2606, -1.4555,  ...,  0.5474,  0.2267,  0.0087],\n",
       "         [ 0.4809,  0.2608, -1.4653,  ...,  0.5475,  0.2266,  0.0088],\n",
       "         ...,\n",
       "         [ 0.4832,  0.2490, -1.4275,  ...,  0.5557,  0.2156,  0.0123],\n",
       "         [ 0.4845,  0.2491, -1.4277,  ...,  0.5566,  0.2158,  0.0124],\n",
       "         [ 0.4855,  0.2492, -1.4367,  ...,  0.5573,  0.2163,  0.0122]])]"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the sequences\n",
    "import torch\n",
    "sequences = load_keypoint_sequences(file_paths)\n",
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([21, 160, 1662])\n"
     ]
    }
   ],
   "source": [
    "# Pad the sequences to the same length\n",
    "padded_sequences = pad_sequence(sequences, batch_first=True)\n",
    "pad_sequence\n",
    "print(padded_sequences.shape) # (batch_size, max_sequence_length, num_keypoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['กฎกระทรวง',\n",
       " 'กฎหมายรัฐธรรมนูญ',\n",
       " 'กรมอนามัย',\n",
       " 'กรรม',\n",
       " 'กรรมสิทธิ์',\n",
       " 'กระโดด',\n",
       " 'กล้วยบวชชี',\n",
       " 'กล้วยเชื่อม',\n",
       " 'เขิน',\n",
       " 'เขื่อนดิน',\n",
       " 'เขื่อนสิริกิติ์',\n",
       " 'เข้าใจผิด',\n",
       " 'เคย',\n",
       " 'เครียด',\n",
       " 'เครื่องปั่นดิน',\n",
       " 'เครื่องหมายการค้า',\n",
       " 'เจอ',\n",
       " 'เจ้าหนี้',\n",
       " 'เช่าซื้อ',\n",
       " 'เช่าทรัพย์',\n",
       " 'โจทก์']"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = [action.split(\".\")[0] for action in actions]\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20], dtype=int64)"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "labels = le.fit_transform(labels)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Create a custom dataset\n",
    "class KeypointDataset(Dataset):\n",
    "    def __init__(self, file_paths, labels):\n",
    "        self.file_paths = file_paths\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        keypoints = np.load(self.file_paths[idx])\n",
    "        label = self.labels[idx]\n",
    "        return torch.tensor(keypoints, dtype=torch.float32), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset\n",
    "dataset = KeypointDataset(file_paths, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Data for different actions/กฎกระทรวง.mp4/กฎกระทรวง.npy', 'Data for different actions/กฎหมายรัฐธรรมนูญ.mp4/กฎหมายรัฐธรรมนูญ.npy', 'Data for different actions/กรมอนามัย.mp4/กรมอนามัย.npy', 'Data for different actions/กรรม.mp4/กรรม.npy', 'Data for different actions/กรรมสิทธิ์.mp4/กรรมสิทธิ์.npy', 'Data for different actions/กระโดด.mp4/กระโดด.npy', 'Data for different actions/กล้วยบวชชี.mp4/กล้วยบวชชี.npy', 'Data for different actions/กล้วยเชื่อม.mp4/กล้วยเชื่อม.npy', 'Data for different actions/เขิน.mp4/เขิน.npy', 'Data for different actions/เขื่อนดิน.mp4/เขื่อนดิน.npy', 'Data for different actions/เขื่อนสิริกิติ์.mp4/เขื่อนสิริกิติ์.npy', 'Data for different actions/เข้าใจผิด.mp4/เข้าใจผิด.npy', 'Data for different actions/เคย.mp4/เคย.npy', 'Data for different actions/เครียด.mp4/เครียด.npy', 'Data for different actions/เครื่องปั่นดิน.mp4/เครื่องปั่นดิน.npy', 'Data for different actions/เครื่องหมายการค้า.mp4/เครื่องหมายการค้า.npy', 'Data for different actions/เจอ.mp4/เจอ.npy', 'Data for different actions/เจ้าหนี้.mp4/เจ้าหนี้.npy', 'Data for different actions/เช่าซื้อ.mp4/เช่าซื้อ.npy', 'Data for different actions/เช่าทรัพย์.mp4/เช่าทรัพย์.npy', 'Data for different actions/โจทก์.mp4/โจทก์.npy']\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20]\n"
     ]
    }
   ],
   "source": [
    "print(dataset.file_paths)\n",
    "print(dataset.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collate function for padding\n",
    "def collate_fn(batch):\n",
    "    sequences, labels = zip(*batch)\n",
    "    padded_sequences = pad_sequence(sequences, batch_first=True)\n",
    "    return padded_sequences, torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x22e6482e930>"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the DataLoader\n",
    "batch_size = 4\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        # Forward propagate the LSTM\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        # Use the last time step's output for classification\n",
    "        out = out[:, -1, :]\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set device (use GPU if available)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model, loss function, and optimizer\n",
    "model = LSTMModel(input_size=1662, hidden_size=128, num_layers=2, num_classes=21).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()  # For multi-class classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/300], Loss: 3.0771\n",
      "Epoch [2/300], Loss: 3.3151\n",
      "Epoch [3/300], Loss: 3.3754\n",
      "Epoch [4/300], Loss: 3.4476\n",
      "Epoch [5/300], Loss: 3.1946\n",
      "Epoch [6/300], Loss: 3.5937\n",
      "Epoch [7/300], Loss: 3.7086\n",
      "Epoch [8/300], Loss: 3.6550\n",
      "Epoch [9/300], Loss: 3.8139\n",
      "Epoch [10/300], Loss: 3.6697\n",
      "Epoch [11/300], Loss: 3.2579\n",
      "Epoch [12/300], Loss: 3.5836\n",
      "Epoch [13/300], Loss: 3.2831\n",
      "Epoch [14/300], Loss: 3.5168\n",
      "Epoch [15/300], Loss: 3.3247\n",
      "Epoch [16/300], Loss: 3.4316\n",
      "Epoch [17/300], Loss: 1.4669\n",
      "Epoch [18/300], Loss: 3.1141\n",
      "Epoch [19/300], Loss: 3.3620\n",
      "Epoch [20/300], Loss: 0.9862\n",
      "Epoch [21/300], Loss: 3.3801\n",
      "Epoch [22/300], Loss: 2.8123\n",
      "Epoch [23/300], Loss: 3.5468\n",
      "Epoch [24/300], Loss: 3.2391\n",
      "Epoch [25/300], Loss: 3.2749\n",
      "Epoch [26/300], Loss: 0.9542\n",
      "Epoch [27/300], Loss: 3.0362\n",
      "Epoch [28/300], Loss: 1.3972\n",
      "Epoch [29/300], Loss: 3.0906\n",
      "Epoch [30/300], Loss: 1.5403\n",
      "Epoch [31/300], Loss: 3.1929\n",
      "Epoch [32/300], Loss: 2.8340\n",
      "Epoch [33/300], Loss: 2.5325\n",
      "Epoch [34/300], Loss: 2.5039\n",
      "Epoch [35/300], Loss: 3.7402\n",
      "Epoch [36/300], Loss: 1.1321\n",
      "Epoch [37/300], Loss: 4.4052\n",
      "Epoch [38/300], Loss: 2.0557\n",
      "Epoch [39/300], Loss: 3.3436\n",
      "Epoch [40/300], Loss: 3.9001\n",
      "Epoch [41/300], Loss: 1.3540\n",
      "Epoch [42/300], Loss: 3.6032\n",
      "Epoch [43/300], Loss: 3.1374\n",
      "Epoch [44/300], Loss: 3.0030\n",
      "Epoch [45/300], Loss: 0.9493\n",
      "Epoch [46/300], Loss: 1.2296\n",
      "Epoch [47/300], Loss: 2.4657\n",
      "Epoch [48/300], Loss: 2.1329\n",
      "Epoch [49/300], Loss: 1.3958\n",
      "Epoch [50/300], Loss: 1.7010\n",
      "Epoch [51/300], Loss: 2.4247\n",
      "Epoch [52/300], Loss: 4.5295\n",
      "Epoch [53/300], Loss: 2.0440\n",
      "Epoch [54/300], Loss: 1.7957\n",
      "Epoch [55/300], Loss: 4.6934\n",
      "Epoch [56/300], Loss: 4.0275\n",
      "Epoch [57/300], Loss: 2.5361\n",
      "Epoch [58/300], Loss: 1.3897\n",
      "Epoch [59/300], Loss: 3.6114\n",
      "Epoch [60/300], Loss: 0.2047\n",
      "Epoch [61/300], Loss: 3.2760\n",
      "Epoch [62/300], Loss: 2.9796\n",
      "Epoch [63/300], Loss: 2.4702\n",
      "Epoch [64/300], Loss: 3.5494\n",
      "Epoch [65/300], Loss: 3.5686\n",
      "Epoch [66/300], Loss: 1.8355\n",
      "Epoch [67/300], Loss: 1.7450\n",
      "Epoch [68/300], Loss: 0.8355\n",
      "Epoch [69/300], Loss: 4.4012\n",
      "Epoch [70/300], Loss: 2.6710\n",
      "Epoch [71/300], Loss: 1.9381\n",
      "Epoch [72/300], Loss: 0.8853\n",
      "Epoch [73/300], Loss: 1.2147\n",
      "Epoch [74/300], Loss: 6.0293\n",
      "Epoch [75/300], Loss: 4.9648\n",
      "Epoch [76/300], Loss: 3.3663\n",
      "Epoch [77/300], Loss: 4.1915\n",
      "Epoch [78/300], Loss: 1.5598\n",
      "Epoch [79/300], Loss: 0.9445\n",
      "Epoch [80/300], Loss: 1.6276\n",
      "Epoch [81/300], Loss: 0.8529\n",
      "Epoch [82/300], Loss: 2.5951\n",
      "Epoch [83/300], Loss: 2.6178\n",
      "Epoch [84/300], Loss: 3.1183\n",
      "Epoch [85/300], Loss: 3.1856\n",
      "Epoch [86/300], Loss: 1.2708\n",
      "Epoch [87/300], Loss: 0.1999\n",
      "Epoch [88/300], Loss: 4.0960\n",
      "Epoch [89/300], Loss: 2.0729\n",
      "Epoch [90/300], Loss: 2.3454\n",
      "Epoch [91/300], Loss: 1.7050\n",
      "Epoch [92/300], Loss: 2.1584\n",
      "Epoch [93/300], Loss: 3.2794\n",
      "Epoch [94/300], Loss: 0.4439\n",
      "Epoch [95/300], Loss: 3.9999\n",
      "Epoch [96/300], Loss: 2.0479\n",
      "Epoch [97/300], Loss: 2.1490\n",
      "Epoch [98/300], Loss: 2.6473\n",
      "Epoch [99/300], Loss: 1.7066\n",
      "Epoch [100/300], Loss: 3.1658\n",
      "Epoch [101/300], Loss: 4.6496\n",
      "Epoch [102/300], Loss: 1.3834\n",
      "Epoch [103/300], Loss: 1.9505\n",
      "Epoch [104/300], Loss: 1.8890\n",
      "Epoch [105/300], Loss: 1.1231\n",
      "Epoch [106/300], Loss: 4.4588\n",
      "Epoch [107/300], Loss: 3.9304\n",
      "Epoch [108/300], Loss: 3.6817\n",
      "Epoch [109/300], Loss: 2.9343\n",
      "Epoch [110/300], Loss: 0.7565\n",
      "Epoch [111/300], Loss: 0.6273\n",
      "Epoch [112/300], Loss: 0.4458\n",
      "Epoch [113/300], Loss: 3.0508\n",
      "Epoch [114/300], Loss: 0.9071\n",
      "Epoch [115/300], Loss: 3.0118\n",
      "Epoch [116/300], Loss: 3.9398\n",
      "Epoch [117/300], Loss: 3.8583\n",
      "Epoch [118/300], Loss: 0.8730\n",
      "Epoch [119/300], Loss: 2.0004\n",
      "Epoch [120/300], Loss: 0.6304\n",
      "Epoch [121/300], Loss: 3.4528\n",
      "Epoch [122/300], Loss: 2.7238\n",
      "Epoch [123/300], Loss: 2.5543\n",
      "Epoch [124/300], Loss: 0.0608\n",
      "Epoch [125/300], Loss: 3.2160\n",
      "Epoch [126/300], Loss: 1.7020\n",
      "Epoch [127/300], Loss: 2.0639\n",
      "Epoch [128/300], Loss: 1.4822\n",
      "Epoch [129/300], Loss: 3.1431\n",
      "Epoch [130/300], Loss: 2.1438\n",
      "Epoch [131/300], Loss: 1.3285\n",
      "Epoch [132/300], Loss: 2.5409\n",
      "Epoch [133/300], Loss: 1.6786\n",
      "Epoch [134/300], Loss: 0.7572\n",
      "Epoch [135/300], Loss: 2.8460\n",
      "Epoch [136/300], Loss: 3.6221\n",
      "Epoch [137/300], Loss: 1.3948\n",
      "Epoch [138/300], Loss: 1.0343\n",
      "Epoch [139/300], Loss: 2.0301\n",
      "Epoch [140/300], Loss: 3.7687\n",
      "Epoch [141/300], Loss: 1.6833\n",
      "Epoch [142/300], Loss: 0.5828\n",
      "Epoch [143/300], Loss: 0.0424\n",
      "Epoch [144/300], Loss: 1.2792\n",
      "Epoch [145/300], Loss: 3.2796\n",
      "Epoch [146/300], Loss: 1.6637\n",
      "Epoch [147/300], Loss: 1.0321\n",
      "Epoch [148/300], Loss: 1.1104\n",
      "Epoch [149/300], Loss: 2.6932\n",
      "Epoch [150/300], Loss: 0.5791\n",
      "Epoch [151/300], Loss: 3.2345\n",
      "Epoch [152/300], Loss: 3.6916\n",
      "Epoch [153/300], Loss: 2.1440\n",
      "Epoch [154/300], Loss: 1.7831\n",
      "Epoch [155/300], Loss: 1.3969\n",
      "Epoch [156/300], Loss: 0.0396\n",
      "Epoch [157/300], Loss: 0.5756\n",
      "Epoch [158/300], Loss: 2.0478\n",
      "Epoch [159/300], Loss: 2.8066\n",
      "Epoch [160/300], Loss: 2.4324\n",
      "Epoch [161/300], Loss: 2.9326\n",
      "Epoch [162/300], Loss: 1.6325\n",
      "Epoch [163/300], Loss: 2.0210\n",
      "Epoch [164/300], Loss: 1.7195\n",
      "Epoch [165/300], Loss: 3.0146\n",
      "Epoch [166/300], Loss: 3.0432\n",
      "Epoch [167/300], Loss: 2.5129\n",
      "Epoch [168/300], Loss: 2.5087\n",
      "Epoch [169/300], Loss: 2.4241\n",
      "Epoch [170/300], Loss: 1.1665\n",
      "Epoch [171/300], Loss: 2.0967\n",
      "Epoch [172/300], Loss: 2.7239\n",
      "Epoch [173/300], Loss: 1.6683\n",
      "Epoch [174/300], Loss: 3.1500\n",
      "Epoch [175/300], Loss: 1.3151\n",
      "Epoch [176/300], Loss: 2.1208\n",
      "Epoch [177/300], Loss: 2.6959\n",
      "Epoch [178/300], Loss: 2.2557\n",
      "Epoch [179/300], Loss: 0.6642\n",
      "Epoch [180/300], Loss: 2.3611\n",
      "Epoch [181/300], Loss: 1.9368\n",
      "Epoch [182/300], Loss: 2.0457\n",
      "Epoch [183/300], Loss: 1.4137\n",
      "Epoch [184/300], Loss: 2.8030\n",
      "Epoch [185/300], Loss: 0.8104\n",
      "Epoch [186/300], Loss: 3.2988\n",
      "Epoch [187/300], Loss: 2.9159\n",
      "Epoch [188/300], Loss: 0.8472\n",
      "Epoch [189/300], Loss: 1.9961\n",
      "Epoch [190/300], Loss: 2.5693\n",
      "Epoch [191/300], Loss: 1.3782\n",
      "Epoch [192/300], Loss: 2.3618\n",
      "Epoch [193/300], Loss: 1.3508\n",
      "Epoch [194/300], Loss: 1.1458\n",
      "Epoch [195/300], Loss: 0.7743\n",
      "Epoch [196/300], Loss: 3.6215\n",
      "Epoch [197/300], Loss: 4.7502\n",
      "Epoch [198/300], Loss: 2.2219\n",
      "Epoch [199/300], Loss: 1.3955\n",
      "Epoch [200/300], Loss: 0.8253\n",
      "Epoch [201/300], Loss: 2.4063\n",
      "Epoch [202/300], Loss: 2.2145\n",
      "Epoch [203/300], Loss: 3.0302\n",
      "Epoch [204/300], Loss: 0.8544\n",
      "Epoch [205/300], Loss: 2.2100\n",
      "Epoch [206/300], Loss: 1.5589\n",
      "Epoch [207/300], Loss: 2.0776\n",
      "Epoch [208/300], Loss: 1.2859\n",
      "Epoch [209/300], Loss: 0.9943\n",
      "Epoch [210/300], Loss: 3.3222\n",
      "Epoch [211/300], Loss: 0.6647\n",
      "Epoch [212/300], Loss: 0.9215\n",
      "Epoch [213/300], Loss: 2.9599\n",
      "Epoch [214/300], Loss: 1.0124\n",
      "Epoch [215/300], Loss: 0.9771\n",
      "Epoch [216/300], Loss: 1.0967\n",
      "Epoch [217/300], Loss: 0.8599\n",
      "Epoch [218/300], Loss: 0.6854\n",
      "Epoch [219/300], Loss: 0.5962\n",
      "Epoch [220/300], Loss: 3.0694\n",
      "Epoch [221/300], Loss: 0.5772\n",
      "Epoch [222/300], Loss: 2.3956\n",
      "Epoch [223/300], Loss: 1.0812\n",
      "Epoch [224/300], Loss: 0.5616\n",
      "Epoch [225/300], Loss: 2.9212\n",
      "Epoch [226/300], Loss: 2.8368\n",
      "Epoch [227/300], Loss: 2.6070\n",
      "Epoch [228/300], Loss: 0.9932\n",
      "Epoch [229/300], Loss: 2.2976\n",
      "Epoch [230/300], Loss: 0.6305\n",
      "Epoch [231/300], Loss: 1.0416\n",
      "Epoch [232/300], Loss: 1.0057\n",
      "Epoch [233/300], Loss: 2.3386\n",
      "Epoch [234/300], Loss: 0.0656\n",
      "Epoch [235/300], Loss: 2.7573\n",
      "Epoch [236/300], Loss: 0.3677\n",
      "Epoch [237/300], Loss: 3.7922\n",
      "Epoch [238/300], Loss: 2.3570\n",
      "Epoch [239/300], Loss: 0.2273\n",
      "Epoch [240/300], Loss: 0.3127\n",
      "Epoch [241/300], Loss: 0.5722\n",
      "Epoch [242/300], Loss: 0.7044\n",
      "Epoch [243/300], Loss: 3.2367\n",
      "Epoch [244/300], Loss: 0.5653\n",
      "Epoch [245/300], Loss: 2.8010\n",
      "Epoch [246/300], Loss: 2.5053\n",
      "Epoch [247/300], Loss: 0.4402\n",
      "Epoch [248/300], Loss: 2.6827\n",
      "Epoch [249/300], Loss: 3.5244\n",
      "Epoch [250/300], Loss: 1.3052\n",
      "Epoch [251/300], Loss: 3.2482\n",
      "Epoch [252/300], Loss: 2.4121\n",
      "Epoch [253/300], Loss: 2.6620\n",
      "Epoch [254/300], Loss: 2.0397\n",
      "Epoch [255/300], Loss: 0.6542\n",
      "Epoch [256/300], Loss: 2.6206\n",
      "Epoch [257/300], Loss: 1.9568\n",
      "Epoch [258/300], Loss: 1.2736\n",
      "Epoch [259/300], Loss: 1.0891\n",
      "Epoch [260/300], Loss: 0.5927\n",
      "Epoch [261/300], Loss: 0.0428\n",
      "Epoch [262/300], Loss: 0.3618\n",
      "Epoch [263/300], Loss: 0.5582\n",
      "Epoch [264/300], Loss: 0.8031\n",
      "Epoch [265/300], Loss: 0.2388\n",
      "Epoch [266/300], Loss: 0.3044\n",
      "Epoch [267/300], Loss: 0.1698\n",
      "Epoch [268/300], Loss: 3.8757\n",
      "Epoch [269/300], Loss: 3.2603\n",
      "Epoch [270/300], Loss: 2.7055\n",
      "Epoch [271/300], Loss: 0.0420\n",
      "Epoch [272/300], Loss: 0.1664\n",
      "Epoch [273/300], Loss: 2.3038\n",
      "Epoch [274/300], Loss: 2.0597\n",
      "Epoch [275/300], Loss: 0.7708\n",
      "Epoch [276/300], Loss: 2.2926\n",
      "Epoch [277/300], Loss: 0.0200\n",
      "Epoch [278/300], Loss: 1.0474\n",
      "Epoch [279/300], Loss: 0.0187\n",
      "Epoch [280/300], Loss: 0.4506\n",
      "Epoch [281/300], Loss: 1.4370\n",
      "Epoch [282/300], Loss: 0.8910\n",
      "Epoch [283/300], Loss: 0.7577\n",
      "Epoch [284/300], Loss: 2.6828\n",
      "Epoch [285/300], Loss: 1.9692\n",
      "Epoch [286/300], Loss: 1.9545\n",
      "Epoch [287/300], Loss: 0.2540\n",
      "Epoch [288/300], Loss: 0.6899\n",
      "Epoch [289/300], Loss: 0.6485\n",
      "Epoch [290/300], Loss: 0.3704\n",
      "Epoch [291/300], Loss: 1.2480\n",
      "Epoch [292/300], Loss: 0.0472\n",
      "Epoch [293/300], Loss: 3.2871\n",
      "Epoch [294/300], Loss: 0.0143\n",
      "Epoch [295/300], Loss: 0.7309\n",
      "Epoch [296/300], Loss: 2.0605\n",
      "Epoch [297/300], Loss: 1.2866\n",
      "Epoch [298/300], Loss: 1.1554\n",
      "Epoch [299/300], Loss: 1.5433\n",
      "Epoch [300/300], Loss: 1.7564\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 300\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for i, (sequences, labels) in enumerate(data_loader):\n",
    "        # Move data to the device\n",
    "        sequences = sequences.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(sequences)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.4922,  0.2382, -1.2850,  ...,  0.5578,  0.2124,  0.0094],\n",
       "         [ 0.4920,  0.2405, -1.4288,  ...,  0.5571,  0.2116,  0.0099],\n",
       "         [ 0.4920,  0.2409, -1.4093,  ...,  0.5567,  0.2122,  0.0098],\n",
       "         ...,\n",
       "         [ 0.4814,  0.2260, -1.3318,  ...,  0.5503,  0.1923,  0.0123],\n",
       "         [ 0.4815,  0.2257, -1.3351,  ...,  0.5503,  0.1921,  0.0122],\n",
       "         [ 0.4815,  0.2255, -1.3497,  ...,  0.5501,  0.1919,  0.0124]]])"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_sequence(sequences, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4.5624,  0.7731, -1.6153, -2.3025, -3.1507,  0.1362, -3.7282,  0.6829,\n",
       "         -2.6290, -1.6117, -3.3710, -2.5167, -2.1433,  3.0694, -2.7982, -0.6838,\n",
       "         -0.1604,  3.6548, -0.0283,  2.6791,  2.9549],\n",
       "        [-1.6816,  5.0027, -2.4236, -1.5547, -4.9270,  2.0412, -6.1473,  3.3839,\n",
       "         -1.5182, -1.0241, -3.2922, -1.4616, -0.2065,  2.6639, -4.4712,  1.2923,\n",
       "          1.1402,  2.6263,  1.5590, -1.0941,  0.4534],\n",
       "        [-1.4900,  0.2579,  6.3374,  2.1396, -3.5190, -1.9748, -1.7491,  0.6330,\n",
       "          1.5378, -2.4932, -0.8810,  3.0957, -2.5056, -0.6579, -3.3464,  2.3156,\n",
       "         -1.5508, -1.5359,  0.1149, -1.3826, -1.7413],\n",
       "        [-4.7755, -3.3709, -0.0426,  5.7503, -2.4058,  1.0891, -3.0297, -1.0608,\n",
       "          5.8777, -3.7441, -1.9000,  5.7858,  1.2945, -1.7067, -2.2699,  1.4590,\n",
       "          1.2112, -2.0696, -1.0037, -0.7496, -1.3273],\n",
       "        [-0.7007, -0.8241, -1.6710, -1.2503,  6.1529, -2.3028,  1.0994, -0.4096,\n",
       "         -2.0354, -0.7682,  1.7312, -1.1438, -2.2746, -0.7845,  6.0864, -0.3998,\n",
       "         -3.0495, -0.7288, -0.4228, -0.5606, -1.4405],\n",
       "        [-3.5052, -1.4607, -3.3096,  0.6968, -3.2983,  4.4394, -3.5108,  0.5077,\n",
       "          1.0084, -2.5167, -2.2540,  0.7745,  4.2622,  1.9294, -3.0040, -1.9268,\n",
       "          4.7040,  1.9921, -4.0343, -0.0587,  1.7931],\n",
       "        [ 1.4417, -1.0976,  0.0228, -1.6417,  0.6214, -1.9801,  6.8123, -1.2407,\n",
       "         -2.1561, -0.1194, -0.1998, -1.8058, -1.8652, -0.0513,  0.8348, -0.7941,\n",
       "         -1.8308,  0.2528, -2.0915,  2.8372,  0.0653],\n",
       "        [-3.7709,  3.5616, -2.3773, -0.5595, -4.2323,  2.2138, -5.8599,  4.8095,\n",
       "         -0.6355, -1.6174, -3.0133, -0.3185,  1.0709,  2.2296, -3.5448,  1.9475,\n",
       "          1.8281,  2.2484, -0.8733, -1.4717,  0.3592],\n",
       "        [-4.7744, -3.3711, -0.0436,  5.7507, -2.4023,  1.0903, -3.0266, -1.0621,\n",
       "          5.8785, -3.7428, -1.8961,  5.7842,  1.2955, -1.7071, -2.2682,  1.4571,\n",
       "          1.2126, -2.0707, -1.0037, -0.7492, -1.3251],\n",
       "        [ 0.2959,  1.7774, -1.7278, -2.4803, -2.8422,  1.2946, -2.5118,  1.3598,\n",
       "         -2.5495,  4.4233, -2.1399, -2.9157,  0.7953,  2.0639, -3.3475, -1.4227,\n",
       "          1.6741,  3.3674, -1.3066, -1.0553, -0.0847],\n",
       "        [-1.9974,  0.7011,  0.0292, -0.4424,  0.6112, -1.5380, -0.9144,  0.8727,\n",
       "         -0.1591, -0.4307,  7.1907,  0.1751, -1.1608, -0.6922,  0.4200,  0.5420,\n",
       "         -1.3946, -1.0190,  0.5368, -3.0369, -2.6996],\n",
       "        [-4.8140, -3.3567, -0.0117,  5.7351, -2.5000,  1.0449, -3.1124, -1.0169,\n",
       "          5.8535, -3.7854, -2.0119,  5.8345,  1.2512, -1.6909, -2.3173,  1.5215,\n",
       "          1.1617, -2.0345, -0.9939, -0.7612, -1.3948],\n",
       "        [-3.4155, -1.6409, -3.4220,  0.8604, -3.1915,  4.5162, -3.3324,  0.3278,\n",
       "          1.1652, -2.4110, -2.0637,  0.8642,  4.6535,  1.5703, -2.8556, -2.1488,\n",
       "          4.8602,  1.7191, -4.0978, -0.0979,  1.6803],\n",
       "        [-2.8082, -0.1325, -4.6176, -1.2244, -3.8981,  2.2679, -4.5843,  1.7235,\n",
       "         -1.1195, -1.9570, -4.0719, -1.3798,  0.7554,  4.6093, -3.7100, -0.6529,\n",
       "          1.8286,  4.6314, -3.7293,  2.0718,  2.8424],\n",
       "        [-0.7051, -0.8208, -1.6638, -1.2496,  6.1382, -2.3044,  1.1263, -0.4133,\n",
       "         -2.0288, -0.7691,  1.7475, -1.1409, -2.2789, -0.7817,  6.0827, -0.4081,\n",
       "         -3.0521, -0.7266, -0.4228, -0.5533, -1.4401],\n",
       "        [-4.5205,  1.4547, -1.4575,  1.7540, -3.5610, -0.0684, -4.3100,  3.0426,\n",
       "          1.5689, -2.7882, -2.6711,  2.2536, -0.9925, -0.7570, -3.1990,  6.9126,\n",
       "         -0.5362, -0.9101,  1.2361, -1.6898, -1.5689],\n",
       "        [-3.4165, -1.5225, -3.3707,  0.6646, -3.2928,  4.4561, -3.4463,  0.4248,\n",
       "          0.9730, -2.4565, -2.3181,  0.6996,  4.3182,  1.9231, -3.0102, -1.9845,\n",
       "          4.7779,  1.9914, -4.0859,  0.0238,  1.8699],\n",
       "        [-2.8553, -0.2384, -4.6495, -1.1896, -3.8848,  2.2325, -4.3489,  1.6392,\n",
       "         -1.0361, -2.0353, -3.9606, -1.3224,  0.7397,  4.5984, -3.7663, -0.7185,\n",
       "          1.8736,  4.6361, -3.6870,  2.1126,  2.8318],\n",
       "        [-0.6381,  1.2811, -0.0624,  0.6533, -3.2274, -1.0160, -3.8264,  1.1877,\n",
       "          0.8941, -3.3587, -0.9958,  1.8662, -3.1778, -0.3918, -3.0349,  4.9386,\n",
       "         -1.4430, -1.0210,  5.7926, -2.0078, -1.6962],\n",
       "        [-0.0930, -2.0448, -3.4220, -0.8876, -3.4499,  2.1485, -2.0037, -0.3304,\n",
       "         -0.7687, -1.7009, -4.5858, -1.5727,  1.2944,  3.7050, -3.4145, -2.0596,\n",
       "          2.6551,  3.5760, -4.8049,  6.6479,  6.0745],\n",
       "        [-2.2857, -1.1049, -4.5859, -1.3433, -3.8363,  2.5778, -3.3518,  0.8742,\n",
       "         -0.9647, -2.0686, -4.4289, -1.2297,  1.1181,  4.3745, -3.9082, -0.8861,\n",
       "          2.3894,  4.3991, -3.6828,  3.5326,  3.8698]])"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Put the model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# No need to track gradients during inference\n",
    "with torch.no_grad():\n",
    "    # Get the model's output (logits)\n",
    "    outputs = model(padded_sequences)\n",
    "\n",
    "# outputs = torch.softmax(outputs, dim=1)\n",
    "# outputs = torch.max(outputs,1)\n",
    "\n",
    "outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.5013,  0.2452, -1.2167,  ...,  0.5663,  0.2188,  0.0098],\n",
       "         [ 0.4997,  0.2482, -1.4690,  ...,  0.5652,  0.2181,  0.0106],\n",
       "         [ 0.4984,  0.2500, -1.4853,  ...,  0.5654,  0.2185,  0.0112],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 0.4922,  0.2382, -1.2850,  ...,  0.5578,  0.2124,  0.0094],\n",
       "         [ 0.4920,  0.2405, -1.4288,  ...,  0.5571,  0.2116,  0.0099],\n",
       "         [ 0.4920,  0.2409, -1.4093,  ...,  0.5567,  0.2122,  0.0098],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 0.5049,  0.2371, -1.2115,  ...,  0.5643,  0.2082,  0.0088],\n",
       "         [ 0.5045,  0.2381, -1.1896,  ...,  0.5643,  0.2081,  0.0085],\n",
       "         [ 0.5041,  0.2385, -1.1915,  ...,  0.5643,  0.2080,  0.0089],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.4945,  0.2362, -1.1227,  ...,  0.5529,  0.2086,  0.0088],\n",
       "         [ 0.4947,  0.2363, -1.1264,  ...,  0.5528,  0.2094,  0.0088],\n",
       "         [ 0.4951,  0.2364, -1.1371,  ...,  0.5531,  0.2099,  0.0089],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 0.5038,  0.2425, -1.1550,  ...,  0.5665,  0.2136,  0.0080],\n",
       "         [ 0.5027,  0.2423, -1.2423,  ...,  0.5653,  0.2133,  0.0092],\n",
       "         [ 0.5018,  0.2423, -1.2565,  ...,  0.5649,  0.2130,  0.0094],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 0.4828,  0.2604, -1.2334,  ...,  0.5483,  0.2258,  0.0073],\n",
       "         [ 0.4816,  0.2606, -1.4555,  ...,  0.5474,  0.2267,  0.0087],\n",
       "         [ 0.4809,  0.2608, -1.4653,  ...,  0.5475,  0.2266,  0.0088],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]])"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(padded_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = [\"Data for different actions/กล้วยบวชชี_2.mp4/กล้วยบวชชี_2.npy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.5319,  0.2821, -1.6797,  ...,  0.6169,  0.2514,  0.0109],\n",
       "         [ 0.5346,  0.2813, -1.7538,  ...,  0.6164,  0.2506,  0.0119],\n",
       "         [ 0.5365,  0.2812, -1.7556,  ...,  0.6163,  0.2507,  0.0109],\n",
       "         ...,\n",
       "         [ 0.5340,  0.2852, -1.2054,  ...,  0.6196,  0.2422,  0.0160],\n",
       "         [ 0.5345,  0.2806, -1.4284,  ...,  0.6202,  0.2413,  0.0160],\n",
       "         [ 0.5356,  0.2758, -1.3837,  ...,  0.6207,  0.2407,  0.0163]]])"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the sequences\n",
    "import torch\n",
    "sequences = load_keypoint_sequences(file_paths)\n",
    "# Change list to numpy array \n",
    "sequences = np.array(sequences)\n",
    "# Change numpy array to tensor\n",
    "sequences = torch.FloatTensor(sequences)\n",
    "sequences = pad_sequence(sequences, batch_first=True)\n",
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.7387, -1.2895, -0.3022, -1.4025, -1.3495, -0.6996,  4.4123, -1.6544,\n",
       "         -1.6072, -0.0755, -1.7109, -1.6890, -1.0813,  1.1921, -1.0088, -1.3502,\n",
       "         -0.4197,  1.3098, -2.9438,  4.7029,  1.8431]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model(sequences)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['กฎกระทรวง',\n",
       " 'กฎหมายรัฐธรรมนูญ',\n",
       " 'กรมอนามัย',\n",
       " 'กรรม',\n",
       " 'กรรมสิทธิ์',\n",
       " 'กระโดด',\n",
       " 'กล้วยบวชชี',\n",
       " 'กล้วยเชื่อม',\n",
       " 'เขิน',\n",
       " 'เขื่อนดิน',\n",
       " 'เขื่อนสิริกิติ์',\n",
       " 'เข้าใจผิด',\n",
       " 'เคย',\n",
       " 'เครียด',\n",
       " 'เครื่องปั่นดิน',\n",
       " 'เครื่องหมายการค้า',\n",
       " 'เจอ',\n",
       " 'เจ้าหนี้',\n",
       " 'เช่าซื้อ',\n",
       " 'เช่าทรัพย์',\n",
       " 'โจทก์']"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = [action.split(\".\")[0] for action in actions]\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -------------------------------------------------------------------------------------------------------------------------------------------- ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "# Load the sequences\n",
    "import numpy as np \n",
    "import os\n",
    "Model_Data=os.path.join('Data for different actions')\n",
    "action = \"กฎหมายรัฐธรรมนูญ.mp4\"\n",
    "a = np.load(os.path.join(Model_Data, action, \"กฎหมายรัฐธรรมนูญ.npy\"))\n",
    "a = torch.from_numpy(a)\n",
    "\n",
    "# Pad the sequences to the same length\n",
    "padded_sequences = pad_sequence(a, batch_first=True)\n",
    "len(padded_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 803,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 804,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelMap = {label:num for num, label in enumerate(actions)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 806,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs, labels = [], []\n",
    "\n",
    "for action in actions:\n",
    "    for seq in range(no_of_seqs):\n",
    "        window = []\n",
    "        for frame_num in range(seq_length):\n",
    "            res = np.load(os.path.join(Model_Data, action, f\"frame_{frame_num}.npy\")) \n",
    "            window.append(res)\n",
    "        seqs.append(window)\n",
    "\n",
    "        labels.append(labelMap[action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(seqs).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 809,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = np.array(seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing the labels from 0,1,2 to categorical data for easier accessebility\n",
    "Y_label = to_categorical(labels).astype(int)\n",
    "Y_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_data, Y_label, test_size=0.3)\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 815,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 816,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding the logs folder\n",
    "log_dir = os.path.join('Logs')\n",
    "tb_callback = TensorBoard(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural network\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, return_sequences=True, activation='relu', input_shape=(30,1662)))\n",
    "model.add(LSTM(128, return_sequences=True, activation='relu'))\n",
    "model.add(LSTM(64, return_sequences=False, activation='relu'))\n",
    "\n",
    "# adding 64 units for dense layer\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(actions.shape[0], activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eg\n",
    "eg_res = [.7, 0.2, 0.1]\n",
    "actions[np.argmax(eg_res)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 819,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, Y_train, epochs=300, callbacks=[tb_callback])\n",
    "# tensorboard --logdir=."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res=model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# again the actions with the max value provided by softmax is returned\n",
    "actions[np.argmax(res[4])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions[np.argmax(Y_test[4])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 836,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_hat = model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 838,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_true = np.argmax(Y_train, axis=1).tolist()\n",
    "# one hot encoding\n",
    "Y_hat = np.argmax(Y_hat, axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confution matrix\n",
    "multilabel_confusion_matrix(Y_true, Y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(Y_true, Y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
