{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holist = mp.solutions.holistic \n",
    "mp_draw = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(img, model):\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img.flags.writeable = False                 \n",
    "    result = model.process(img)                 # Make prediction\n",
    "    img.flags.writeable = True                   \n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR) \n",
    "    return img, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks(img, result):\n",
    "    mp_draw.draw_landmarks(img, result.face_landmarks, mp_holist.FACEMESH_CONTOURS) # Draw face connections\n",
    "    mp_draw.draw_landmarks(img, result.pose_landmarks, mp_holist.POSE_CONNECTIONS) # Draw pose connections\n",
    "    mp_draw.draw_landmarks(img, result.left_hand_landmarks, mp_holist.HAND_CONNECTIONS) # Draw left hand connections\n",
    "    mp_draw.draw_landmarks(img, result.right_hand_landmarks, mp_holist.HAND_CONNECTIONS) # Draw right hand connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_styled_landmarks(img, result):\n",
    "    mp_draw.draw_landmarks(img, result.face_landmarks, mp_holist.FACEMESH_CONTOURS, \n",
    "                             mp_draw.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1), # color the joint \n",
    "                             mp_draw.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1) #color the connection\n",
    "                             ) \n",
    "    # mp_draw.draw_landmarks(img, result.face_landmarks, mp_holist.FACEMESH_CONTOURS, \n",
    "    #                          mp_draw.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1), # color the joint \n",
    "    #                          mp_draw.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1) #color the connection\n",
    "    #                          ) \n",
    "    \n",
    "    mp_draw.draw_landmarks(img, result.pose_landmarks, mp_holist.POSE_CONNECTIONS,\n",
    "                             mp_draw.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4), \n",
    "                             mp_draw.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    mp_draw.draw_landmarks(img, result.left_hand_landmarks, mp_holist.HAND_CONNECTIONS, \n",
    "                             mp_draw.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4), \n",
    "                             mp_draw.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    mp_draw.draw_landmarks(img, result.right_hand_landmarks, mp_holist.HAND_CONNECTIONS, \n",
    "                             mp_draw.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4), \n",
    "                             mp_draw.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                             ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "frozenset({(0, 1),\n",
       "           (0, 4),\n",
       "           (1, 2),\n",
       "           (2, 3),\n",
       "           (3, 7),\n",
       "           (4, 5),\n",
       "           (5, 6),\n",
       "           (6, 8),\n",
       "           (9, 10),\n",
       "           (11, 12),\n",
       "           (11, 13),\n",
       "           (11, 23),\n",
       "           (12, 14),\n",
       "           (12, 24),\n",
       "           (13, 15),\n",
       "           (14, 16),\n",
       "           (15, 17),\n",
       "           (15, 19),\n",
       "           (15, 21),\n",
       "           (16, 18),\n",
       "           (16, 20),\n",
       "           (16, 22),\n",
       "           (17, 19),\n",
       "           (18, 20),\n",
       "           (23, 24),\n",
       "           (23, 25),\n",
       "           (24, 26),\n",
       "           (25, 27),\n",
       "           (26, 28),\n",
       "           (27, 29),\n",
       "           (27, 31),\n",
       "           (28, 30),\n",
       "           (28, 32),\n",
       "           (29, 31),\n",
       "           (30, 32)})"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mp_holist.POSE_CONNECTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "# Set mediapipe model \n",
    "with mp_holist.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        print(results)\n",
    "        \n",
    "        draw_styled_landmarks(image, results)\n",
    "\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9997581839561462"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.pose_landmarks.landmark[0].visibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results.pose_landmarks.landmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(results):\n",
    "    pose=np.array([[res.x,res.y,res.z,res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    left_hnd=np.array([[res.x,res.y,res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    right_hnd=np.array([[res.x,res.y,res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    face=np.array([[res.x,res.y,res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "    return np.concatenate([pose,left_hnd,right_hnd,face])\n",
    "# concatenating for the model to detect the sign language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1662,)"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_keypoints(results).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "video_dir = \"C:/Users/araya/Desktop/keypoints/video_extract\"\n",
    "video_list = []\n",
    "video_list = os.listdir(video_dir)\n",
    "\n",
    "len(video_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['กฎกระทรวง.mp4',\n",
       " 'กฎหมายรัฐธรรมนูญ.mp4',\n",
       " 'กรมอนามัย.mp4',\n",
       " 'กรรม.mp4',\n",
       " 'กรรมสิทธิ์.mp4',\n",
       " 'กระโดด.mp4',\n",
       " 'กล้วยบวชชี.mp4',\n",
       " 'กล้วยเชื่อม.mp4']"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for exported data, numpy arrays\n",
    "Model_Data=os.path.join('Data for different actions')\n",
    "\n",
    "actions = np.array(video_list)\n",
    "\n",
    "no_of_seqs = 1\n",
    "\n",
    "# 30 frames in length\n",
    "seq_length = 160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['กฎกระทรวง.mp4', 'กฎหมายรัฐธรรมนูญ.mp4', 'กรมอนามัย.mp4',\n",
       "       'กรรม.mp4', 'กรรมสิทธิ์.mp4', 'กระโดด.mp4', 'กล้วยบวชชี.mp4',\n",
       "       'กล้วยเชื่อม.mp4'], dtype='<U20')"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just creating the folders and sub folders\n",
    "\n",
    "for action in actions: \n",
    "    try: \n",
    "        os.makedirs(os.path.join(Model_Data, action))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# augment_dir = \"C:/Users/araya/Desktop/augments\"\n",
    "\n",
    "# augment_list = []\n",
    "# augment_list = os.listdir(augment_dir)\n",
    "# augment_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actions = list(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x in augment_list:\n",
    "#     # print(x)\n",
    "#     actions.append(x)\n",
    "# actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actions = np.array(actions)\n",
    "# actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collecting keypoint values for Training nd Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory where your videos are stored\n",
    "directory = \"C:/Users/araya/Desktop/keypoints/video_extract\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:/Users/araya/Desktop/keypoints/video_extract'"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'my name is Peter', 'I am 26 years old']\n"
     ]
    }
   ],
   "source": [
    "txt = \"hello, my name is Peter, I am 26 years old\"\n",
    "\n",
    "x = txt.split(\", \")\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/araya/Desktop/keypoints/video_extract/กฎกระทรวง.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/กฎหมายรัฐธรรมนูญ.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/กรมอนามัย.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/กรรม.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/กรรมสิทธิ์.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/กระโดด.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/กล้วยบวชชี.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/กล้วยเชื่อม.mp4\n"
     ]
    }
   ],
   "source": [
    "for filename in actions:\n",
    "    print(directory + '/' + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set mediapipe model \n",
    "# for action in actions:\n",
    "#     video_path = os.path.join(\"C:/Users/araya/Desktop/keypoints/video_extract\", action)\n",
    "#     cap = cv2.VideoCapture(video_path)\n",
    "#     cap.set(cv2.CAP_PROP_FPS, 60)\n",
    "#     length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "#     print(\"LENGTH:\" + str(length))\n",
    "#     # keypoints = []\n",
    "\n",
    "#     if not cap.isOpened():\n",
    "#         print(f\"Error opening video file: {video_path}\")\n",
    "#         continue\n",
    "\n",
    "#     with mp_holist.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "#         for seq in range(no_of_seqs):\n",
    "#             for frame_num in range(seq_length):\n",
    "\n",
    "#                 ret, frame = cap.read()\n",
    "#                 if not ret:\n",
    "#                     print(f\"End of video {video_path}\")\n",
    "#                     break\n",
    "                \n",
    "#                 img, results = mediapipe_detection(frame, holistic)\n",
    "#                 draw_styled_landmarks(img, results)\n",
    "\n",
    "#                 # print(frame_num)\n",
    "\n",
    "#                 if frame_num == 0: \n",
    "#                     cv2.putText(img, 'DATA COLLECTION STARTED', (120,200), \n",
    "#                                 cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255, 0), 4, cv2.LINE_AA)\n",
    "#                     cv2.putText(img, f'Collecting frames for - {action} Sequence Number - {seq}', (15,12), \n",
    "#                                 cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "#                     cv2.imshow('OpenCV Window', img)\n",
    "#                     cv2.waitKey(2000)  # 2 seconds delay for setup\n",
    "#                 else: \n",
    "#                     cv2.putText(img, f'Collecting frames for - {action} Sequence Number - {seq}', (15,12), \n",
    "#                                 cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "#                     cv2.imshow('OpenCV Window', img)\n",
    "\n",
    "#                 keypoints = extract_keypoints(results)\n",
    "#                 # keypoints.append(results)\n",
    "#                 npy_path = os.path.join(Model_Data, action, f\"frame_{frame_num}.npy\")\n",
    "#                 os.makedirs(os.path.dirname(npy_path), exist_ok=True)\n",
    "#                 np.save(npy_path, keypoints)\n",
    "\n",
    "#                 if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#                     break\n",
    "\n",
    "#             if not ret:\n",
    "#                 break\n",
    "\n",
    "#     cap.release()\n",
    "#     cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np \n",
    "\n",
    "# X = [[1,2,3]]\n",
    "# X.append([6,8,10])\n",
    "# X.append([20,9,4])\n",
    "# X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LENGTH:142\n",
      "End of video C:/Users/araya/Desktop/keypoints/video_extract\\กฎกระทรวง.mp4\n",
      "LENGTH:134\n",
      "End of video C:/Users/araya/Desktop/keypoints/video_extract\\กฎหมายรัฐธรรมนูญ.mp4\n",
      "LENGTH:151\n",
      "End of video C:/Users/araya/Desktop/keypoints/video_extract\\กรมอนามัย.mp4\n",
      "LENGTH:100\n",
      "End of video C:/Users/araya/Desktop/keypoints/video_extract\\กรรม.mp4\n",
      "LENGTH:184\n",
      "LENGTH:94\n",
      "End of video C:/Users/araya/Desktop/keypoints/video_extract\\กระโดด.mp4\n",
      "LENGTH:182\n",
      "LENGTH:128\n",
      "End of video C:/Users/araya/Desktop/keypoints/video_extract\\กล้วยเชื่อม.mp4\n"
     ]
    }
   ],
   "source": [
    "# Set mediapipe model \n",
    "for action in actions:\n",
    "    video_path = os.path.join(\"C:/Users/araya/Desktop/keypoints/video_extract\", action)\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    cap.set(cv2.CAP_PROP_FPS, 60)\n",
    "    length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    print(\"LENGTH:\" + str(length))\n",
    "    keypoints = []\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error opening video file: {video_path}\")\n",
    "        continue\n",
    "\n",
    "    with mp_holist.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "        for seq in range(no_of_seqs):\n",
    "            for frame_num in range(seq_length):\n",
    "\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    print(f\"End of video {video_path}\")\n",
    "                    break\n",
    "                \n",
    "                img, results = mediapipe_detection(frame, holistic)\n",
    "                draw_styled_landmarks(img, results)\n",
    "\n",
    "                # print(frame_num)\n",
    "\n",
    "                if frame_num == 0: \n",
    "                    cv2.putText(img, 'DATA COLLECTION STARTED', (120,200), \n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255, 0), 4, cv2.LINE_AA)\n",
    "                    cv2.putText(img, f'Collecting frames for - {action} Sequence Number - {seq}', (15,12), \n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                    cv2.imshow('OpenCV Window', img)\n",
    "                    cv2.waitKey(2000)  # 2 seconds delay for setup\n",
    "                else: \n",
    "                    cv2.putText(img, f'Collecting frames for - {action} Sequence Number - {seq}', (15,12), \n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                    cv2.imshow('OpenCV Window', img)\n",
    "\n",
    "                x = extract_keypoints(results)\n",
    "                keypoints.append(x)\n",
    "                npy_path = os.path.join(Model_Data, action, f\"{action.split(\".\")[0]}.npy\")\n",
    "                os.makedirs(os.path.dirname(npy_path), exist_ok=True)\n",
    "                np.save(npy_path, keypoints)\n",
    "\n",
    "                if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                    break\n",
    "\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Data for different actions/กฎกระทรวง.mp4/กฎกระทรวง.npy', 'Data for different actions/กฎหมายรัฐธรรมนูญ.mp4/กฎหมายรัฐธรรมนูญ.npy', 'Data for different actions/กรมอนามัย.mp4/กรมอนามัย.npy', 'Data for different actions/กรรม.mp4/กรรม.npy', 'Data for different actions/กรรมสิทธิ์.mp4/กรรมสิทธิ์.npy', 'Data for different actions/กระโดด.mp4/กระโดด.npy', 'Data for different actions/กล้วยบวชชี.mp4/กล้วยบวชชี.npy', 'Data for different actions/กล้วยเชื่อม.mp4/กล้วยเชื่อม.npy']\n"
     ]
    }
   ],
   "source": [
    "file_paths = []\n",
    "for action in actions:\n",
    "    video_path = os.path.join('Data for different actions/', action)\n",
    "    # print(video_path)\n",
    "    # print(action)\n",
    "    file_paths.append(video_path + '/' + action.split(\".\")[0] + \".npy\")\n",
    "print(file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_keypoint_sequences(file_paths):\n",
    "    keypoint_sequences = []\n",
    "    for file_path in file_paths:\n",
    "        keypoints = np.load(file_path)\n",
    "        keypoint_sequences.append(torch.tensor(keypoints, dtype=torch.float32))\n",
    "    return keypoint_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 0.5013,  0.2452, -1.2167,  ...,  0.5663,  0.2188,  0.0098],\n",
       "         [ 0.4997,  0.2482, -1.4690,  ...,  0.5652,  0.2181,  0.0106],\n",
       "         [ 0.4984,  0.2500, -1.4853,  ...,  0.5654,  0.2185,  0.0112],\n",
       "         ...,\n",
       "         [ 0.4861,  0.2513, -1.3416,  ...,  0.5572,  0.2177,  0.0091],\n",
       "         [ 0.4873,  0.2514, -1.3574,  ...,  0.5575,  0.2172,  0.0097],\n",
       "         [ 0.4883,  0.2516, -1.3579,  ...,  0.5577,  0.2170,  0.0101]]),\n",
       " tensor([[ 0.4922,  0.2382, -1.2850,  ...,  0.5578,  0.2124,  0.0094],\n",
       "         [ 0.4920,  0.2405, -1.4288,  ...,  0.5571,  0.2116,  0.0099],\n",
       "         [ 0.4920,  0.2409, -1.4093,  ...,  0.5567,  0.2122,  0.0098],\n",
       "         ...,\n",
       "         [ 0.4814,  0.2260, -1.3318,  ...,  0.5503,  0.1923,  0.0123],\n",
       "         [ 0.4815,  0.2257, -1.3351,  ...,  0.5503,  0.1921,  0.0122],\n",
       "         [ 0.4815,  0.2255, -1.3497,  ...,  0.5501,  0.1919,  0.0124]]),\n",
       " tensor([[ 0.5049,  0.2371, -1.2115,  ...,  0.5643,  0.2082,  0.0088],\n",
       "         [ 0.5045,  0.2381, -1.1896,  ...,  0.5643,  0.2081,  0.0085],\n",
       "         [ 0.5041,  0.2385, -1.1915,  ...,  0.5643,  0.2080,  0.0089],\n",
       "         ...,\n",
       "         [ 0.4963,  0.2307, -1.3096,  ...,  0.5624,  0.2043,  0.0127],\n",
       "         [ 0.4965,  0.2308, -1.3085,  ...,  0.5625,  0.2045,  0.0126],\n",
       "         [ 0.4972,  0.2312, -1.3053,  ...,  0.5633,  0.2045,  0.0128]]),\n",
       " tensor([[ 0.5134,  0.2614, -1.4426,  ...,  0.5818,  0.2272,  0.0153],\n",
       "         [ 0.5130,  0.2604, -1.4262,  ...,  0.5810,  0.2273,  0.0147],\n",
       "         [ 0.5126,  0.2599, -1.4278,  ...,  0.5810,  0.2276,  0.0150],\n",
       "         ...,\n",
       "         [ 0.5079,  0.2693, -1.4999,  ...,  0.5780,  0.2349,  0.0115],\n",
       "         [ 0.5090,  0.2688, -1.4936,  ...,  0.5782,  0.2346,  0.0116],\n",
       "         [ 0.5092,  0.2683, -1.4518,  ...,  0.5786,  0.2341,  0.0116]]),\n",
       " tensor([[ 0.4883,  0.2402, -1.1024,  ...,  0.5482,  0.2132,  0.0081],\n",
       "         [ 0.4878,  0.2402, -1.1906,  ...,  0.5469,  0.2135,  0.0083],\n",
       "         [ 0.4863,  0.2402, -1.1774,  ...,  0.5477,  0.2141,  0.0087],\n",
       "         ...,\n",
       "         [ 0.4788,  0.3129, -1.6072,  ...,  0.5349,  0.2531,  0.0022],\n",
       "         [ 0.4782,  0.3129, -1.6350,  ...,  0.5344,  0.2525,  0.0022],\n",
       "         [ 0.4771,  0.3131, -1.6312,  ...,  0.5339,  0.2515,  0.0022]]),\n",
       " tensor([[ 0.4992,  0.1994, -1.1906,  ...,  0.5657,  0.1731,  0.0116],\n",
       "         [ 0.4988,  0.2047, -1.3590,  ...,  0.5663,  0.1723,  0.0123],\n",
       "         [ 0.4982,  0.2082, -1.3140,  ...,  0.5671,  0.1730,  0.0129],\n",
       "         ...,\n",
       "         [ 0.4743,  0.1974, -1.3230,  ...,  0.5475,  0.1620,  0.0155],\n",
       "         [ 0.4732,  0.1974, -1.3174,  ...,  0.5461,  0.1615,  0.0151],\n",
       "         [ 0.4720,  0.1974, -1.3149,  ...,  0.5453,  0.1614,  0.0149]]),\n",
       " tensor([[ 0.5023,  0.2809, -1.6242,  ...,  0.5847,  0.2321,  0.0112],\n",
       "         [ 0.5023,  0.2806, -1.6631,  ...,  0.5840,  0.2322,  0.0130],\n",
       "         [ 0.5022,  0.2805, -1.6912,  ...,  0.5837,  0.2322,  0.0122],\n",
       "         ...,\n",
       "         [ 0.5037,  0.2791, -1.5789,  ...,  0.5834,  0.2224,  0.0129],\n",
       "         [ 0.5044,  0.2774, -1.5686,  ...,  0.5831,  0.2220,  0.0130],\n",
       "         [ 0.5052,  0.2722, -1.5527,  ...,  0.5827,  0.2216,  0.0132]]),\n",
       " tensor([[ 0.4868,  0.2821, -1.4668,  ...,  0.5711,  0.2335,  0.0116],\n",
       "         [ 0.4861,  0.2786, -1.5812,  ...,  0.5702,  0.2334,  0.0129],\n",
       "         [ 0.4856,  0.2769, -1.6059,  ...,  0.5699,  0.2333,  0.0131],\n",
       "         ...,\n",
       "         [ 0.4826,  0.2583, -1.6140,  ...,  0.5606,  0.2261,  0.0154],\n",
       "         [ 0.4820,  0.2583, -1.5231,  ...,  0.5603,  0.2261,  0.0156],\n",
       "         [ 0.4818,  0.2580, -1.5255,  ...,  0.5600,  0.2263,  0.0156]])]"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the sequences\n",
    "import torch\n",
    "sequences = load_keypoint_sequences(file_paths)\n",
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 160, 1662])\n"
     ]
    }
   ],
   "source": [
    "# Pad the sequences to the same length\n",
    "padded_sequences = pad_sequence(sequences, batch_first=True)\n",
    "pad_sequence\n",
    "print(padded_sequences.shape) # (batch_size, max_sequence_length, num_keypoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['กฎกระทรวง',\n",
       " 'กฎหมายรัฐธรรมนูญ',\n",
       " 'กรมอนามัย',\n",
       " 'กรรม',\n",
       " 'กรรมสิทธิ์',\n",
       " 'กระโดด',\n",
       " 'กล้วยบวชชี',\n",
       " 'กล้วยเชื่อม']"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = [action.split(\".\")[0] for action in actions]\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7], dtype=int64)"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "labels = le.fit_transform(labels)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Create a custom dataset\n",
    "class KeypointDataset(Dataset):\n",
    "    def __init__(self, file_paths, labels):\n",
    "        self.file_paths = file_paths\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        keypoints = np.load(self.file_paths[idx])\n",
    "        label = self.labels[idx]\n",
    "        return torch.tensor(keypoints, dtype=torch.float32), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset\n",
    "dataset = KeypointDataset(file_paths, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Data for different actions/กฎกระทรวง.mp4/กฎกระทรวง.npy', 'Data for different actions/กฎหมายรัฐธรรมนูญ.mp4/กฎหมายรัฐธรรมนูญ.npy', 'Data for different actions/กรมอนามัย.mp4/กรมอนามัย.npy', 'Data for different actions/กรรม.mp4/กรรม.npy', 'Data for different actions/กรรมสิทธิ์.mp4/กรรมสิทธิ์.npy', 'Data for different actions/กระโดด.mp4/กระโดด.npy', 'Data for different actions/กล้วยบวชชี.mp4/กล้วยบวชชี.npy', 'Data for different actions/กล้วยเชื่อม.mp4/กล้วยเชื่อม.npy']\n",
      "[0 1 2 3 4 5 6 7]\n"
     ]
    }
   ],
   "source": [
    "print(dataset.file_paths)\n",
    "print(dataset.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collate function for padding\n",
    "def collate_fn(batch):\n",
    "    sequences, labels = zip(*batch)\n",
    "    padded_sequences = pad_sequence(sequences, batch_first=True)\n",
    "    return padded_sequences, torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x153f7437230>"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the DataLoader\n",
    "batch_size = 4\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        # Forward propagate the LSTM\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        # Use the last time step's output for classification\n",
    "        out = out[:, -1, :]\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set device (use GPU if available)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model, loss function, and optimizer\n",
    "model = LSTMModel(input_size=1662, hidden_size=128, num_layers=2, num_classes=8).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()  # For multi-class classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/300], Loss: 2.1132\n",
      "Epoch [2/300], Loss: 2.0554\n",
      "Epoch [3/300], Loss: 2.1685\n",
      "Epoch [4/300], Loss: 1.8254\n",
      "Epoch [5/300], Loss: 1.7872\n",
      "Epoch [6/300], Loss: 1.8781\n",
      "Epoch [7/300], Loss: 2.1101\n",
      "Epoch [8/300], Loss: 1.5984\n",
      "Epoch [9/300], Loss: 1.7760\n",
      "Epoch [10/300], Loss: 1.7647\n",
      "Epoch [11/300], Loss: 1.6601\n",
      "Epoch [12/300], Loss: 1.7151\n",
      "Epoch [13/300], Loss: 1.1476\n",
      "Epoch [14/300], Loss: 1.5082\n",
      "Epoch [15/300], Loss: 1.3512\n",
      "Epoch [16/300], Loss: 1.2460\n",
      "Epoch [17/300], Loss: 1.4524\n",
      "Epoch [18/300], Loss: 2.2085\n",
      "Epoch [19/300], Loss: 1.1737\n",
      "Epoch [20/300], Loss: 1.6004\n",
      "Epoch [21/300], Loss: 0.8288\n",
      "Epoch [22/300], Loss: 1.1285\n",
      "Epoch [23/300], Loss: 1.3481\n",
      "Epoch [24/300], Loss: 1.0385\n",
      "Epoch [25/300], Loss: 2.4781\n",
      "Epoch [26/300], Loss: 1.1170\n",
      "Epoch [27/300], Loss: 0.8672\n",
      "Epoch [28/300], Loss: 1.5650\n",
      "Epoch [29/300], Loss: 0.9237\n",
      "Epoch [30/300], Loss: 0.7731\n",
      "Epoch [31/300], Loss: 1.4146\n",
      "Epoch [32/300], Loss: 0.5361\n",
      "Epoch [33/300], Loss: 0.7450\n",
      "Epoch [34/300], Loss: 0.6195\n",
      "Epoch [35/300], Loss: 0.7954\n",
      "Epoch [36/300], Loss: 0.6008\n",
      "Epoch [37/300], Loss: 0.5750\n",
      "Epoch [38/300], Loss: 0.8338\n",
      "Epoch [39/300], Loss: 0.4684\n",
      "Epoch [40/300], Loss: 0.5374\n",
      "Epoch [41/300], Loss: 2.3115\n",
      "Epoch [42/300], Loss: 0.6422\n",
      "Epoch [43/300], Loss: 1.0641\n",
      "Epoch [44/300], Loss: 1.1077\n",
      "Epoch [45/300], Loss: 0.5688\n",
      "Epoch [46/300], Loss: 0.6543\n",
      "Epoch [47/300], Loss: 0.4422\n",
      "Epoch [48/300], Loss: 0.9963\n",
      "Epoch [49/300], Loss: 1.9342\n",
      "Epoch [50/300], Loss: 1.4404\n",
      "Epoch [51/300], Loss: 0.6629\n",
      "Epoch [52/300], Loss: 0.9194\n",
      "Epoch [53/300], Loss: 0.6547\n",
      "Epoch [54/300], Loss: 1.0342\n",
      "Epoch [55/300], Loss: 0.3867\n",
      "Epoch [56/300], Loss: 0.6481\n",
      "Epoch [57/300], Loss: 0.9356\n",
      "Epoch [58/300], Loss: 1.1459\n",
      "Epoch [59/300], Loss: 0.4484\n",
      "Epoch [60/300], Loss: 0.5187\n",
      "Epoch [61/300], Loss: 0.4870\n",
      "Epoch [62/300], Loss: 0.4934\n",
      "Epoch [63/300], Loss: 0.4147\n",
      "Epoch [64/300], Loss: 0.3272\n",
      "Epoch [65/300], Loss: 0.3151\n",
      "Epoch [66/300], Loss: 1.2015\n",
      "Epoch [67/300], Loss: 0.2205\n",
      "Epoch [68/300], Loss: 0.3736\n",
      "Epoch [69/300], Loss: 0.2847\n",
      "Epoch [70/300], Loss: 0.3056\n",
      "Epoch [71/300], Loss: 0.3892\n",
      "Epoch [72/300], Loss: 1.1168\n",
      "Epoch [73/300], Loss: 1.0230\n",
      "Epoch [74/300], Loss: 0.4865\n",
      "Epoch [75/300], Loss: 0.3371\n",
      "Epoch [76/300], Loss: 0.9126\n",
      "Epoch [77/300], Loss: 0.3136\n",
      "Epoch [78/300], Loss: 1.6876\n",
      "Epoch [79/300], Loss: 0.3251\n",
      "Epoch [80/300], Loss: 0.4589\n",
      "Epoch [81/300], Loss: 0.4277\n",
      "Epoch [82/300], Loss: 0.3828\n",
      "Epoch [83/300], Loss: 0.8967\n",
      "Epoch [84/300], Loss: 0.3215\n",
      "Epoch [85/300], Loss: 0.2851\n",
      "Epoch [86/300], Loss: 0.1606\n",
      "Epoch [87/300], Loss: 0.4170\n",
      "Epoch [88/300], Loss: 0.4097\n",
      "Epoch [89/300], Loss: 0.4493\n",
      "Epoch [90/300], Loss: 0.3004\n",
      "Epoch [91/300], Loss: 1.2587\n",
      "Epoch [92/300], Loss: 0.4377\n",
      "Epoch [93/300], Loss: 0.3374\n",
      "Epoch [94/300], Loss: 2.4082\n",
      "Epoch [95/300], Loss: 0.7920\n",
      "Epoch [96/300], Loss: 0.3596\n",
      "Epoch [97/300], Loss: 0.4781\n",
      "Epoch [98/300], Loss: 0.3560\n",
      "Epoch [99/300], Loss: 0.1218\n",
      "Epoch [100/300], Loss: 0.2785\n",
      "Epoch [101/300], Loss: 1.0402\n",
      "Epoch [102/300], Loss: 0.1694\n",
      "Epoch [103/300], Loss: 1.4593\n",
      "Epoch [104/300], Loss: 0.3186\n",
      "Epoch [105/300], Loss: 0.7745\n",
      "Epoch [106/300], Loss: 0.7839\n",
      "Epoch [107/300], Loss: 0.3081\n",
      "Epoch [108/300], Loss: 0.3266\n",
      "Epoch [109/300], Loss: 0.1636\n",
      "Epoch [110/300], Loss: 0.1802\n",
      "Epoch [111/300], Loss: 1.4726\n",
      "Epoch [112/300], Loss: 0.3298\n",
      "Epoch [113/300], Loss: 0.1434\n",
      "Epoch [114/300], Loss: 0.6393\n",
      "Epoch [115/300], Loss: 0.8127\n",
      "Epoch [116/300], Loss: 0.3499\n",
      "Epoch [117/300], Loss: 0.4781\n",
      "Epoch [118/300], Loss: 1.7233\n",
      "Epoch [119/300], Loss: 1.3080\n",
      "Epoch [120/300], Loss: 0.6945\n",
      "Epoch [121/300], Loss: 0.4642\n",
      "Epoch [122/300], Loss: 0.3961\n",
      "Epoch [123/300], Loss: 0.4357\n",
      "Epoch [124/300], Loss: 0.4880\n",
      "Epoch [125/300], Loss: 0.2060\n",
      "Epoch [126/300], Loss: 0.1567\n",
      "Epoch [127/300], Loss: 0.1354\n",
      "Epoch [128/300], Loss: 1.1555\n",
      "Epoch [129/300], Loss: 0.4342\n",
      "Epoch [130/300], Loss: 0.2198\n",
      "Epoch [131/300], Loss: 1.1794\n",
      "Epoch [132/300], Loss: 0.2966\n",
      "Epoch [133/300], Loss: 0.1462\n",
      "Epoch [134/300], Loss: 0.4647\n",
      "Epoch [135/300], Loss: 0.2992\n",
      "Epoch [136/300], Loss: 0.8743\n",
      "Epoch [137/300], Loss: 0.2330\n",
      "Epoch [138/300], Loss: 0.4472\n",
      "Epoch [139/300], Loss: 2.2840\n",
      "Epoch [140/300], Loss: 0.6272\n",
      "Epoch [141/300], Loss: 0.4314\n",
      "Epoch [142/300], Loss: 0.4138\n",
      "Epoch [143/300], Loss: 0.2554\n",
      "Epoch [144/300], Loss: 0.1250\n",
      "Epoch [145/300], Loss: 0.4049\n",
      "Epoch [146/300], Loss: 1.3552\n",
      "Epoch [147/300], Loss: 0.8429\n",
      "Epoch [148/300], Loss: 0.3044\n",
      "Epoch [149/300], Loss: 0.2347\n",
      "Epoch [150/300], Loss: 0.4134\n",
      "Epoch [151/300], Loss: 0.0728\n",
      "Epoch [152/300], Loss: 0.2363\n",
      "Epoch [153/300], Loss: 0.2440\n",
      "Epoch [154/300], Loss: 0.3181\n",
      "Epoch [155/300], Loss: 0.4077\n",
      "Epoch [156/300], Loss: 0.0852\n",
      "Epoch [157/300], Loss: 0.3661\n",
      "Epoch [158/300], Loss: 0.1263\n",
      "Epoch [159/300], Loss: 1.0956\n",
      "Epoch [160/300], Loss: 0.2982\n",
      "Epoch [161/300], Loss: 0.2203\n",
      "Epoch [162/300], Loss: 0.0444\n",
      "Epoch [163/300], Loss: 0.1048\n",
      "Epoch [164/300], Loss: 0.4258\n",
      "Epoch [165/300], Loss: 1.2841\n",
      "Epoch [166/300], Loss: 0.7988\n",
      "Epoch [167/300], Loss: 0.3355\n",
      "Epoch [168/300], Loss: 0.1010\n",
      "Epoch [169/300], Loss: 0.1422\n",
      "Epoch [170/300], Loss: 0.2749\n",
      "Epoch [171/300], Loss: 0.7766\n",
      "Epoch [172/300], Loss: 0.3600\n",
      "Epoch [173/300], Loss: 0.1338\n",
      "Epoch [174/300], Loss: 0.9069\n",
      "Epoch [175/300], Loss: 0.3763\n",
      "Epoch [176/300], Loss: 0.2061\n",
      "Epoch [177/300], Loss: 0.7381\n",
      "Epoch [178/300], Loss: 0.1758\n",
      "Epoch [179/300], Loss: 0.0980\n",
      "Epoch [180/300], Loss: 0.3041\n",
      "Epoch [181/300], Loss: 0.2154\n",
      "Epoch [182/300], Loss: 0.3719\n",
      "Epoch [183/300], Loss: 0.2421\n",
      "Epoch [184/300], Loss: 0.1892\n",
      "Epoch [185/300], Loss: 0.4419\n",
      "Epoch [186/300], Loss: 0.2241\n",
      "Epoch [187/300], Loss: 1.3973\n",
      "Epoch [188/300], Loss: 0.1782\n",
      "Epoch [189/300], Loss: 0.8299\n",
      "Epoch [190/300], Loss: 1.5024\n",
      "Epoch [191/300], Loss: 1.0272\n",
      "Epoch [192/300], Loss: 0.7059\n",
      "Epoch [193/300], Loss: 0.5327\n",
      "Epoch [194/300], Loss: 0.3514\n",
      "Epoch [195/300], Loss: 0.5110\n",
      "Epoch [196/300], Loss: 0.2956\n",
      "Epoch [197/300], Loss: 0.6815\n",
      "Epoch [198/300], Loss: 0.3979\n",
      "Epoch [199/300], Loss: 1.7694\n",
      "Epoch [200/300], Loss: 0.2532\n",
      "Epoch [201/300], Loss: 0.3234\n",
      "Epoch [202/300], Loss: 0.2553\n",
      "Epoch [203/300], Loss: 0.2970\n",
      "Epoch [204/300], Loss: 1.0205\n",
      "Epoch [205/300], Loss: 1.1211\n",
      "Epoch [206/300], Loss: 0.2548\n",
      "Epoch [207/300], Loss: 0.4110\n",
      "Epoch [208/300], Loss: 0.3358\n",
      "Epoch [209/300], Loss: 0.6150\n",
      "Epoch [210/300], Loss: 0.2778\n",
      "Epoch [211/300], Loss: 0.1242\n",
      "Epoch [212/300], Loss: 0.1350\n",
      "Epoch [213/300], Loss: 1.0301\n",
      "Epoch [214/300], Loss: 0.0958\n",
      "Epoch [215/300], Loss: 0.2338\n",
      "Epoch [216/300], Loss: 0.2546\n",
      "Epoch [217/300], Loss: 0.5847\n",
      "Epoch [218/300], Loss: 1.6613\n",
      "Epoch [219/300], Loss: 0.2304\n",
      "Epoch [220/300], Loss: 0.2350\n",
      "Epoch [221/300], Loss: 0.3443\n",
      "Epoch [222/300], Loss: 0.2723\n",
      "Epoch [223/300], Loss: 0.2138\n",
      "Epoch [224/300], Loss: 0.4055\n",
      "Epoch [225/300], Loss: 0.3180\n",
      "Epoch [226/300], Loss: 0.1011\n",
      "Epoch [227/300], Loss: 0.4015\n",
      "Epoch [228/300], Loss: 0.1109\n",
      "Epoch [229/300], Loss: 0.2288\n",
      "Epoch [230/300], Loss: 0.5693\n",
      "Epoch [231/300], Loss: 0.2505\n",
      "Epoch [232/300], Loss: 0.2402\n",
      "Epoch [233/300], Loss: 0.3992\n",
      "Epoch [234/300], Loss: 0.9206\n",
      "Epoch [235/300], Loss: 0.2261\n",
      "Epoch [236/300], Loss: 1.0613\n",
      "Epoch [237/300], Loss: 0.4624\n",
      "Epoch [238/300], Loss: 0.2499\n",
      "Epoch [239/300], Loss: 0.0994\n",
      "Epoch [240/300], Loss: 1.1839\n",
      "Epoch [241/300], Loss: 0.2855\n",
      "Epoch [242/300], Loss: 0.2598\n",
      "Epoch [243/300], Loss: 0.1509\n",
      "Epoch [244/300], Loss: 0.3034\n",
      "Epoch [245/300], Loss: 0.2668\n",
      "Epoch [246/300], Loss: 0.2845\n",
      "Epoch [247/300], Loss: 0.2352\n",
      "Epoch [248/300], Loss: 0.2278\n",
      "Epoch [249/300], Loss: 0.0986\n",
      "Epoch [250/300], Loss: 0.3772\n",
      "Epoch [251/300], Loss: 0.2655\n",
      "Epoch [252/300], Loss: 0.9352\n",
      "Epoch [253/300], Loss: 0.5809\n",
      "Epoch [254/300], Loss: 0.2176\n",
      "Epoch [255/300], Loss: 1.1879\n",
      "Epoch [256/300], Loss: 0.7956\n",
      "Epoch [257/300], Loss: 0.9052\n",
      "Epoch [258/300], Loss: 0.2642\n",
      "Epoch [259/300], Loss: 0.2759\n",
      "Epoch [260/300], Loss: 0.1205\n",
      "Epoch [261/300], Loss: 0.2275\n",
      "Epoch [262/300], Loss: 0.2522\n",
      "Epoch [263/300], Loss: 0.2186\n",
      "Epoch [264/300], Loss: 0.4926\n",
      "Epoch [265/300], Loss: 0.3675\n",
      "Epoch [266/300], Loss: 0.0729\n",
      "Epoch [267/300], Loss: 0.2012\n",
      "Epoch [268/300], Loss: 0.2032\n",
      "Epoch [269/300], Loss: 0.1914\n",
      "Epoch [270/300], Loss: 0.2536\n",
      "Epoch [271/300], Loss: 0.7101\n",
      "Epoch [272/300], Loss: 0.2589\n",
      "Epoch [273/300], Loss: 1.3866\n",
      "Epoch [274/300], Loss: 0.2101\n",
      "Epoch [275/300], Loss: 0.8384\n",
      "Epoch [276/300], Loss: 0.3610\n",
      "Epoch [277/300], Loss: 0.2310\n",
      "Epoch [278/300], Loss: 0.2260\n",
      "Epoch [279/300], Loss: 0.2568\n",
      "Epoch [280/300], Loss: 0.2912\n",
      "Epoch [281/300], Loss: 0.2608\n",
      "Epoch [282/300], Loss: 0.2033\n",
      "Epoch [283/300], Loss: 0.2054\n",
      "Epoch [284/300], Loss: 0.3601\n",
      "Epoch [285/300], Loss: 0.2460\n",
      "Epoch [286/300], Loss: 1.1765\n",
      "Epoch [287/300], Loss: 0.2508\n",
      "Epoch [288/300], Loss: 0.1964\n",
      "Epoch [289/300], Loss: 0.2701\n",
      "Epoch [290/300], Loss: 0.2070\n",
      "Epoch [291/300], Loss: 0.2205\n",
      "Epoch [292/300], Loss: 0.9288\n",
      "Epoch [293/300], Loss: 0.2207\n",
      "Epoch [294/300], Loss: 0.2473\n",
      "Epoch [295/300], Loss: 0.0801\n",
      "Epoch [296/300], Loss: 0.3759\n",
      "Epoch [297/300], Loss: 0.3583\n",
      "Epoch [298/300], Loss: 0.0319\n",
      "Epoch [299/300], Loss: 0.0486\n",
      "Epoch [300/300], Loss: 0.1977\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 300\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for i, (sequences, labels) in enumerate(data_loader):\n",
    "        # Move data to the device\n",
    "        sequences = sequences.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(sequences)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.5134,  0.2614, -1.4426,  ...,  0.5818,  0.2272,  0.0153],\n",
       "         [ 0.5130,  0.2604, -1.4262,  ...,  0.5810,  0.2273,  0.0147],\n",
       "         [ 0.5126,  0.2599, -1.4278,  ...,  0.5810,  0.2276,  0.0150],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 0.4868,  0.2821, -1.4668,  ...,  0.5711,  0.2335,  0.0116],\n",
       "         [ 0.4861,  0.2786, -1.5812,  ...,  0.5702,  0.2334,  0.0129],\n",
       "         [ 0.4856,  0.2769, -1.6059,  ...,  0.5699,  0.2333,  0.0131],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 0.4883,  0.2402, -1.1024,  ...,  0.5482,  0.2132,  0.0081],\n",
       "         [ 0.4878,  0.2402, -1.1906,  ...,  0.5469,  0.2135,  0.0083],\n",
       "         [ 0.4863,  0.2402, -1.1774,  ...,  0.5477,  0.2141,  0.0087],\n",
       "         ...,\n",
       "         [ 0.4788,  0.3129, -1.6072,  ...,  0.5349,  0.2531,  0.0022],\n",
       "         [ 0.4782,  0.3129, -1.6350,  ...,  0.5344,  0.2525,  0.0022],\n",
       "         [ 0.4771,  0.3131, -1.6312,  ...,  0.5339,  0.2515,  0.0022]],\n",
       "\n",
       "        [[ 0.5049,  0.2371, -1.2115,  ...,  0.5643,  0.2082,  0.0088],\n",
       "         [ 0.5045,  0.2381, -1.1896,  ...,  0.5643,  0.2081,  0.0085],\n",
       "         [ 0.5041,  0.2385, -1.1915,  ...,  0.5643,  0.2080,  0.0089],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]])"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_sequence(sequences, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.1559,  2.6587, -1.5481, -3.4535, -4.0416, -4.0040, -2.6531,  1.7368],\n",
       "        [-0.0665,  5.8299, -1.7130, -1.8845, -3.0622, -2.6629, -4.2754,  3.1133],\n",
       "        [ 0.0817,  0.8608,  5.5420, -2.7566, -1.7972, -3.0823, -1.3752,  0.5680],\n",
       "        [-1.9050, -1.6504, -3.1102,  5.5015, -1.8252,  5.5709, -1.3097,  0.8515],\n",
       "        [-0.8221, -0.9136, -0.9300, -1.0733,  7.2498, -0.6583,  1.7050, -1.6706],\n",
       "        [-1.9049, -1.6505, -3.1102,  5.5016, -1.8250,  5.5710, -1.3096,  0.8513],\n",
       "        [ 0.9522, -0.7505, -1.5533, -1.9857,  0.0274, -1.6237,  5.7415, -2.2517],\n",
       "        [-1.0058,  1.1484, -2.6844,  1.2455, -2.8941, -0.0091, -3.3391,  5.6267]])"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Put the model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# No need to track gradients during inference\n",
    "with torch.no_grad():\n",
    "    # Get the model's output (logits)\n",
    "    outputs = model(padded_sequences)\n",
    "\n",
    "# outputs = torch.softmax(outputs, dim=1)\n",
    "# outputs = torch.max(outputs,1)\n",
    "\n",
    "outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.5013,  0.2452, -1.2167,  ...,  0.5663,  0.2188,  0.0098],\n",
       "         [ 0.4997,  0.2482, -1.4690,  ...,  0.5652,  0.2181,  0.0106],\n",
       "         [ 0.4984,  0.2500, -1.4853,  ...,  0.5654,  0.2185,  0.0112],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 0.4922,  0.2382, -1.2850,  ...,  0.5578,  0.2124,  0.0094],\n",
       "         [ 0.4920,  0.2405, -1.4288,  ...,  0.5571,  0.2116,  0.0099],\n",
       "         [ 0.4920,  0.2409, -1.4093,  ...,  0.5567,  0.2122,  0.0098],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 0.5049,  0.2371, -1.2115,  ...,  0.5643,  0.2082,  0.0088],\n",
       "         [ 0.5045,  0.2381, -1.1896,  ...,  0.5643,  0.2081,  0.0085],\n",
       "         [ 0.5041,  0.2385, -1.1915,  ...,  0.5643,  0.2080,  0.0089],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.4992,  0.1994, -1.1906,  ...,  0.5657,  0.1731,  0.0116],\n",
       "         [ 0.4988,  0.2047, -1.3590,  ...,  0.5663,  0.1723,  0.0123],\n",
       "         [ 0.4982,  0.2082, -1.3140,  ...,  0.5671,  0.1730,  0.0129],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 0.5023,  0.2809, -1.6242,  ...,  0.5847,  0.2321,  0.0112],\n",
       "         [ 0.5023,  0.2806, -1.6631,  ...,  0.5840,  0.2322,  0.0130],\n",
       "         [ 0.5022,  0.2805, -1.6912,  ...,  0.5837,  0.2322,  0.0122],\n",
       "         ...,\n",
       "         [ 0.5037,  0.2791, -1.5789,  ...,  0.5834,  0.2224,  0.0129],\n",
       "         [ 0.5044,  0.2774, -1.5686,  ...,  0.5831,  0.2220,  0.0130],\n",
       "         [ 0.5052,  0.2722, -1.5527,  ...,  0.5827,  0.2216,  0.0132]],\n",
       "\n",
       "        [[ 0.4868,  0.2821, -1.4668,  ...,  0.5711,  0.2335,  0.0116],\n",
       "         [ 0.4861,  0.2786, -1.5812,  ...,  0.5702,  0.2334,  0.0129],\n",
       "         [ 0.4856,  0.2769, -1.6059,  ...,  0.5699,  0.2333,  0.0131],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]])"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.1559,  2.6587, -1.5481, -3.4535, -4.0416, -4.0040, -2.6531,  1.7368],\n",
       "        [-0.0665,  5.8299, -1.7130, -1.8845, -3.0622, -2.6629, -4.2754,  3.1133],\n",
       "        [ 0.0817,  0.8608,  5.5420, -2.7566, -1.7972, -3.0823, -1.3752,  0.5680],\n",
       "        [-1.9050, -1.6504, -3.1102,  5.5015, -1.8252,  5.5709, -1.3097,  0.8515],\n",
       "        [-0.8221, -0.9136, -0.9300, -1.0733,  7.2498, -0.6583,  1.7050, -1.6706],\n",
       "        [-1.9049, -1.6505, -3.1102,  5.5016, -1.8250,  5.5710, -1.3096,  0.8513],\n",
       "        [ 0.9522, -0.7505, -1.5533, -1.9857,  0.0274, -1.6237,  5.7415, -2.2517],\n",
       "        [-1.0058,  1.1484, -2.6844,  1.2455, -2.8941, -0.0091, -3.3391,  5.6267]])"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Put the model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# No need to track gradients during inference\n",
    "with torch.no_grad():\n",
    "    # Get the model's output (logits)\n",
    "    outputs = model(padded_sequences)\n",
    "\n",
    "# outputs = torch.softmax(outputs, dim=1)\n",
    "# outputs = torch.max(outputs,1)\n",
    "\n",
    "outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(padded_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = [\"Data for different actions/กฎกระทรวง_0.mp4/กฎกระทรวง_0.npy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.4759,  0.2493, -1.3079,  ...,  0.5265,  0.2095,  0.0107],\n",
       "         [ 0.4728,  0.2493, -1.4649,  ...,  0.5274,  0.2096,  0.0116],\n",
       "         [ 0.4709,  0.2493, -1.4833,  ...,  0.5269,  0.2097,  0.0114],\n",
       "         ...,\n",
       "         [ 0.4805,  0.2502, -1.3315,  ...,  0.5333,  0.2057,  0.0135],\n",
       "         [ 0.4801,  0.2497, -1.3008,  ...,  0.5333,  0.2054,  0.0136],\n",
       "         [ 0.4799,  0.2497, -1.3287,  ...,  0.5327,  0.2052,  0.0132]]])"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the sequences\n",
    "import torch\n",
    "sequences = load_keypoint_sequences(file_paths)\n",
    "# Change list to numpy array \n",
    "sequences = np.array(sequences)\n",
    "# Change numpy array to tensor\n",
    "sequences = torch.FloatTensor(sequences)\n",
    "# sequences = pad_sequence(sequences, batch_first=True)\n",
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.3375,  1.6678, -0.5642, -3.2502, -3.0815, -3.7259, -0.0089,  0.6178]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model(sequences)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -------------------------------------------------------------------------------------------------------------------------------------------- ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "# Load the sequences\n",
    "import numpy as np \n",
    "import os\n",
    "Model_Data=os.path.join('Data for different actions')\n",
    "action = \"กฎหมายรัฐธรรมนูญ.mp4\"\n",
    "a = np.load(os.path.join(Model_Data, action, \"กฎหมายรัฐธรรมนูญ.npy\"))\n",
    "a = torch.from_numpy(a)\n",
    "\n",
    "# Pad the sequences to the same length\n",
    "padded_sequences = pad_sequence(a, batch_first=True)\n",
    "len(padded_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 803,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 804,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelMap = {label:num for num, label in enumerate(actions)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 806,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs, labels = [], []\n",
    "\n",
    "for action in actions:\n",
    "    for seq in range(no_of_seqs):\n",
    "        window = []\n",
    "        for frame_num in range(seq_length):\n",
    "            res = np.load(os.path.join(Model_Data, action, f\"frame_{frame_num}.npy\")) \n",
    "            window.append(res)\n",
    "        seqs.append(window)\n",
    "\n",
    "        labels.append(labelMap[action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(seqs).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 809,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = np.array(seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing the labels from 0,1,2 to categorical data for easier accessebility\n",
    "Y_label = to_categorical(labels).astype(int)\n",
    "Y_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_data, Y_label, test_size=0.3)\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 815,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 816,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding the logs folder\n",
    "log_dir = os.path.join('Logs')\n",
    "tb_callback = TensorBoard(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural network\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, return_sequences=True, activation='relu', input_shape=(30,1662)))\n",
    "model.add(LSTM(128, return_sequences=True, activation='relu'))\n",
    "model.add(LSTM(64, return_sequences=False, activation='relu'))\n",
    "\n",
    "# adding 64 units for dense layer\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(actions.shape[0], activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eg\n",
    "eg_res = [.7, 0.2, 0.1]\n",
    "actions[np.argmax(eg_res)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 819,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, Y_train, epochs=300, callbacks=[tb_callback])\n",
    "# tensorboard --logdir=."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res=model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# again the actions with the max value provided by softmax is returned\n",
    "actions[np.argmax(res[4])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions[np.argmax(Y_test[4])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 836,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_hat = model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 838,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_true = np.argmax(Y_train, axis=1).tolist()\n",
    "# one hot encoding\n",
    "Y_hat = np.argmax(Y_hat, axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confution matrix\n",
    "multilabel_confusion_matrix(Y_true, Y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(Y_true, Y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
