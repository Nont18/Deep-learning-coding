{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video Classification using LSTM + Attention layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 855,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp \n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 856,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holist = mp.solutions.holistic \n",
    "mp_draw = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 857,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(img, model):\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img.flags.writeable = False                 \n",
    "    result = model.process(img)                 # Make prediction\n",
    "    img.flags.writeable = True                   \n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR) \n",
    "    return img, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 858,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks(img, result):\n",
    "    mp_draw.draw_landmarks(img, result.face_landmarks, mp_holist.FACEMESH_CONTOURS) # Draw face connections\n",
    "    mp_draw.draw_landmarks(img, result.pose_landmarks, mp_holist.POSE_CONNECTIONS) # Draw pose connections\n",
    "    mp_draw.draw_landmarks(img, result.left_hand_landmarks, mp_holist.HAND_CONNECTIONS) # Draw left hand connections\n",
    "    mp_draw.draw_landmarks(img, result.right_hand_landmarks, mp_holist.HAND_CONNECTIONS) # Draw right hand connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 859,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_styled_landmarks(img, result):\n",
    "    mp_draw.draw_landmarks(img, result.face_landmarks, mp_holist.FACEMESH_CONTOURS, \n",
    "                             mp_draw.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1), # color the joint \n",
    "                             mp_draw.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1) #color the connection\n",
    "                             ) \n",
    "    # mp_draw.draw_landmarks(img, result.face_landmarks, mp_holist.FACEMESH_CONTOURS, \n",
    "    #                          mp_draw.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1), # color the joint \n",
    "    #                          mp_draw.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1) #color the connection\n",
    "    #                          ) \n",
    "    \n",
    "    mp_draw.draw_landmarks(img, result.pose_landmarks, mp_holist.POSE_CONNECTIONS,\n",
    "                             mp_draw.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4), \n",
    "                             mp_draw.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    mp_draw.draw_landmarks(img, result.left_hand_landmarks, mp_holist.HAND_CONNECTIONS, \n",
    "                             mp_draw.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4), \n",
    "                             mp_draw.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    mp_draw.draw_landmarks(img, result.right_hand_landmarks, mp_holist.HAND_CONNECTIONS, \n",
    "                             mp_draw.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4), \n",
    "                             mp_draw.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                             ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 860,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(results):\n",
    "    pose=np.array([[res.x,res.y] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*2)\n",
    "    left_hnd=np.array([[res.x,res.y] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*2)\n",
    "    right_hnd=np.array([[res.x,res.y] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*2)\n",
    "    face=np.array([[res.x,res.y] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*2)\n",
    "    return np.concatenate([pose,left_hnd,right_hnd,face])\n",
    "# concatenating for the model to detect the sign language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 861,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 861,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "video_dir = \"C:/Users/araya/Desktop/keypoints/video_extract\"\n",
    "video_list = []\n",
    "video_list = os.listdir(video_dir)\n",
    "\n",
    "len(video_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 862,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For Training Only\n",
    "# import os \n",
    "# video_dir = \"C:/Users/araya/Desktop/vscode/Deep-learning-coding/MediaPipe/Data for different actions\"\n",
    "# video_list = []\n",
    "# video_list = os.listdir(video_dir)\n",
    "\n",
    "# len(video_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 863,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['กฎกระทรวง.mp4',\n",
       " 'กฎหมายรัฐธรรมนูญ.mp4',\n",
       " 'กรมอนามัย.mp4',\n",
       " 'กรรม.mp4',\n",
       " 'กรรมสิทธิ์.mp4',\n",
       " 'กระโดด.mp4',\n",
       " 'กล้วยบวชชี.mp4',\n",
       " 'กล้วยเชื่อม.mp4',\n",
       " 'กังวล.mp4',\n",
       " 'กีฬา.mp4',\n",
       " 'น้อง.mp4',\n",
       " 'เขิน.mp4',\n",
       " 'เขื่อนดิน.mp4',\n",
       " 'เขื่อนสิริกิติ์.mp4',\n",
       " 'เข้าใจผิด.mp4',\n",
       " 'เคย.mp4',\n",
       " 'เครียด.mp4',\n",
       " 'เครื่องปั่นดิน.mp4',\n",
       " 'เครื่องหมายการค้า.mp4',\n",
       " 'เจอ.mp4',\n",
       " 'เจ้าหนี้.mp4',\n",
       " 'เช่าซื้อ.mp4',\n",
       " 'เช่าทรัพย์.mp4',\n",
       " 'เซอร์เบีย.mp4',\n",
       " 'เซเนกัล.mp4',\n",
       " 'เซ็ง.mp4',\n",
       " 'เดิน.mp4',\n",
       " 'เดิมพัน.mp4',\n",
       " 'เพลีย.mp4',\n",
       " 'เมื่อย.mp4',\n",
       " 'เม็กซิโก.mp4',\n",
       " 'เฮโรอีน.mp4',\n",
       " 'แกมเบีย.mp4',\n",
       " 'แซมเบีย.mp4',\n",
       " 'โกหก.mp4',\n",
       " 'โจทก์.mp4',\n",
       " 'โชจู.mp4',\n",
       " 'ใกล้.mp4',\n",
       " 'ไดโนเสาร์.mp4',\n",
       " 'ไอซ์.mp4']"
      ]
     },
     "execution_count": 863,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 864,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for exported data, numpy arrays\n",
    "Model_Data=os.path.join('Data for different actions')\n",
    "\n",
    "actions = np.array(video_list)\n",
    "\n",
    "no_of_seqs = 1\n",
    "\n",
    "# 30 frames in length\n",
    "seq_length = 160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 865,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['กฎกระทรวง.mp4', 'กฎหมายรัฐธรรมนูญ.mp4', 'กรมอนามัย.mp4',\n",
       "       'กรรม.mp4', 'กรรมสิทธิ์.mp4', 'กระโดด.mp4', 'กล้วยบวชชี.mp4',\n",
       "       'กล้วยเชื่อม.mp4', 'กังวล.mp4', 'กีฬา.mp4', 'น้อง.mp4', 'เขิน.mp4',\n",
       "       'เขื่อนดิน.mp4', 'เขื่อนสิริกิติ์.mp4', 'เข้าใจผิด.mp4', 'เคย.mp4',\n",
       "       'เครียด.mp4', 'เครื่องปั่นดิน.mp4', 'เครื่องหมายการค้า.mp4',\n",
       "       'เจอ.mp4', 'เจ้าหนี้.mp4', 'เช่าซื้อ.mp4', 'เช่าทรัพย์.mp4',\n",
       "       'เซอร์เบีย.mp4', 'เซเนกัล.mp4', 'เซ็ง.mp4', 'เดิน.mp4',\n",
       "       'เดิมพัน.mp4', 'เพลีย.mp4', 'เมื่อย.mp4', 'เม็กซิโก.mp4',\n",
       "       'เฮโรอีน.mp4', 'แกมเบีย.mp4', 'แซมเบีย.mp4', 'โกหก.mp4',\n",
       "       'โจทก์.mp4', 'โชจู.mp4', 'ใกล้.mp4', 'ไดโนเสาร์.mp4', 'ไอซ์.mp4'],\n",
       "      dtype='<U21')"
      ]
     },
     "execution_count": 865,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collecting keypoint values for Training nd Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 866,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory where your videos are stored\n",
    "directory = \"C:/Users/araya/Desktop/keypoints/video_extract\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 867,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For Training Only\n",
    "# directory = \"C:/Users/araya/Desktop/vscode/Deep-learning-coding/MediaPipe/Data for different actions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 868,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:/Users/araya/Desktop/keypoints/video_extract'"
      ]
     },
     "execution_count": 868,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 869,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/araya/Desktop/keypoints/video_extract/กฎกระทรวง.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/กฎหมายรัฐธรรมนูญ.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/กรมอนามัย.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/กรรม.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/กรรมสิทธิ์.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/กระโดด.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/กล้วยบวชชี.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/กล้วยเชื่อม.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/กังวล.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/กีฬา.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/น้อง.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/เขิน.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/เขื่อนดิน.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/เขื่อนสิริกิติ์.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/เข้าใจผิด.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/เคย.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/เครียด.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/เครื่องปั่นดิน.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/เครื่องหมายการค้า.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/เจอ.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/เจ้าหนี้.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/เช่าซื้อ.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/เช่าทรัพย์.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/เซอร์เบีย.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/เซเนกัล.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/เซ็ง.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/เดิน.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/เดิมพัน.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/เพลีย.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/เมื่อย.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/เม็กซิโก.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/เฮโรอีน.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/แกมเบีย.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/แซมเบีย.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/โกหก.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/โจทก์.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/โชจู.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/ใกล้.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/ไดโนเสาร์.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/ไอซ์.mp4\n"
     ]
    }
   ],
   "source": [
    "for filename in actions:\n",
    "    print(directory + '/' + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 870,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Data for different actions/กฎกระทรวง.mp4/กฎกระทรวง.npy', 'Data for different actions/กฎหมายรัฐธรรมนูญ.mp4/กฎหมายรัฐธรรมนูญ.npy', 'Data for different actions/กรมอนามัย.mp4/กรมอนามัย.npy', 'Data for different actions/กรรม.mp4/กรรม.npy', 'Data for different actions/กรรมสิทธิ์.mp4/กรรมสิทธิ์.npy', 'Data for different actions/กระโดด.mp4/กระโดด.npy', 'Data for different actions/กล้วยบวชชี.mp4/กล้วยบวชชี.npy', 'Data for different actions/กล้วยเชื่อม.mp4/กล้วยเชื่อม.npy', 'Data for different actions/กังวล.mp4/กังวล.npy', 'Data for different actions/กีฬา.mp4/กีฬา.npy', 'Data for different actions/น้อง.mp4/น้อง.npy', 'Data for different actions/เขิน.mp4/เขิน.npy', 'Data for different actions/เขื่อนดิน.mp4/เขื่อนดิน.npy', 'Data for different actions/เขื่อนสิริกิติ์.mp4/เขื่อนสิริกิติ์.npy', 'Data for different actions/เข้าใจผิด.mp4/เข้าใจผิด.npy', 'Data for different actions/เคย.mp4/เคย.npy', 'Data for different actions/เครียด.mp4/เครียด.npy', 'Data for different actions/เครื่องปั่นดิน.mp4/เครื่องปั่นดิน.npy', 'Data for different actions/เครื่องหมายการค้า.mp4/เครื่องหมายการค้า.npy', 'Data for different actions/เจอ.mp4/เจอ.npy', 'Data for different actions/เจ้าหนี้.mp4/เจ้าหนี้.npy', 'Data for different actions/เช่าซื้อ.mp4/เช่าซื้อ.npy', 'Data for different actions/เช่าทรัพย์.mp4/เช่าทรัพย์.npy', 'Data for different actions/เซอร์เบีย.mp4/เซอร์เบีย.npy', 'Data for different actions/เซเนกัล.mp4/เซเนกัล.npy', 'Data for different actions/เซ็ง.mp4/เซ็ง.npy', 'Data for different actions/เดิน.mp4/เดิน.npy', 'Data for different actions/เดิมพัน.mp4/เดิมพัน.npy', 'Data for different actions/เพลีย.mp4/เพลีย.npy', 'Data for different actions/เมื่อย.mp4/เมื่อย.npy', 'Data for different actions/เม็กซิโก.mp4/เม็กซิโก.npy', 'Data for different actions/เฮโรอีน.mp4/เฮโรอีน.npy', 'Data for different actions/แกมเบีย.mp4/แกมเบีย.npy', 'Data for different actions/แซมเบีย.mp4/แซมเบีย.npy', 'Data for different actions/โกหก.mp4/โกหก.npy', 'Data for different actions/โจทก์.mp4/โจทก์.npy', 'Data for different actions/โชจู.mp4/โชจู.npy', 'Data for different actions/ใกล้.mp4/ใกล้.npy', 'Data for different actions/ไดโนเสาร์.mp4/ไดโนเสาร์.npy', 'Data for different actions/ไอซ์.mp4/ไอซ์.npy']\n"
     ]
    }
   ],
   "source": [
    "file_paths = []\n",
    "for action in actions:\n",
    "    video_path = os.path.join('Data for different actions/', action)\n",
    "    # print(video_path)\n",
    "    # print(action)\n",
    "    file_paths.append(video_path + '/' + action.split(\".\")[0] + \".npy\")\n",
    "print(file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 871,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_keypoint_sequences(file_paths):\n",
    "    keypoint_sequences = []\n",
    "    for file_path in file_paths:\n",
    "        keypoints = np.load(file_path)\n",
    "        keypoint_sequences.append(torch.tensor(keypoints, dtype=torch.float32))\n",
    "    return keypoint_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 872,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[0.5013, 0.2452, 0.5303,  ..., 0.2221, 0.5663, 0.2188],\n",
       "         [0.4997, 0.2482, 0.5302,  ..., 0.2211, 0.5652, 0.2181],\n",
       "         [0.4984, 0.2500, 0.5301,  ..., 0.2215, 0.5654, 0.2185],\n",
       "         ...,\n",
       "         [0.4861, 0.2513, 0.5202,  ..., 0.2205, 0.5572, 0.2177],\n",
       "         [0.4873, 0.2514, 0.5210,  ..., 0.2199, 0.5575, 0.2172],\n",
       "         [0.4883, 0.2516, 0.5217,  ..., 0.2198, 0.5577, 0.2170]]),\n",
       " tensor([[0.4922, 0.2382, 0.5228,  ..., 0.2153, 0.5578, 0.2124],\n",
       "         [0.4920, 0.2405, 0.5227,  ..., 0.2143, 0.5571, 0.2116],\n",
       "         [0.4920, 0.2409, 0.5228,  ..., 0.2150, 0.5567, 0.2122],\n",
       "         ...,\n",
       "         [0.4814, 0.2260, 0.5160,  ..., 0.1952, 0.5503, 0.1923],\n",
       "         [0.4815, 0.2257, 0.5162,  ..., 0.1949, 0.5503, 0.1921],\n",
       "         [0.4815, 0.2255, 0.5163,  ..., 0.1946, 0.5501, 0.1919]]),\n",
       " tensor([[0.5049, 0.2371, 0.5331,  ..., 0.2112, 0.5643, 0.2082],\n",
       "         [0.5045, 0.2381, 0.5333,  ..., 0.2108, 0.5643, 0.2081],\n",
       "         [0.5041, 0.2385, 0.5334,  ..., 0.2107, 0.5643, 0.2080],\n",
       "         ...,\n",
       "         [0.4963, 0.2307, 0.5296,  ..., 0.2070, 0.5624, 0.2043],\n",
       "         [0.4965, 0.2308, 0.5298,  ..., 0.2072, 0.5625, 0.2045],\n",
       "         [0.4972, 0.2312, 0.5301,  ..., 0.2073, 0.5633, 0.2045]]),\n",
       " tensor([[0.5134, 0.2614, 0.5447,  ..., 0.2301, 0.5818, 0.2272],\n",
       "         [0.5130, 0.2604, 0.5444,  ..., 0.2300, 0.5810, 0.2273],\n",
       "         [0.5126, 0.2599, 0.5442,  ..., 0.2304, 0.5810, 0.2276],\n",
       "         ...,\n",
       "         [0.5079, 0.2693, 0.5401,  ..., 0.2373, 0.5780, 0.2349],\n",
       "         [0.5090, 0.2688, 0.5409,  ..., 0.2369, 0.5782, 0.2346],\n",
       "         [0.5092, 0.2683, 0.5411,  ..., 0.2365, 0.5786, 0.2341]]),\n",
       " tensor([[0.4883, 0.2402, 0.5162,  ..., 0.2161, 0.5482, 0.2132],\n",
       "         [0.4878, 0.2402, 0.5162,  ..., 0.2163, 0.5469, 0.2135],\n",
       "         [0.4863, 0.2402, 0.5160,  ..., 0.2169, 0.5477, 0.2141],\n",
       "         ...,\n",
       "         [0.4660, 0.2589, 0.4990,  ..., 0.2230, 0.5337, 0.2207],\n",
       "         [0.4651, 0.2560, 0.4985,  ..., 0.2216, 0.5337, 0.2192],\n",
       "         [0.4646, 0.2539, 0.4980,  ..., 0.2197, 0.5334, 0.2174]]),\n",
       " tensor([[0.4992, 0.1994, 0.5284,  ..., 0.1762, 0.5657, 0.1731],\n",
       "         [0.4988, 0.2047, 0.5288,  ..., 0.1755, 0.5663, 0.1723],\n",
       "         [0.4982, 0.2082, 0.5291,  ..., 0.1761, 0.5671, 0.1730],\n",
       "         ...,\n",
       "         [0.4743, 0.1974, 0.5095,  ..., 0.1651, 0.5475, 0.1620],\n",
       "         [0.4732, 0.1974, 0.5084,  ..., 0.1647, 0.5461, 0.1615],\n",
       "         [0.4720, 0.1974, 0.5073,  ..., 0.1645, 0.5453, 0.1614]]),\n",
       " tensor([[0.5023, 0.2809, 0.5396,  ..., 0.2357, 0.5847, 0.2321],\n",
       "         [0.5023, 0.2806, 0.5389,  ..., 0.2352, 0.5840, 0.2322],\n",
       "         [0.5022, 0.2805, 0.5382,  ..., 0.2352, 0.5837, 0.2322],\n",
       "         ...,\n",
       "         [0.5024, 0.2594, 0.5358,  ..., 0.2248, 0.5784, 0.2214],\n",
       "         [0.5023, 0.2594, 0.5356,  ..., 0.2251, 0.5787, 0.2217],\n",
       "         [0.5022, 0.2594, 0.5355,  ..., 0.2251, 0.5789, 0.2217]]),\n",
       " tensor([[0.4868, 0.2821, 0.5248,  ..., 0.2365, 0.5711, 0.2335],\n",
       "         [0.4861, 0.2786, 0.5238,  ..., 0.2362, 0.5702, 0.2334],\n",
       "         [0.4856, 0.2769, 0.5231,  ..., 0.2361, 0.5699, 0.2333],\n",
       "         ...,\n",
       "         [0.4826, 0.2583, 0.5177,  ..., 0.2290, 0.5606, 0.2261],\n",
       "         [0.4820, 0.2583, 0.5173,  ..., 0.2290, 0.5603, 0.2261],\n",
       "         [0.4818, 0.2580, 0.5171,  ..., 0.2292, 0.5600, 0.2263]]),\n",
       " tensor([[0.5016, 0.2321, 0.5315,  ..., 0.2168, 0.5647, 0.2139],\n",
       "         [0.5015, 0.2340, 0.5317,  ..., 0.2161, 0.5645, 0.2135],\n",
       "         [0.5016, 0.2346, 0.5319,  ..., 0.2162, 0.5644, 0.2137],\n",
       "         ...,\n",
       "         [0.4953, 0.2390, 0.5281,  ..., 0.2114, 0.5598, 0.2091],\n",
       "         [0.4950, 0.2392, 0.5278,  ..., 0.2116, 0.5589, 0.2092],\n",
       "         [0.4949, 0.2392, 0.5276,  ..., 0.2111, 0.5585, 0.2087]]),\n",
       " tensor([[0.4926, 0.1945, 0.5176,  ..., 0.1603, 0.5488, 0.1581],\n",
       "         [0.4942, 0.1949, 0.5188,  ..., 0.1596, 0.5481, 0.1576],\n",
       "         [0.4948, 0.1960, 0.5192,  ..., 0.1601, 0.5479, 0.1581],\n",
       "         ...,\n",
       "         [0.4872, 0.1918, 0.5092,  ..., 0.1586, 0.5375, 0.1565],\n",
       "         [0.4869, 0.1921, 0.5089,  ..., 0.1588, 0.5375, 0.1567],\n",
       "         [0.4867, 0.1924, 0.5087,  ..., 0.1588, 0.5374, 0.1567]]),\n",
       " tensor([[0.4956, 0.2681, 0.5254,  ..., 0.2363, 0.5554, 0.2332],\n",
       "         [0.4948, 0.2681, 0.5253,  ..., 0.2359, 0.5540, 0.2333],\n",
       "         [0.4943, 0.2681, 0.5253,  ..., 0.2360, 0.5538, 0.2332],\n",
       "         ...,\n",
       "         [0.4895, 0.2689, 0.5229,  ..., 0.2302, 0.5513, 0.2276],\n",
       "         [0.4889, 0.2688, 0.5223,  ..., 0.2303, 0.5508, 0.2276],\n",
       "         [0.4882, 0.2688, 0.5216,  ..., 0.2303, 0.5507, 0.2276]]),\n",
       " tensor([[0.5032, 0.2228, 0.5332,  ..., 0.2003, 0.5663, 0.1971],\n",
       "         [0.5035, 0.2228, 0.5342,  ..., 0.2000, 0.5662, 0.1969],\n",
       "         [0.5035, 0.2230, 0.5346,  ..., 0.1996, 0.5664, 0.1963],\n",
       "         ...,\n",
       "         [0.4982, 0.2206, 0.5314,  ..., 0.1923, 0.5578, 0.1892],\n",
       "         [0.4983, 0.2178, 0.5312,  ..., 0.1913, 0.5581, 0.1883],\n",
       "         [0.4979, 0.2173, 0.5308,  ..., 0.1908, 0.5578, 0.1877]]),\n",
       " tensor([[0.5442, 0.2507, 0.5775,  ..., 0.2238, 0.6105, 0.2203],\n",
       "         [0.5444, 0.2508, 0.5777,  ..., 0.2229, 0.6094, 0.2198],\n",
       "         [0.5445, 0.2510, 0.5778,  ..., 0.2227, 0.6093, 0.2195],\n",
       "         ...,\n",
       "         [0.5323, 0.2560, 0.5694,  ..., 0.2233, 0.5966, 0.2206],\n",
       "         [0.5323, 0.2553, 0.5693,  ..., 0.2228, 0.5957, 0.2201],\n",
       "         [0.5322, 0.2550, 0.5691,  ..., 0.2227, 0.5952, 0.2199]]),\n",
       " tensor([[0.5402, 0.2562, 0.5746,  ..., 0.2290, 0.6041, 0.2257],\n",
       "         [0.5389, 0.2596, 0.5743,  ..., 0.2292, 0.6031, 0.2264],\n",
       "         [0.5379, 0.2616, 0.5740,  ..., 0.2286, 0.6027, 0.2258],\n",
       "         ...,\n",
       "         [0.5382, 0.2599, 0.5730,  ..., 0.2260, 0.6022, 0.2236],\n",
       "         [0.5381, 0.2599, 0.5730,  ..., 0.2252, 0.6013, 0.2227],\n",
       "         [0.5381, 0.2600, 0.5730,  ..., 0.2248, 0.6005, 0.2222]]),\n",
       " tensor([[0.5030, 0.2553, 0.5347,  ..., 0.2292, 0.5699, 0.2265],\n",
       "         [0.5028, 0.2582, 0.5347,  ..., 0.2294, 0.5689, 0.2266],\n",
       "         [0.5028, 0.2605, 0.5347,  ..., 0.2296, 0.5690, 0.2269],\n",
       "         ...,\n",
       "         [0.5028, 0.2675, 0.5362,  ..., 0.2346, 0.5672, 0.2323],\n",
       "         [0.5007, 0.2672, 0.5343,  ..., 0.2340, 0.5668, 0.2317],\n",
       "         [0.4988, 0.2671, 0.5325,  ..., 0.2335, 0.5670, 0.2312]]),\n",
       " tensor([[0.5069, 0.2355, 0.5408,  ..., 0.2083, 0.5700, 0.2051],\n",
       "         [0.5045, 0.2395, 0.5388,  ..., 0.2074, 0.5699, 0.2043],\n",
       "         [0.5028, 0.2420, 0.5375,  ..., 0.2076, 0.5699, 0.2045],\n",
       "         ...,\n",
       "         [0.4952, 0.2462, 0.5304,  ..., 0.2042, 0.5658, 0.2015],\n",
       "         [0.4952, 0.2448, 0.5305,  ..., 0.2041, 0.5661, 0.2014],\n",
       "         [0.4953, 0.2436, 0.5307,  ..., 0.2040, 0.5662, 0.2013]]),\n",
       " tensor([[0.5035, 0.2282, 0.5348,  ..., 0.1982, 0.5710, 0.1953],\n",
       "         [0.5033, 0.2284, 0.5348,  ..., 0.1983, 0.5713, 0.1954],\n",
       "         [0.5032, 0.2286, 0.5348,  ..., 0.1984, 0.5715, 0.1957],\n",
       "         ...,\n",
       "         [0.4810, 0.2296, 0.5146,  ..., 0.1927, 0.5542, 0.1897],\n",
       "         [0.4812, 0.2296, 0.5148,  ..., 0.1929, 0.5549, 0.1900],\n",
       "         [0.4816, 0.2299, 0.5152,  ..., 0.1932, 0.5557, 0.1903]]),\n",
       " tensor([[0.5108, 0.2425, 0.5387,  ..., 0.2205, 0.5724, 0.2176],\n",
       "         [0.5091, 0.2430, 0.5379,  ..., 0.2205, 0.5713, 0.2177],\n",
       "         [0.5080, 0.2432, 0.5374,  ..., 0.2207, 0.5714, 0.2179],\n",
       "         ...,\n",
       "         [0.4996, 0.2549, 0.5312,  ..., 0.2291, 0.5629, 0.2266],\n",
       "         [0.4994, 0.2549, 0.5310,  ..., 0.2292, 0.5629, 0.2267],\n",
       "         [0.4993, 0.2549, 0.5310,  ..., 0.2294, 0.5630, 0.2269]]),\n",
       " tensor([[0.4878, 0.2235, 0.5181,  ..., 0.2010, 0.5511, 0.1981],\n",
       "         [0.4870, 0.2286, 0.5177,  ..., 0.2005, 0.5506, 0.1977],\n",
       "         [0.4865, 0.2315, 0.5175,  ..., 0.2013, 0.5508, 0.1984],\n",
       "         ...,\n",
       "         [0.4901, 0.2217, 0.5252,  ..., 0.1917, 0.5576, 0.1888],\n",
       "         [0.4898, 0.2224, 0.5251,  ..., 0.1921, 0.5575, 0.1892],\n",
       "         [0.4896, 0.2232, 0.5250,  ..., 0.1926, 0.5572, 0.1897]]),\n",
       " tensor([[0.5194, 0.2227, 0.5555,  ..., 0.2000, 0.5911, 0.1970],\n",
       "         [0.5200, 0.2228, 0.5560,  ..., 0.1995, 0.5912, 0.1965],\n",
       "         [0.5202, 0.2230, 0.5563,  ..., 0.1997, 0.5909, 0.1970],\n",
       "         ...,\n",
       "         [0.5146, 0.2198, 0.5498,  ..., 0.1937, 0.5821, 0.1907],\n",
       "         [0.5141, 0.2205, 0.5488,  ..., 0.1941, 0.5816, 0.1911],\n",
       "         [0.5137, 0.2209, 0.5481,  ..., 0.1947, 0.5812, 0.1917]]),\n",
       " tensor([[0.5103, 0.2428, 0.5393,  ..., 0.2209, 0.5757, 0.2179],\n",
       "         [0.5106, 0.2437, 0.5403,  ..., 0.2212, 0.5751, 0.2183],\n",
       "         [0.5112, 0.2443, 0.5412,  ..., 0.2224, 0.5748, 0.2189],\n",
       "         ...,\n",
       "         [0.4800, 0.2603, 0.5132,  ..., 0.2267, 0.5532, 0.2237],\n",
       "         [0.4807, 0.2604, 0.5139,  ..., 0.2267, 0.5539, 0.2238],\n",
       "         [0.4813, 0.2604, 0.5146,  ..., 0.2268, 0.5548, 0.2238]]),\n",
       " tensor([[0.4945, 0.2362, 0.5214,  ..., 0.2115, 0.5529, 0.2086],\n",
       "         [0.4947, 0.2363, 0.5218,  ..., 0.2122, 0.5528, 0.2094],\n",
       "         [0.4951, 0.2364, 0.5225,  ..., 0.2128, 0.5531, 0.2099],\n",
       "         ...,\n",
       "         [0.4933, 0.2480, 0.5269,  ..., 0.2209, 0.5563, 0.2184],\n",
       "         [0.4933, 0.2480, 0.5269,  ..., 0.2209, 0.5565, 0.2185],\n",
       "         [0.4932, 0.2480, 0.5268,  ..., 0.2208, 0.5564, 0.2185]]),\n",
       " tensor([[0.5038, 0.2425, 0.5312,  ..., 0.2163, 0.5665, 0.2136],\n",
       "         [0.5027, 0.2423, 0.5311,  ..., 0.2159, 0.5653, 0.2133],\n",
       "         [0.5018, 0.2423, 0.5310,  ..., 0.2156, 0.5649, 0.2130],\n",
       "         ...,\n",
       "         [0.4950, 0.2412, 0.5274,  ..., 0.2200, 0.5624, 0.2175],\n",
       "         [0.4951, 0.2412, 0.5277,  ..., 0.2202, 0.5625, 0.2177],\n",
       "         [0.4951, 0.2412, 0.5278,  ..., 0.2205, 0.5626, 0.2180]]),\n",
       " tensor([[0.4916, 0.2518, 0.5239,  ..., 0.2234, 0.5597, 0.2202],\n",
       "         [0.4907, 0.2519, 0.5239,  ..., 0.2238, 0.5580, 0.2209],\n",
       "         [0.4896, 0.2520, 0.5237,  ..., 0.2239, 0.5580, 0.2210],\n",
       "         ...,\n",
       "         [0.4850, 0.2552, 0.5177,  ..., 0.2228, 0.5509, 0.2199],\n",
       "         [0.4839, 0.2550, 0.5167,  ..., 0.2229, 0.5505, 0.2199],\n",
       "         [0.4831, 0.2549, 0.5158,  ..., 0.2230, 0.5501, 0.2200]]),\n",
       " tensor([[0.5051, 0.2347, 0.5396,  ..., 0.2125, 0.5717, 0.2094],\n",
       "         [0.5024, 0.2389, 0.5385,  ..., 0.2114, 0.5707, 0.2087],\n",
       "         [0.5005, 0.2417, 0.5378,  ..., 0.2117, 0.5704, 0.2090],\n",
       "         ...,\n",
       "         [0.4950, 0.2365, 0.5280,  ..., 0.2101, 0.5663, 0.2072],\n",
       "         [0.4950, 0.2366, 0.5280,  ..., 0.2104, 0.5667, 0.2075],\n",
       "         [0.4949, 0.2366, 0.5280,  ..., 0.2107, 0.5671, 0.2078]]),\n",
       " tensor([[0.5064, 0.2529, 0.5408,  ..., 0.2289, 0.5751, 0.2261],\n",
       "         [0.5055, 0.2550, 0.5406,  ..., 0.2281, 0.5752, 0.2257],\n",
       "         [0.5048, 0.2568, 0.5404,  ..., 0.2285, 0.5751, 0.2260],\n",
       "         ...,\n",
       "         [0.4960, 0.2624, 0.5307,  ..., 0.2333, 0.5642, 0.2311],\n",
       "         [0.4961, 0.2619, 0.5308,  ..., 0.2330, 0.5652, 0.2307],\n",
       "         [0.4961, 0.2615, 0.5308,  ..., 0.2326, 0.5653, 0.2302]]),\n",
       " tensor([[0.5267, 0.2322, 0.5590,  ..., 0.2023, 0.5921, 0.1996],\n",
       "         [0.5267, 0.2305, 0.5591,  ..., 0.2029, 0.5925, 0.1999],\n",
       "         [0.5266, 0.2296, 0.5591,  ..., 0.2027, 0.5928, 0.1997],\n",
       "         ...,\n",
       "         [0.5122, 0.2243, 0.5489,  ..., 0.1887, 0.5819, 0.1855],\n",
       "         [0.5114, 0.2239, 0.5483,  ..., 0.1889, 0.5817, 0.1857],\n",
       "         [0.5110, 0.2235, 0.5479,  ..., 0.1890, 0.5815, 0.1858]]),\n",
       " tensor([[0.5050, 0.2164, 0.5329,  ..., 0.1991, 0.5700, 0.1959],\n",
       "         [0.5041, 0.2187, 0.5330,  ..., 0.1986, 0.5689, 0.1959],\n",
       "         [0.5031, 0.2218, 0.5330,  ..., 0.1986, 0.5693, 0.1959],\n",
       "         ...,\n",
       "         [0.4987, 0.2252, 0.5316,  ..., 0.1958, 0.5655, 0.1932],\n",
       "         [0.4982, 0.2238, 0.5315,  ..., 0.1951, 0.5653, 0.1926],\n",
       "         [0.4969, 0.2230, 0.5312,  ..., 0.1945, 0.5649, 0.1919]]),\n",
       " tensor([[0.5097, 0.2301, 0.5414,  ..., 0.2103, 0.5773, 0.2070],\n",
       "         [0.5098, 0.2316, 0.5418,  ..., 0.2101, 0.5762, 0.2072],\n",
       "         [0.5099, 0.2328, 0.5420,  ..., 0.2103, 0.5754, 0.2074],\n",
       "         ...,\n",
       "         [0.5106, 0.2519, 0.5435,  ..., 0.2234, 0.5731, 0.2204],\n",
       "         [0.5082, 0.2514, 0.5417,  ..., 0.2223, 0.5717, 0.2193],\n",
       "         [0.5074, 0.2516, 0.5411,  ..., 0.2213, 0.5703, 0.2182]]),\n",
       " tensor([[0.5410, 0.2495, 0.5727,  ..., 0.2254, 0.6085, 0.2224],\n",
       "         [0.5399, 0.2502, 0.5727,  ..., 0.2246, 0.6066, 0.2219],\n",
       "         [0.5390, 0.2511, 0.5727,  ..., 0.2243, 0.6062, 0.2215],\n",
       "         ...,\n",
       "         [0.5131, 0.2461, 0.5444,  ..., 0.2186, 0.5857, 0.2158],\n",
       "         [0.5137, 0.2459, 0.5446,  ..., 0.2182, 0.5858, 0.2153],\n",
       "         [0.5141, 0.2456, 0.5448,  ..., 0.2178, 0.5860, 0.2150]]),\n",
       " tensor([[0.4813, 0.2267, 0.5131,  ..., 0.2046, 0.5499, 0.2013],\n",
       "         [0.4796, 0.2283, 0.5128,  ..., 0.2053, 0.5479, 0.2026],\n",
       "         [0.4786, 0.2300, 0.5127,  ..., 0.2053, 0.5480, 0.2025],\n",
       "         ...,\n",
       "         [0.4782, 0.2297, 0.5102,  ..., 0.1990, 0.5464, 0.1965],\n",
       "         [0.4781, 0.2300, 0.5102,  ..., 0.1985, 0.5459, 0.1960],\n",
       "         [0.4780, 0.2304, 0.5103,  ..., 0.1979, 0.5461, 0.1953]]),\n",
       " tensor([[0.5051, 0.2433, 0.5385,  ..., 0.2218, 0.5701, 0.2185],\n",
       "         [0.5042, 0.2464, 0.5381,  ..., 0.2222, 0.5696, 0.2193],\n",
       "         [0.5039, 0.2479, 0.5379,  ..., 0.2232, 0.5700, 0.2201],\n",
       "         ...,\n",
       "         [0.4965, 0.2517, 0.5321,  ..., 0.2245, 0.5645, 0.2217],\n",
       "         [0.4956, 0.2516, 0.5311,  ..., 0.2241, 0.5637, 0.2215],\n",
       "         [0.4943, 0.2514, 0.5299,  ..., 0.2234, 0.5628, 0.2207]]),\n",
       " tensor([[0.5269, 0.2488, 0.5598,  ..., 0.2212, 0.5929, 0.2181],\n",
       "         [0.5260, 0.2493, 0.5595,  ..., 0.2200, 0.5918, 0.2172],\n",
       "         [0.5250, 0.2497, 0.5588,  ..., 0.2200, 0.5916, 0.2171],\n",
       "         ...,\n",
       "         [0.5126, 0.2514, 0.5469,  ..., 0.2241, 0.5827, 0.2212],\n",
       "         [0.5125, 0.2512, 0.5467,  ..., 0.2232, 0.5813, 0.2204],\n",
       "         [0.5123, 0.2512, 0.5464,  ..., 0.2229, 0.5795, 0.2200]]),\n",
       " tensor([[0.5103, 0.2330, 0.5429,  ..., 0.2087, 0.5745, 0.2056],\n",
       "         [0.5071, 0.2360, 0.5417,  ..., 0.2081, 0.5731, 0.2055],\n",
       "         [0.5052, 0.2381, 0.5409,  ..., 0.2081, 0.5733, 0.2056],\n",
       "         ...,\n",
       "         [0.4875, 0.2316, 0.5212,  ..., 0.2020, 0.5606, 0.1993],\n",
       "         [0.4872, 0.2318, 0.5207,  ..., 0.2024, 0.5604, 0.1997],\n",
       "         [0.4870, 0.2318, 0.5204,  ..., 0.2027, 0.5604, 0.2000]]),\n",
       " tensor([[0.4952, 0.2338, 0.5275,  ..., 0.2118, 0.5593, 0.2078],\n",
       "         [0.4926, 0.2435, 0.5271,  ..., 0.2111, 0.5600, 0.2072],\n",
       "         [0.4916, 0.2483, 0.5269,  ..., 0.2115, 0.5596, 0.2079],\n",
       "         ...,\n",
       "         [0.4846, 0.2462, 0.5202,  ..., 0.2075, 0.5649, 0.2045],\n",
       "         [0.4852, 0.2450, 0.5209,  ..., 0.2067, 0.5648, 0.2038],\n",
       "         [0.4857, 0.2436, 0.5214,  ..., 0.2062, 0.5648, 0.2033]]),\n",
       " tensor([[0.4828, 0.2604, 0.5156,  ..., 0.2290, 0.5483, 0.2258],\n",
       "         [0.4816, 0.2606, 0.5144,  ..., 0.2298, 0.5474, 0.2267],\n",
       "         [0.4809, 0.2608, 0.5137,  ..., 0.2298, 0.5475, 0.2266],\n",
       "         ...,\n",
       "         [0.4832, 0.2490, 0.5165,  ..., 0.2185, 0.5557, 0.2156],\n",
       "         [0.4845, 0.2491, 0.5178,  ..., 0.2186, 0.5566, 0.2158],\n",
       "         [0.4855, 0.2492, 0.5189,  ..., 0.2191, 0.5573, 0.2163]]),\n",
       " tensor([[0.5122, 0.2456, 0.5457,  ..., 0.2240, 0.5836, 0.2207],\n",
       "         [0.5121, 0.2461, 0.5456,  ..., 0.2236, 0.5818, 0.2208],\n",
       "         [0.5121, 0.2465, 0.5455,  ..., 0.2233, 0.5815, 0.2204],\n",
       "         ...,\n",
       "         [0.5017, 0.2434, 0.5393,  ..., 0.2133, 0.5812, 0.2105],\n",
       "         [0.5022, 0.2433, 0.5403,  ..., 0.2130, 0.5818, 0.2102],\n",
       "         [0.5032, 0.2431, 0.5414,  ..., 0.2126, 0.5821, 0.2097]]),\n",
       " tensor([[0.4849, 0.2484, 0.5181,  ..., 0.2198, 0.5543, 0.2162],\n",
       "         [0.4846, 0.2500, 0.5179,  ..., 0.2190, 0.5539, 0.2157],\n",
       "         [0.4844, 0.2507, 0.5178,  ..., 0.2192, 0.5537, 0.2161],\n",
       "         ...,\n",
       "         [0.4589, 0.2435, 0.4910,  ..., 0.2089, 0.5299, 0.2058],\n",
       "         [0.4586, 0.2436, 0.4907,  ..., 0.2086, 0.5293, 0.2056],\n",
       "         [0.4583, 0.2437, 0.4902,  ..., 0.2083, 0.5289, 0.2052]]),\n",
       " tensor([[0.5199, 0.2290, 0.5497,  ..., 0.2089, 0.5876, 0.2058],\n",
       "         [0.5195, 0.2307, 0.5494,  ..., 0.2087, 0.5873, 0.2059],\n",
       "         [0.5193, 0.2327, 0.5491,  ..., 0.2088, 0.5877, 0.2059],\n",
       "         ...,\n",
       "         [0.5108, 0.2431, 0.5433,  ..., 0.2101, 0.5781, 0.2077],\n",
       "         [0.5093, 0.2422, 0.5419,  ..., 0.2093, 0.5768, 0.2068],\n",
       "         [0.5079, 0.2405, 0.5405,  ..., 0.2076, 0.5751, 0.2053]]),\n",
       " tensor([[0.5005, 0.2439, 0.5342,  ..., 0.2173, 0.5694, 0.2139],\n",
       "         [0.5002, 0.2448, 0.5342,  ..., 0.2159, 0.5688, 0.2128],\n",
       "         [0.4999, 0.2457, 0.5341,  ..., 0.2164, 0.5683, 0.2134],\n",
       "         ...,\n",
       "         [0.4965, 0.2433, 0.5309,  ..., 0.2143, 0.5670, 0.2114],\n",
       "         [0.4960, 0.2433, 0.5308,  ..., 0.2136, 0.5660, 0.2107],\n",
       "         [0.4950, 0.2433, 0.5305,  ..., 0.2130, 0.5651, 0.2100]])]"
      ]
     },
     "execution_count": 872,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the sequences\n",
    "import torch\n",
    "sequences = load_keypoint_sequences(file_paths)\n",
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 875,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40, 210, 1086])\n"
     ]
    }
   ],
   "source": [
    "# Pad the sequences to the same length\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "padded_sequences = pad_sequence(sequences, batch_first=True)\n",
    "pad_sequence\n",
    "print(padded_sequences.shape) # (batch_size, max_sequence_length, num_keypoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 876,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['กฎกระทรวง',\n",
       " 'กฎหมายรัฐธรรมนูญ',\n",
       " 'กรมอนามัย',\n",
       " 'กรรม',\n",
       " 'กรรมสิทธิ์',\n",
       " 'กระโดด',\n",
       " 'กล้วยบวชชี',\n",
       " 'กล้วยเชื่อม',\n",
       " 'กังวล',\n",
       " 'กีฬา',\n",
       " 'น้อง',\n",
       " 'เขิน',\n",
       " 'เขื่อนดิน',\n",
       " 'เขื่อนสิริกิติ์',\n",
       " 'เข้าใจผิด',\n",
       " 'เคย',\n",
       " 'เครียด',\n",
       " 'เครื่องปั่นดิน',\n",
       " 'เครื่องหมายการค้า',\n",
       " 'เจอ',\n",
       " 'เจ้าหนี้',\n",
       " 'เช่าซื้อ',\n",
       " 'เช่าทรัพย์',\n",
       " 'เซอร์เบีย',\n",
       " 'เซเนกัล',\n",
       " 'เซ็ง',\n",
       " 'เดิน',\n",
       " 'เดิมพัน',\n",
       " 'เพลีย',\n",
       " 'เมื่อย',\n",
       " 'เม็กซิโก',\n",
       " 'เฮโรอีน',\n",
       " 'แกมเบีย',\n",
       " 'แซมเบีย',\n",
       " 'โกหก',\n",
       " 'โจทก์',\n",
       " 'โชจู',\n",
       " 'ใกล้',\n",
       " 'ไดโนเสาร์',\n",
       " 'ไอซ์']"
      ]
     },
     "execution_count": 876,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = [action.split(\".\")[0] for action in actions]\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "labels = le.fit_transform(labels)\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Traning only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 784,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# df = pd.read_csv(\"script1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 785,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels = []\n",
    "\n",
    "# for i in range(len(df.label)):\n",
    "#     labels.append(df.label[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 786,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 851,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Create a custom dataset\n",
    "class KeypointDataset(Dataset):\n",
    "    def __init__(self, file_paths, labels):\n",
    "        self.file_paths = file_paths\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        keypoints = np.load(self.file_paths[idx])\n",
    "        label = self.labels[idx]\n",
    "        return torch.tensor(keypoints, dtype=torch.float32), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 852,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset\n",
    "dataset = KeypointDataset(file_paths, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.file_paths)\n",
    "print(dataset.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 854,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collate function for padding\n",
    "def collate_fn(batch):\n",
    "    sequences, labels = zip(*batch)\n",
    "    padded_sequences = pad_sequence(sequences, batch_first=True)\n",
    "    return padded_sequences, torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DataLoader\n",
    "batch_size = 16\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (data, label) in enumerate(data_loader):\n",
    "    print(data.shape)\n",
    "    print(label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device (use GPU if available)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 794,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        self.attention_weights = nn.Linear(hidden_size, 1, bias=False)\n",
    "\n",
    "    def forward(self, lstm_output):\n",
    "        # lstm_output: (batch_size, sequence_length, hidden_size)\n",
    "        attention_scores = self.attention_weights(lstm_output)  # (batch_size, sequence_length, 1)\n",
    "        attention_weights = torch.softmax(attention_scores, dim=1)  # (batch_size, sequence_length, 1)\n",
    "        weighted_output = torch.sum(lstm_output * attention_weights, dim=1)  # (batch_size, hidden_size)\n",
    "        return weighted_output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 795,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes,dropout):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        # self.pool = nn.MaxPool2d(kernel_size=(2,1), stride=1)\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.attention = AttentionLayer(hidden_size)\n",
    "        self.fc1 = nn.Linear(hidden_size, 256)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, h, c):\n",
    "        # h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        # c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        # Apply pooling before LSTM\n",
    "        # x = self.pool(x)\n",
    "\n",
    "        # Forward propagate the LSTM\n",
    "        lstm_output, (h,c) = self.lstm(x, (h,c))\n",
    "\n",
    "        # Apply attention to the LSTM output\n",
    "        attention_output, attention_weights = self.attention(lstm_output)\n",
    "\n",
    "        # Classification based on attention output\n",
    "        out = self.fc1(attention_output)\n",
    "        # out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        return out, h, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 796,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 1086\n",
    "hidden_size = 256 \n",
    "num_layers = 2\n",
    "num_classes = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 797,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model, loss function, and optimizer\n",
    "model = LSTMModel(input_size=1086, hidden_size=256, num_layers=2, num_classes=40, dropout=0.3).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 799,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()  # For multi-class classification \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 800,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Load model\n",
    "# model.load_state_dict(torch.load('saved_data/attention_lstm.pt'))\n",
    "# optimizer.load_state_dict(torch.load('saved_data/optimizer.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 801,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load model\n",
    "model.load_state_dict(torch.load('saved_data/fixed_attention_lstm.pt'))\n",
    "optimizer.load_state_dict(torch.load('saved_data/fixed_attention_lstm_optimizer.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "# References : https://saturncloud.io/blog/calculating-the-accuracy-of-pytorch-models-every-epoch/#:~:text=In%20order%20to%20calculate%20the,tensor%20along%20a%20specified%20dimension\n",
    "\n",
    "from sklearn.metrics import f1_score, recall_score\n",
    "\n",
    "num_epochs = 300\n",
    "loss_logger = []\n",
    "accuracy_logger = []\n",
    "f1_logger = []\n",
    "recall_logger = []\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "# n_epochs = []\n",
    "for epoch in range(num_epochs):\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    model.train()\n",
    "\n",
    "    h = torch.zeros(num_layers, batch_size, hidden_size).to(device)\n",
    "    c = torch.zeros(num_layers, batch_size, hidden_size).to(device)\n",
    "\n",
    "    for i, (sequences, labels) in enumerate(data_loader):\n",
    "        # Move data to the device\n",
    "        # labels = labels.type(torch.LongTensor)   # casting to long\n",
    "        sequences = sequences.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        h = h.detach()\n",
    "        c = c.detach()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs, h, c = model(sequences, h, c)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        total_correct += (predicted == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "        # Store predictions and labels for calculating metrics\n",
    "        all_preds.extend(predicted.cpu().numpy())  # Store predictions\n",
    "        all_labels.extend(labels.cpu().numpy())    # Store true labels\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # loss_logger.append(loss.item())\n",
    "    loss_logger.append(loss.item())\n",
    "    accuracy = 100 * total_correct /total_samples\n",
    "\n",
    "    # Calculate F1 score and recall\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')  # Weighted average for multi-class\n",
    "    recall = recall_score(all_labels, all_preds, average='weighted')\n",
    "\n",
    "    f1_logger.append(f1)\n",
    "    recall_logger.append(recall)\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f} , Accuracy : {accuracy:.2f}%, F1 Score: {f1:.2f}, Recall: {recall:.2f}')\n",
    "    accuracy_logger.append(accuracy)\n",
    "    # n_epochs.append(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.title(\"Loss Curve\")\n",
    "plt.plot(loss_logger, label='train_loss')\n",
    "# plt.plot(accuracy_logger,label='accuracy')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.title(\"Accuracy Curve\")\n",
    "# plt.plot(loss_logger, label='train_loss')\n",
    "plt.plot(accuracy_logger,label='accuracy')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 802,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy(loader, model, h, c):\n",
    "    if data_loader:\n",
    "        print(\"Checking accuracy on training data\")\n",
    "    else:\n",
    "        print(\"Checking accuracy on test data\")  \n",
    "          \n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device)\n",
    "            y = y.to(device=device)\n",
    "            # x = x.reshape(x.shape[0], -1)\n",
    "            \n",
    "            scores,h,c = model(x,h,c)\n",
    "            _, predictions = scores.max(1)\n",
    "            num_correct += (predictions == y).sum()\n",
    "            num_samples += predictions.size(0)\n",
    "            \n",
    "        print(f\"Got {num_correct} / {num_samples} with accuracy {float(num_correct)/float(num_samples)*100:.2f}\")\n",
    "        \n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 807,
   "metadata": {},
   "outputs": [],
   "source": [
    "# h = torch.zeros(num_layers, 8, hidden_size).to(device)\n",
    "# c = torch.zeros(num_layers, 8, hidden_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_accuracy(data_loader, model,h,c)\n",
    "# check_accuracy(test_loader, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save and Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_sequence(sequences, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# No need to track gradients during inference\n",
    "with torch.no_grad():\n",
    "    # Get the model's output (logits)\n",
    "    outputs = model(padded_sequences.to(device),h,c)\n",
    "\n",
    "# outputs = torch.softmax(outputs, dim=1)\n",
    "# outputs = torch.max(outputs,1)\n",
    "\n",
    "outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1171,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = [\"Data for different actions/เขิน.mp4/เขิน.npy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the sequences\n",
    "import torch\n",
    "sequences = load_keypoint_sequences(file_paths)\n",
    "# Change list to numpy array \n",
    "sequences = np.array(sequences)\n",
    "# Change numpy array to tensor\n",
    "sequences = torch.FloatTensor(sequences)\n",
    "sequences = pad_sequence(sequences, batch_first=True)\n",
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(sequences.to(device))\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [action.split(\".\")[0] for action in actions]\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change from tensor to numpy arrat\n",
    "outputs = outputs.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, word in enumerate(outputs):\n",
    "    # max_value = torch.max(outputs)\n",
    "    list_outputs = max(outputs)\n",
    "    print(list_outputs)\n",
    "    # print(max_value)\n",
    "    # print(max_value.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_max = max(range(len(list_outputs)), key=list_outputs.__getitem__)\n",
    "index_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labels[index_max])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test accuracy on training dataset\n",
    "import os \n",
    "video_dir = \"C:/Users/araya/Desktop/vscode/Deep-learning-coding/MediaPipe/Data for different actions\"\n",
    "video_list = []\n",
    "video_list = os.listdir(video_dir)\n",
    "\n",
    "len(video_list)\n",
    "\n",
    "Model_Data=os.path.join('Data for different actions')\n",
    "\n",
    "actions = np.array(video_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_name = [action.split(\".\")[0] for action in actions]\n",
    "train_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "num_clip = 0\n",
    "for clip in train_name:\n",
    "    # print(clip)\n",
    "    file_paths = [f\"Data for different actions/{clip}.mp4/{clip}.npy\"]\n",
    "    # print(file_paths)\n",
    "\n",
    "    sequences = load_keypoint_sequences(file_paths)\n",
    "    # Change list to numpy array \n",
    "    sequences = np.array(sequences)\n",
    "    # Change numpy array to tensor\n",
    "    sequences = torch.FloatTensor(sequences)\n",
    "    sequences = pad_sequence(sequences, batch_first=True)\n",
    "\n",
    "    outputs = model(sequences.to(device),h,c)\n",
    "\n",
    "    for idx, word in enumerate(outputs):\n",
    "        # max_value = torch.max(outputs)\n",
    "        list_outputs = max(outputs)\n",
    "\n",
    "    index_max = max(range(len(list_outputs)), key=list_outputs.__getitem__)\n",
    "\n",
    "    print(f\"Input : {clip} Predicted : {labels[index_max]}\")\n",
    "\n",
    "    clip = clip.split(\"_\")[0]\n",
    "\n",
    "    if clip == labels[index_max]:\n",
    "        correct = correct+1\n",
    "    \n",
    "    num_clip = num_clip + 1 \n",
    "\n",
    "print(f\"Correct Predicted on Training set : {correct} Corrct percentage : {correct*100/num_clip}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os \n",
    "# video_dir = \"C:/Users/araya/Desktop/augments\"\n",
    "# video_list = []\n",
    "# video_list = os.listdir(video_dir)\n",
    "\n",
    "# len(video_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "video_dir = \"C:/Users/araya/Desktop/vscode/Deep-learning-coding/MediaPipe/Test\"\n",
    "video_list = []\n",
    "video_list = os.listdir(video_dir)\n",
    "\n",
    "len(video_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for exported data, numpy arrays\n",
    "Model_Data=os.path.join('../MediaPipe/Test')\n",
    "\n",
    "actions_test = np.array(video_list)\n",
    "\n",
    "no_of_seqs = 1\n",
    "\n",
    "# 30 frames in length\n",
    "seq_length = 160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_test = [action.split(\".\")[0] for action in actions_test]\n",
    "labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = []\n",
    "for action in actions_test:\n",
    "    video_path = os.path.join('Test/', action)\n",
    "    # print(video_path)\n",
    "    # print(action)\n",
    "    file_paths.append(video_path + '/' + action.split(\".\")[0] + \".npy\")\n",
    "print(file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "num_clip = 0\n",
    "for clip in labels_test:\n",
    "    # print(clip)\n",
    "    file_paths = [f\"Test/{clip}.mp4/{clip}.npy\"]\n",
    "    # print(file_paths)\n",
    "\n",
    "    sequences = load_keypoint_sequences(file_paths)\n",
    "    # Change list to numpy array \n",
    "    sequences = np.array(sequences)\n",
    "    # Change numpy array to tensor\n",
    "    sequences = torch.FloatTensor(sequences)\n",
    "    sequences = pad_sequence(sequences, batch_first=True)\n",
    "\n",
    "    outputs = model(sequences.to(device))\n",
    "\n",
    "    for idx, word in enumerate(outputs):\n",
    "        # max_value = torch.max(outputs)\n",
    "        list_outputs = max(outputs)\n",
    "\n",
    "    index_max = max(range(len(list_outputs)), key=list_outputs.__getitem__)\n",
    "\n",
    "    print(f\"Input : {clip} Predicted : {labels[index_max]}\")\n",
    "\n",
    "    clip = clip.split(\"_\")[0]\n",
    "\n",
    "    if clip == labels[index_max]:\n",
    "        correct = correct+1\n",
    "    \n",
    "    num_clip = num_clip + 1 \n",
    "\n",
    "print(f\"Correct Predicted on Training set : {correct} Corrct percentage : {correct*100/num_clip}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), 'saved_data/attention_lstm.pt')\n",
    "# torch.save(optimizer.state_dict(), 'saved_data/optimizer.pt')\n",
    "# # torch.save(scheduler.state_dict(), 'saved_data/scheduler.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real-Time Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 683,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 684,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holist = mp.solutions.holistic \n",
    "mp_draw = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 685,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(img, model):\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img.flags.writeable = False                 \n",
    "    result = model.process(img)                 # Make prediction\n",
    "    img.flags.writeable = True                   \n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR) \n",
    "    return img, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 686,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks(img, result):\n",
    "    mp_draw.draw_landmarks(img, result.face_landmarks, mp_holist.FACEMESH_CONTOURS) # Draw face connections\n",
    "    mp_draw.draw_landmarks(img, result.pose_landmarks, mp_holist.POSE_CONNECTIONS) # Draw pose connections\n",
    "    mp_draw.draw_landmarks(img, result.left_hand_landmarks, mp_holist.HAND_CONNECTIONS) # Draw left hand connections\n",
    "    mp_draw.draw_landmarks(img, result.right_hand_landmarks, mp_holist.HAND_CONNECTIONS) # Draw right hand connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 687,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_styled_landmarks(img, result):\n",
    "    mp_draw.draw_landmarks(img, result.face_landmarks, mp_holist.FACEMESH_CONTOURS, \n",
    "                             mp_draw.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1), # color the joint \n",
    "                             mp_draw.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1) #color the connection\n",
    "                             ) \n",
    "    # mp_draw.draw_landmarks(img, result.face_landmarks, mp_holist.FACEMESH_CONTOURS, \n",
    "    #                          mp_draw.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1), # color the joint \n",
    "    #                          mp_draw.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1) #color the connection\n",
    "    #                          ) \n",
    "    \n",
    "    mp_draw.draw_landmarks(img, result.pose_landmarks, mp_holist.POSE_CONNECTIONS,\n",
    "                             mp_draw.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4), \n",
    "                             mp_draw.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    mp_draw.draw_landmarks(img, result.left_hand_landmarks, mp_holist.HAND_CONNECTIONS, \n",
    "                             mp_draw.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4), \n",
    "                             mp_draw.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    mp_draw.draw_landmarks(img, result.right_hand_landmarks, mp_holist.HAND_CONNECTIONS, \n",
    "                             mp_draw.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4), \n",
    "                             mp_draw.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                             ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 688,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(results):\n",
    "    pose=np.array([[res.x,res.y] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*2)\n",
    "    left_hnd=np.array([[res.x,res.y] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*2)\n",
    "    right_hnd=np.array([[res.x,res.y] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*2)\n",
    "    face=np.array([[res.x,res.y] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*2)\n",
    "    return np.concatenate([pose,left_hnd,right_hnd,face])\n",
    "# concatenating for the model to detect the sign language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize MediaPipe holistic model\n",
    "# mp_holistic = mp.solutions.holistic\n",
    "# holistic = mp_holistic.Holistic()\n",
    "\n",
    "# # Initialize OpenCV video capture\n",
    "# cap = cv2.VideoCapture(0)\n",
    "\n",
    "# sequence = []\n",
    "# sequence_length = 160\n",
    "\n",
    "# while cap.isOpened():\n",
    "#     ret, frame = cap.read()\n",
    "#     if not ret:\n",
    "#         break\n",
    "    \n",
    "#     # Convert the image to RGB (MediaPipe expects RGB images)\n",
    "#     image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "#     # Process the frame with MediaPipe\n",
    "#     results = holistic.process(image_rgb)\n",
    "    \n",
    "#     # Extract keypoints (face, hands, and pose landmarks)\n",
    "#     # Assume extract_keypoints function converts keypoints into a flattened vector of size 1662\n",
    "#     keypoints = extract_keypoints(results) \n",
    "    \n",
    "#     # Append keypoints to the sequence buffer\n",
    "#     sequence.append(keypoints)\n",
    "    \n",
    "#     # Ensure the sequence only keeps the last 160 frames\n",
    "#     if len(sequence) > sequence_length:\n",
    "#         sequence.pop(0)\n",
    "    \n",
    "#     if len(sequence) == sequence_length:\n",
    "#         # Convert sequence to tensor\n",
    "#         input_tensor = torch.tensor([sequence], dtype=torch.float32)\n",
    "        \n",
    "#         # Make predictions\n",
    "#         with torch.no_grad():\n",
    "#             prediction = model(input_tensor.to(device))\n",
    "        \n",
    "#         # Decode and display the prediction\n",
    "#         predicted_class = torch.argmax(prediction, dim=1).item()\n",
    "#         print(f\"Predicted Class: {predicted_class}\")\n",
    "    \n",
    "#     # Display the video frame\n",
    "#     cv2.imshow('Real-time Sign Language Recognition', frame)\n",
    "    \n",
    "#     if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "#         break\n",
    "\n",
    "# cap.release()\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to apply CLAHE for light normalization\n",
    "# def apply_clahe(frame):\n",
    "#     # Convert to grayscale\n",
    "#     gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "#     # Apply CLAHE (Contrast Limited Adaptive Histogram Equalization)\n",
    "#     clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "#     equalized = clahe.apply(gray)\n",
    "    \n",
    "#     # Convert back to BGR after CLAHE\n",
    "#     frame_clahe = cv2.cvtColor(equalized, cv2.COLOR_GRAY2BGR)\n",
    "    \n",
    "#     return frame_clahe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rembg import remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.5  # Confidence threshold for prediction\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "sequence = []\n",
    "sequence_length = 60\n",
    "# Set mediapipe model \n",
    "with mp_holist.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.01,static_image_mode=False, \n",
    "    model_complexity=1,) as holistic:\n",
    "    while cap.isOpened():\n",
    "\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # frame = apply_clahe(frame)\n",
    "\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        # print(results)\n",
    "        \n",
    "        keypoints = extract_keypoints(results)\n",
    "\n",
    "        if keypoints.any():\n",
    "            sequence.append(keypoints)\n",
    "\n",
    "            if len(sequence) > sequence_length:\n",
    "                sequence = []\n",
    "        \n",
    "            if len(sequence) == sequence_length:\n",
    "                # Convert sequence to tensor\n",
    "                input_tensor = torch.tensor([sequence], dtype=torch.float32)\n",
    "                sequence = []\n",
    "            \n",
    "                # Make predictions\n",
    "                with torch.no_grad():\n",
    "                    prediction = model(input_tensor.to(device))\n",
    "            \n",
    "                # Get predicted class\n",
    "                predicted_class_idx = torch.argmax(prediction, dim=1).item()\n",
    "                predicted_class_label = labels[predicted_class_idx]\n",
    "                confidence = torch.softmax(prediction, dim=1)[0][predicted_class_idx].item()\n",
    "\n",
    "                # Display prediction result\n",
    "                if confidence > threshold:\n",
    "                    cv2.putText(image, f'{predicted_class_label} ({confidence:.2f})', (50, 50),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "                    print(predicted_class_label)\n",
    "        \n",
    "        draw_styled_landmarks(image, results)\n",
    "\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 681,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save & Load Model Command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1061,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), 'saved_data/attention_lstm.pt')\n",
    "# torch.save(optimizer.state_dict(), 'saved_data/optimizer.pt')\n",
    "# # torch.save(scheduler.state_dict(), 'saved_data/scheduler.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Load model\n",
    "# model.load_state_dict(torch.load('saved_data/model_name.pt'))\n",
    "# optimizer.load_state_dict(torch.load('saved_data/optimizer_namae.pt'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
