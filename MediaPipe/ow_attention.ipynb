{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video Classification using LSTM + Attention layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp \n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holist = mp.solutions.holistic \n",
    "mp_draw = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(img, model):\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img.flags.writeable = False                 \n",
    "    result = model.process(img)                 # Make prediction\n",
    "    img.flags.writeable = True                   \n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR) \n",
    "    return img, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks(img, result):\n",
    "    mp_draw.draw_landmarks(img, result.face_landmarks, mp_holist.FACEMESH_CONTOURS) # Draw face connections\n",
    "    mp_draw.draw_landmarks(img, result.pose_landmarks, mp_holist.POSE_CONNECTIONS) # Draw pose connections\n",
    "    mp_draw.draw_landmarks(img, result.left_hand_landmarks, mp_holist.HAND_CONNECTIONS) # Draw left hand connections\n",
    "    mp_draw.draw_landmarks(img, result.right_hand_landmarks, mp_holist.HAND_CONNECTIONS) # Draw right hand connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_styled_landmarks(img, result):\n",
    "    mp_draw.draw_landmarks(img, result.face_landmarks, mp_holist.FACEMESH_CONTOURS, \n",
    "                             mp_draw.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1), # color the joint \n",
    "                             mp_draw.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1) #color the connection\n",
    "                             ) \n",
    "    # mp_draw.draw_landmarks(img, result.face_landmarks, mp_holist.FACEMESH_CONTOURS, \n",
    "    #                          mp_draw.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1), # color the joint \n",
    "    #                          mp_draw.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1) #color the connection\n",
    "    #                          ) \n",
    "    \n",
    "    mp_draw.draw_landmarks(img, result.pose_landmarks, mp_holist.POSE_CONNECTIONS,\n",
    "                             mp_draw.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4), \n",
    "                             mp_draw.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    mp_draw.draw_landmarks(img, result.left_hand_landmarks, mp_holist.HAND_CONNECTIONS, \n",
    "                             mp_draw.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4), \n",
    "                             mp_draw.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    mp_draw.draw_landmarks(img, result.right_hand_landmarks, mp_holist.HAND_CONNECTIONS, \n",
    "                             mp_draw.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4), \n",
    "                             mp_draw.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                             ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(results):\n",
    "    pose=np.array([[res.x,res.y,res.z,res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    left_hnd=np.array([[res.x,res.y,res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    right_hnd=np.array([[res.x,res.y,res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    face=np.array([[res.x,res.y,res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "    return np.concatenate([pose,left_hnd,right_hnd,face])\n",
    "# concatenating for the model to detect the sign language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "video_dir = \"C:/Users/araya/Desktop/keypoints/video_extract\"\n",
    "video_list = []\n",
    "video_list = os.listdir(video_dir)\n",
    "\n",
    "len(video_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['กฎกระทรวง.mp4',\n",
       " 'กฎหมายรัฐธรรมนูญ.mp4',\n",
       " 'กรมอนามัย.mp4',\n",
       " 'กรรม.mp4',\n",
       " 'กรรมสิทธิ์.mp4',\n",
       " 'กระโดด.mp4',\n",
       " 'กล้วยบวชชี.mp4',\n",
       " 'กล้วยเชื่อม.mp4',\n",
       " 'กังวล.mp4',\n",
       " 'กีฬา.mp4',\n",
       " 'น้อง.mp4',\n",
       " 'เขิน.mp4',\n",
       " 'เขื่อนดิน.mp4',\n",
       " 'เขื่อนสิริกิติ์.mp4',\n",
       " 'เข้าใจผิด.mp4',\n",
       " 'เคย.mp4',\n",
       " 'เครียด.mp4',\n",
       " 'เครื่องปั่นดิน.mp4',\n",
       " 'เครื่องหมายการค้า.mp4',\n",
       " 'เจอ.mp4',\n",
       " 'เจ้าหนี้.mp4',\n",
       " 'เช่าซื้อ.mp4',\n",
       " 'เช่าทรัพย์.mp4',\n",
       " 'เซอร์เบีย.mp4',\n",
       " 'เซเนกัล.mp4',\n",
       " 'เซ็ง.mp4',\n",
       " 'เดิน.mp4',\n",
       " 'เดิมพัน.mp4',\n",
       " 'เพลีย.mp4',\n",
       " 'เมื่อย.mp4',\n",
       " 'เม็กซิโก.mp4',\n",
       " 'เฮโรอีน.mp4',\n",
       " 'แกมเบีย.mp4',\n",
       " 'แซมเบีย.mp4',\n",
       " 'โกหก.mp4',\n",
       " 'โจทก์.mp4',\n",
       " 'โชจู.mp4',\n",
       " 'ใกล้.mp4',\n",
       " 'ไดโนเสาร์.mp4',\n",
       " 'ไอซ์.mp4']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for exported data, numpy arrays\n",
    "Model_Data=os.path.join('Data for different actions')\n",
    "\n",
    "actions = np.array(video_list)\n",
    "\n",
    "no_of_seqs = 1\n",
    "\n",
    "# 30 frames in length\n",
    "seq_length = 160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['กฎกระทรวง.mp4', 'กฎหมายรัฐธรรมนูญ.mp4', 'กรมอนามัย.mp4',\n",
       "       'กรรม.mp4', 'กรรมสิทธิ์.mp4', 'กระโดด.mp4', 'กล้วยบวชชี.mp4',\n",
       "       'กล้วยเชื่อม.mp4', 'กังวล.mp4', 'กีฬา.mp4', 'น้อง.mp4', 'เขิน.mp4',\n",
       "       'เขื่อนดิน.mp4', 'เขื่อนสิริกิติ์.mp4', 'เข้าใจผิด.mp4', 'เคย.mp4',\n",
       "       'เครียด.mp4', 'เครื่องปั่นดิน.mp4', 'เครื่องหมายการค้า.mp4',\n",
       "       'เจอ.mp4', 'เจ้าหนี้.mp4', 'เช่าซื้อ.mp4', 'เช่าทรัพย์.mp4',\n",
       "       'เซอร์เบีย.mp4', 'เซเนกัล.mp4', 'เซ็ง.mp4', 'เดิน.mp4',\n",
       "       'เดิมพัน.mp4', 'เพลีย.mp4', 'เมื่อย.mp4', 'เม็กซิโก.mp4',\n",
       "       'เฮโรอีน.mp4', 'แกมเบีย.mp4', 'แซมเบีย.mp4', 'โกหก.mp4',\n",
       "       'โจทก์.mp4', 'โชจู.mp4', 'ใกล้.mp4', 'ไดโนเสาร์.mp4', 'ไอซ์.mp4'],\n",
       "      dtype='<U21')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collecting keypoint values for Training nd Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory where your videos are stored\n",
    "directory = \"C:/Users/araya/Desktop/keypoints/video_extract\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:/Users/araya/Desktop/keypoints/video_extract'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/araya/Desktop/keypoints/video_extract/กฎกระทรวง.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/กฎหมายรัฐธรรมนูญ.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/กรมอนามัย.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/กรรม.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/กรรมสิทธิ์.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/กระโดด.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/กล้วยบวชชี.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/กล้วยเชื่อม.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/กังวล.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/กีฬา.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/น้อง.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/เขิน.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/เขื่อนดิน.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/เขื่อนสิริกิติ์.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/เข้าใจผิด.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/เคย.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/เครียด.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/เครื่องปั่นดิน.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/เครื่องหมายการค้า.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/เจอ.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/เจ้าหนี้.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/เช่าซื้อ.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/เช่าทรัพย์.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/เซอร์เบีย.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/เซเนกัล.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/เซ็ง.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/เดิน.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/เดิมพัน.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/เพลีย.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/เมื่อย.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/เม็กซิโก.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/เฮโรอีน.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/แกมเบีย.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/แซมเบีย.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/โกหก.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/โจทก์.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/โชจู.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/ใกล้.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/ไดโนเสาร์.mp4\n",
      "C:/Users/araya/Desktop/keypoints/video_extract/ไอซ์.mp4\n"
     ]
    }
   ],
   "source": [
    "for filename in actions:\n",
    "    print(directory + '/' + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Data for different actions/กฎกระทรวง.mp4/กฎกระทรวง.npy', 'Data for different actions/กฎหมายรัฐธรรมนูญ.mp4/กฎหมายรัฐธรรมนูญ.npy', 'Data for different actions/กรมอนามัย.mp4/กรมอนามัย.npy', 'Data for different actions/กรรม.mp4/กรรม.npy', 'Data for different actions/กรรมสิทธิ์.mp4/กรรมสิทธิ์.npy', 'Data for different actions/กระโดด.mp4/กระโดด.npy', 'Data for different actions/กล้วยบวชชี.mp4/กล้วยบวชชี.npy', 'Data for different actions/กล้วยเชื่อม.mp4/กล้วยเชื่อม.npy', 'Data for different actions/กังวล.mp4/กังวล.npy', 'Data for different actions/กีฬา.mp4/กีฬา.npy', 'Data for different actions/น้อง.mp4/น้อง.npy', 'Data for different actions/เขิน.mp4/เขิน.npy', 'Data for different actions/เขื่อนดิน.mp4/เขื่อนดิน.npy', 'Data for different actions/เขื่อนสิริกิติ์.mp4/เขื่อนสิริกิติ์.npy', 'Data for different actions/เข้าใจผิด.mp4/เข้าใจผิด.npy', 'Data for different actions/เคย.mp4/เคย.npy', 'Data for different actions/เครียด.mp4/เครียด.npy', 'Data for different actions/เครื่องปั่นดิน.mp4/เครื่องปั่นดิน.npy', 'Data for different actions/เครื่องหมายการค้า.mp4/เครื่องหมายการค้า.npy', 'Data for different actions/เจอ.mp4/เจอ.npy', 'Data for different actions/เจ้าหนี้.mp4/เจ้าหนี้.npy', 'Data for different actions/เช่าซื้อ.mp4/เช่าซื้อ.npy', 'Data for different actions/เช่าทรัพย์.mp4/เช่าทรัพย์.npy', 'Data for different actions/เซอร์เบีย.mp4/เซอร์เบีย.npy', 'Data for different actions/เซเนกัล.mp4/เซเนกัล.npy', 'Data for different actions/เซ็ง.mp4/เซ็ง.npy', 'Data for different actions/เดิน.mp4/เดิน.npy', 'Data for different actions/เดิมพัน.mp4/เดิมพัน.npy', 'Data for different actions/เพลีย.mp4/เพลีย.npy', 'Data for different actions/เมื่อย.mp4/เมื่อย.npy', 'Data for different actions/เม็กซิโก.mp4/เม็กซิโก.npy', 'Data for different actions/เฮโรอีน.mp4/เฮโรอีน.npy', 'Data for different actions/แกมเบีย.mp4/แกมเบีย.npy', 'Data for different actions/แซมเบีย.mp4/แซมเบีย.npy', 'Data for different actions/โกหก.mp4/โกหก.npy', 'Data for different actions/โจทก์.mp4/โจทก์.npy', 'Data for different actions/โชจู.mp4/โชจู.npy', 'Data for different actions/ใกล้.mp4/ใกล้.npy', 'Data for different actions/ไดโนเสาร์.mp4/ไดโนเสาร์.npy', 'Data for different actions/ไอซ์.mp4/ไอซ์.npy']\n"
     ]
    }
   ],
   "source": [
    "file_paths = []\n",
    "for action in actions:\n",
    "    video_path = os.path.join('Data for different actions/', action)\n",
    "    # print(video_path)\n",
    "    # print(action)\n",
    "    file_paths.append(video_path + '/' + action.split(\".\")[0] + \".npy\")\n",
    "print(file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_keypoint_sequences(file_paths):\n",
    "    keypoint_sequences = []\n",
    "    for file_path in file_paths:\n",
    "        keypoints = np.load(file_path)\n",
    "        keypoint_sequences.append(torch.tensor(keypoints, dtype=torch.float32))\n",
    "    return keypoint_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 0.5013,  0.2452, -1.2167,  ...,  0.5663,  0.2188,  0.0098],\n",
       "         [ 0.4997,  0.2482, -1.4690,  ...,  0.5652,  0.2181,  0.0106],\n",
       "         [ 0.4984,  0.2500, -1.4853,  ...,  0.5654,  0.2185,  0.0112],\n",
       "         ...,\n",
       "         [ 0.4861,  0.2513, -1.3416,  ...,  0.5572,  0.2177,  0.0091],\n",
       "         [ 0.4873,  0.2514, -1.3574,  ...,  0.5575,  0.2172,  0.0097],\n",
       "         [ 0.4883,  0.2516, -1.3579,  ...,  0.5577,  0.2170,  0.0101]]),\n",
       " tensor([[ 0.4922,  0.2382, -1.2850,  ...,  0.5578,  0.2124,  0.0094],\n",
       "         [ 0.4920,  0.2405, -1.4288,  ...,  0.5571,  0.2116,  0.0099],\n",
       "         [ 0.4920,  0.2409, -1.4093,  ...,  0.5567,  0.2122,  0.0098],\n",
       "         ...,\n",
       "         [ 0.4814,  0.2260, -1.3318,  ...,  0.5503,  0.1923,  0.0123],\n",
       "         [ 0.4815,  0.2257, -1.3351,  ...,  0.5503,  0.1921,  0.0122],\n",
       "         [ 0.4815,  0.2255, -1.3497,  ...,  0.5501,  0.1919,  0.0124]]),\n",
       " tensor([[ 0.5049,  0.2371, -1.2115,  ...,  0.5643,  0.2082,  0.0088],\n",
       "         [ 0.5045,  0.2381, -1.1896,  ...,  0.5643,  0.2081,  0.0085],\n",
       "         [ 0.5041,  0.2385, -1.1915,  ...,  0.5643,  0.2080,  0.0089],\n",
       "         ...,\n",
       "         [ 0.4963,  0.2307, -1.3096,  ...,  0.5624,  0.2043,  0.0127],\n",
       "         [ 0.4965,  0.2308, -1.3085,  ...,  0.5625,  0.2045,  0.0126],\n",
       "         [ 0.4972,  0.2312, -1.3053,  ...,  0.5633,  0.2045,  0.0128]]),\n",
       " tensor([[ 0.5134,  0.2614, -1.4426,  ...,  0.5818,  0.2272,  0.0153],\n",
       "         [ 0.5130,  0.2604, -1.4262,  ...,  0.5810,  0.2273,  0.0147],\n",
       "         [ 0.5126,  0.2599, -1.4278,  ...,  0.5810,  0.2276,  0.0150],\n",
       "         ...,\n",
       "         [ 0.5079,  0.2693, -1.4999,  ...,  0.5780,  0.2349,  0.0115],\n",
       "         [ 0.5090,  0.2688, -1.4936,  ...,  0.5782,  0.2346,  0.0116],\n",
       "         [ 0.5092,  0.2683, -1.4518,  ...,  0.5786,  0.2341,  0.0116]]),\n",
       " tensor([[ 0.4883,  0.2402, -1.1024,  ...,  0.5482,  0.2132,  0.0081],\n",
       "         [ 0.4878,  0.2402, -1.1906,  ...,  0.5469,  0.2135,  0.0083],\n",
       "         [ 0.4863,  0.2402, -1.1774,  ...,  0.5477,  0.2141,  0.0087],\n",
       "         ...,\n",
       "         [ 0.4788,  0.3129, -1.6072,  ...,  0.5349,  0.2531,  0.0022],\n",
       "         [ 0.4782,  0.3129, -1.6350,  ...,  0.5344,  0.2525,  0.0022],\n",
       "         [ 0.4771,  0.3131, -1.6312,  ...,  0.5339,  0.2515,  0.0022]]),\n",
       " tensor([[ 0.4992,  0.1994, -1.1906,  ...,  0.5657,  0.1731,  0.0116],\n",
       "         [ 0.4988,  0.2047, -1.3590,  ...,  0.5663,  0.1723,  0.0123],\n",
       "         [ 0.4982,  0.2082, -1.3140,  ...,  0.5671,  0.1730,  0.0129],\n",
       "         ...,\n",
       "         [ 0.4743,  0.1974, -1.3230,  ...,  0.5475,  0.1620,  0.0155],\n",
       "         [ 0.4732,  0.1974, -1.3174,  ...,  0.5461,  0.1615,  0.0151],\n",
       "         [ 0.4720,  0.1974, -1.3149,  ...,  0.5453,  0.1614,  0.0149]]),\n",
       " tensor([[ 0.5023,  0.2809, -1.6242,  ...,  0.5847,  0.2321,  0.0112],\n",
       "         [ 0.5023,  0.2806, -1.6631,  ...,  0.5840,  0.2322,  0.0130],\n",
       "         [ 0.5022,  0.2805, -1.6912,  ...,  0.5837,  0.2322,  0.0122],\n",
       "         ...,\n",
       "         [ 0.5037,  0.2791, -1.5789,  ...,  0.5834,  0.2224,  0.0129],\n",
       "         [ 0.5044,  0.2774, -1.5686,  ...,  0.5831,  0.2220,  0.0130],\n",
       "         [ 0.5052,  0.2722, -1.5527,  ...,  0.5827,  0.2216,  0.0132]]),\n",
       " tensor([[ 0.4868,  0.2821, -1.4668,  ...,  0.5711,  0.2335,  0.0116],\n",
       "         [ 0.4861,  0.2786, -1.5812,  ...,  0.5702,  0.2334,  0.0129],\n",
       "         [ 0.4856,  0.2769, -1.6059,  ...,  0.5699,  0.2333,  0.0131],\n",
       "         ...,\n",
       "         [ 0.4826,  0.2583, -1.6140,  ...,  0.5606,  0.2261,  0.0154],\n",
       "         [ 0.4820,  0.2583, -1.5231,  ...,  0.5603,  0.2261,  0.0156],\n",
       "         [ 0.4818,  0.2580, -1.5255,  ...,  0.5600,  0.2263,  0.0156]]),\n",
       " tensor([[ 0.5016,  0.2321, -1.1859,  ...,  0.5647,  0.2139,  0.0072],\n",
       "         [ 0.5015,  0.2340, -1.1996,  ...,  0.5645,  0.2135,  0.0076],\n",
       "         [ 0.5016,  0.2346, -1.2272,  ...,  0.5644,  0.2137,  0.0082],\n",
       "         ...,\n",
       "         [ 0.4743,  0.2622, -1.4181,  ...,  0.5499,  0.2133,  0.0080],\n",
       "         [ 0.4763,  0.2599, -1.4742,  ...,  0.5519,  0.2118,  0.0090],\n",
       "         [ 0.4787,  0.2522, -1.6440,  ...,  0.5537,  0.2110,  0.0093]]),\n",
       " tensor([[ 0.4926,  0.1945, -1.2354,  ...,  0.5488,  0.1581,  0.0085],\n",
       "         [ 0.4942,  0.1949, -1.4254,  ...,  0.5481,  0.1576,  0.0085],\n",
       "         [ 0.4948,  0.1960, -1.4483,  ...,  0.5479,  0.1581,  0.0084],\n",
       "         ...,\n",
       "         [ 0.4917,  0.1882, -1.3355,  ...,  0.5458,  0.1536,  0.0126],\n",
       "         [ 0.4918,  0.1885, -1.3154,  ...,  0.5456,  0.1536,  0.0125],\n",
       "         [ 0.4918,  0.1886, -1.3147,  ...,  0.5453,  0.1537,  0.0124]]),\n",
       " tensor([[ 0.4956,  0.2681, -1.1361,  ...,  0.5554,  0.2332,  0.0147],\n",
       "         [ 0.4948,  0.2681, -1.3768,  ...,  0.5540,  0.2333,  0.0116],\n",
       "         [ 0.4943,  0.2681, -1.3754,  ...,  0.5538,  0.2332,  0.0116],\n",
       "         ...,\n",
       "         [ 0.4895,  0.2689, -1.2187,  ...,  0.5513,  0.2276,  0.0135],\n",
       "         [ 0.4889,  0.2688, -1.2198,  ...,  0.5508,  0.2276,  0.0134],\n",
       "         [ 0.4882,  0.2688, -1.2172,  ...,  0.5507,  0.2276,  0.0134]]),\n",
       " tensor([[ 0.5032,  0.2228, -1.1998,  ...,  0.5663,  0.1971,  0.0106],\n",
       "         [ 0.5035,  0.2228, -1.2072,  ...,  0.5662,  0.1969,  0.0106],\n",
       "         [ 0.5035,  0.2230, -1.2082,  ...,  0.5664,  0.1963,  0.0108],\n",
       "         ...,\n",
       "         [ 0.4982,  0.2206, -1.2743,  ...,  0.5578,  0.1892,  0.0092],\n",
       "         [ 0.4983,  0.2178, -1.2380,  ...,  0.5581,  0.1883,  0.0091],\n",
       "         [ 0.4979,  0.2173, -1.2264,  ...,  0.5578,  0.1877,  0.0096]]),\n",
       " tensor([[ 0.5442,  0.2507, -1.4443,  ...,  0.6105,  0.2203,  0.0123],\n",
       "         [ 0.5444,  0.2508, -1.6574,  ...,  0.6094,  0.2198,  0.0134],\n",
       "         [ 0.5445,  0.2510, -1.7250,  ...,  0.6093,  0.2195,  0.0144],\n",
       "         ...,\n",
       "         [ 0.5324,  0.2577, -1.6704,  ...,  0.5975,  0.2210,  0.0145],\n",
       "         [ 0.5323,  0.2560, -1.6673,  ...,  0.5966,  0.2206,  0.0148],\n",
       "         [ 0.5323,  0.2553, -1.6826,  ...,  0.5957,  0.2201,  0.0146]]),\n",
       " tensor([[ 0.5402,  0.2562, -1.5458,  ...,  0.6041,  0.2257,  0.0125],\n",
       "         [ 0.5389,  0.2596, -1.7020,  ...,  0.6031,  0.2264,  0.0130],\n",
       "         [ 0.5379,  0.2616, -1.7134,  ...,  0.6027,  0.2258,  0.0139],\n",
       "         ...,\n",
       "         [ 0.5153,  0.2631, -1.6030,  ...,  0.5956,  0.2125,  0.0142],\n",
       "         [ 0.5176,  0.2626, -1.6101,  ...,  0.5985,  0.2126,  0.0139],\n",
       "         [ 0.5197,  0.2624, -1.5662,  ...,  0.6011,  0.2119,  0.0145]]),\n",
       " tensor([[ 0.5030,  0.2553, -1.1988,  ...,  0.5699,  0.2265,  0.0097],\n",
       "         [ 0.5028,  0.2582, -1.2761,  ...,  0.5689,  0.2266,  0.0109],\n",
       "         [ 0.5028,  0.2605, -1.3315,  ...,  0.5690,  0.2269,  0.0105],\n",
       "         ...,\n",
       "         [ 0.5028,  0.2675, -1.4499,  ...,  0.5672,  0.2323,  0.0110],\n",
       "         [ 0.5007,  0.2672, -1.4234,  ...,  0.5668,  0.2317,  0.0115],\n",
       "         [ 0.4988,  0.2671, -1.4308,  ...,  0.5670,  0.2312,  0.0118]]),\n",
       " tensor([[ 0.5069,  0.2355, -1.3384,  ...,  0.5700,  0.2051,  0.0053],\n",
       "         [ 0.5045,  0.2395, -1.5097,  ...,  0.5699,  0.2043,  0.0064],\n",
       "         [ 0.5028,  0.2420, -1.5081,  ...,  0.5699,  0.2045,  0.0069],\n",
       "         ...,\n",
       "         [ 0.4952,  0.2462, -1.4319,  ...,  0.5658,  0.2015,  0.0122],\n",
       "         [ 0.4952,  0.2448, -1.4781,  ...,  0.5661,  0.2014,  0.0123],\n",
       "         [ 0.4953,  0.2436, -1.4689,  ...,  0.5662,  0.2013,  0.0124]]),\n",
       " tensor([[ 0.5035,  0.2282, -1.1379,  ...,  0.5710,  0.1953,  0.0120],\n",
       "         [ 0.5033,  0.2284, -1.2771,  ...,  0.5713,  0.1954,  0.0123],\n",
       "         [ 0.5032,  0.2286, -1.2743,  ...,  0.5715,  0.1957,  0.0122],\n",
       "         ...,\n",
       "         [ 0.4810,  0.2296, -1.4129,  ...,  0.5542,  0.1897,  0.0133],\n",
       "         [ 0.4812,  0.2296, -1.4275,  ...,  0.5549,  0.1900,  0.0130],\n",
       "         [ 0.4816,  0.2299, -1.4337,  ...,  0.5557,  0.1903,  0.0133]]),\n",
       " tensor([[ 0.5108,  0.2425, -1.1053,  ...,  0.5724,  0.2176,  0.0127],\n",
       "         [ 0.5091,  0.2430, -1.3007,  ...,  0.5713,  0.2177,  0.0129],\n",
       "         [ 0.5080,  0.2432, -1.3035,  ...,  0.5714,  0.2179,  0.0130],\n",
       "         ...,\n",
       "         [ 0.4966,  0.2620, -1.5367,  ...,  0.5653,  0.2259,  0.0082],\n",
       "         [ 0.4968,  0.2621, -1.5419,  ...,  0.5656,  0.2262,  0.0083],\n",
       "         [ 0.4971,  0.2623, -1.5482,  ...,  0.5658,  0.2263,  0.0086]]),\n",
       " tensor([[ 0.4878,  0.2235, -1.2515,  ...,  0.5511,  0.1981,  0.0114],\n",
       "         [ 0.4870,  0.2286, -1.4279,  ...,  0.5506,  0.1977,  0.0105],\n",
       "         [ 0.4865,  0.2315, -1.4431,  ...,  0.5508,  0.1984,  0.0110],\n",
       "         ...,\n",
       "         [ 0.4901,  0.2217, -1.3449,  ...,  0.5576,  0.1888,  0.0156],\n",
       "         [ 0.4898,  0.2224, -1.3780,  ...,  0.5575,  0.1892,  0.0157],\n",
       "         [ 0.4896,  0.2232, -1.4140,  ...,  0.5572,  0.1897,  0.0156]]),\n",
       " tensor([[ 0.5194,  0.2227, -1.3410,  ...,  0.5911,  0.1970,  0.0142],\n",
       "         [ 0.5200,  0.2228, -1.3298,  ...,  0.5912,  0.1965,  0.0131],\n",
       "         [ 0.5202,  0.2230, -1.3074,  ...,  0.5909,  0.1970,  0.0149],\n",
       "         ...,\n",
       "         [ 0.5146,  0.2198, -1.3267,  ...,  0.5821,  0.1907,  0.0159],\n",
       "         [ 0.5141,  0.2205, -1.3207,  ...,  0.5816,  0.1911,  0.0154],\n",
       "         [ 0.5137,  0.2209, -1.3223,  ...,  0.5812,  0.1917,  0.0161]]),\n",
       " tensor([[ 0.5103,  0.2428, -1.1660,  ...,  0.5757,  0.2179,  0.0081],\n",
       "         [ 0.5106,  0.2437, -1.2850,  ...,  0.5751,  0.2183,  0.0099],\n",
       "         [ 0.5112,  0.2443, -1.2880,  ...,  0.5748,  0.2189,  0.0108],\n",
       "         ...,\n",
       "         [ 0.4800,  0.2603, -1.3086,  ...,  0.5532,  0.2237,  0.0069],\n",
       "         [ 0.4807,  0.2604, -1.2544,  ...,  0.5539,  0.2238,  0.0067],\n",
       "         [ 0.4813,  0.2604, -1.2161,  ...,  0.5548,  0.2238,  0.0065]]),\n",
       " tensor([[ 0.4945,  0.2362, -1.1227,  ...,  0.5529,  0.2086,  0.0088],\n",
       "         [ 0.4947,  0.2363, -1.1264,  ...,  0.5528,  0.2094,  0.0088],\n",
       "         [ 0.4951,  0.2364, -1.1371,  ...,  0.5531,  0.2099,  0.0089],\n",
       "         ...,\n",
       "         [ 0.4933,  0.2480, -1.2424,  ...,  0.5563,  0.2184,  0.0091],\n",
       "         [ 0.4933,  0.2480, -1.2443,  ...,  0.5565,  0.2185,  0.0093],\n",
       "         [ 0.4932,  0.2480, -1.2944,  ...,  0.5564,  0.2185,  0.0095]]),\n",
       " tensor([[ 0.5038,  0.2425, -1.1550,  ...,  0.5665,  0.2136,  0.0080],\n",
       "         [ 0.5027,  0.2423, -1.2423,  ...,  0.5653,  0.2133,  0.0092],\n",
       "         [ 0.5018,  0.2423, -1.2565,  ...,  0.5649,  0.2130,  0.0094],\n",
       "         ...,\n",
       "         [ 0.4950,  0.2412, -1.1727,  ...,  0.5624,  0.2175,  0.0087],\n",
       "         [ 0.4951,  0.2412, -1.1690,  ...,  0.5625,  0.2177,  0.0082],\n",
       "         [ 0.4951,  0.2412, -1.1910,  ...,  0.5626,  0.2180,  0.0079]]),\n",
       " tensor([[ 0.4916,  0.2518, -1.3187,  ...,  0.5597,  0.2202,  0.0108],\n",
       "         [ 0.4907,  0.2519, -1.3672,  ...,  0.5580,  0.2209,  0.0105],\n",
       "         [ 0.4896,  0.2520, -1.4026,  ...,  0.5580,  0.2210,  0.0110],\n",
       "         ...,\n",
       "         [ 0.4850,  0.2552, -1.4312,  ...,  0.5509,  0.2199,  0.0138],\n",
       "         [ 0.4839,  0.2550, -1.4298,  ...,  0.5505,  0.2199,  0.0138],\n",
       "         [ 0.4831,  0.2549, -1.4164,  ...,  0.5501,  0.2200,  0.0137]]),\n",
       " tensor([[ 0.5051,  0.2347, -1.4390,  ...,  0.5717,  0.2094,  0.0091],\n",
       "         [ 0.5024,  0.2389, -1.5627,  ...,  0.5707,  0.2087,  0.0090],\n",
       "         [ 0.5005,  0.2417, -1.5896,  ...,  0.5704,  0.2090,  0.0098],\n",
       "         ...,\n",
       "         [ 0.4950,  0.2365, -1.4622,  ...,  0.5663,  0.2072,  0.0114],\n",
       "         [ 0.4950,  0.2366, -1.4772,  ...,  0.5667,  0.2075,  0.0115],\n",
       "         [ 0.4949,  0.2366, -1.4725,  ...,  0.5671,  0.2078,  0.0120]]),\n",
       " tensor([[ 0.5064,  0.2529, -1.3080,  ...,  0.5751,  0.2261,  0.0114],\n",
       "         [ 0.5055,  0.2550, -1.3556,  ...,  0.5752,  0.2257,  0.0106],\n",
       "         [ 0.5048,  0.2568, -1.3925,  ...,  0.5751,  0.2260,  0.0110],\n",
       "         ...,\n",
       "         [ 0.4960,  0.2624, -1.3587,  ...,  0.5642,  0.2311,  0.0101],\n",
       "         [ 0.4961,  0.2619, -1.3373,  ...,  0.5652,  0.2307,  0.0104],\n",
       "         [ 0.4961,  0.2615, -1.3405,  ...,  0.5653,  0.2302,  0.0102]]),\n",
       " tensor([[ 0.5267,  0.2322, -1.2986,  ...,  0.5921,  0.1996,  0.0123],\n",
       "         [ 0.5267,  0.2305, -1.3156,  ...,  0.5925,  0.1999,  0.0112],\n",
       "         [ 0.5266,  0.2296, -1.3147,  ...,  0.5928,  0.1997,  0.0121],\n",
       "         ...,\n",
       "         [ 0.5122,  0.2243, -1.2697,  ...,  0.5819,  0.1855,  0.0147],\n",
       "         [ 0.5114,  0.2239, -1.2687,  ...,  0.5817,  0.1857,  0.0145],\n",
       "         [ 0.5110,  0.2235, -1.2585,  ...,  0.5815,  0.1858,  0.0145]]),\n",
       " tensor([[ 0.5050,  0.2164, -1.1575,  ...,  0.5700,  0.1959,  0.0103],\n",
       "         [ 0.5041,  0.2187, -1.3621,  ...,  0.5689,  0.1959,  0.0100],\n",
       "         [ 0.5031,  0.2218, -1.3920,  ...,  0.5693,  0.1959,  0.0109],\n",
       "         ...,\n",
       "         [ 0.4987,  0.2252, -1.2609,  ...,  0.5655,  0.1932,  0.0101],\n",
       "         [ 0.4982,  0.2238, -1.2657,  ...,  0.5653,  0.1926,  0.0103],\n",
       "         [ 0.4969,  0.2230, -1.2967,  ...,  0.5649,  0.1919,  0.0105]]),\n",
       " tensor([[ 0.5097,  0.2301, -1.2305,  ...,  0.5773,  0.2070,  0.0135],\n",
       "         [ 0.5098,  0.2316, -1.4094,  ...,  0.5762,  0.2072,  0.0130],\n",
       "         [ 0.5099,  0.2328, -1.4219,  ...,  0.5754,  0.2074,  0.0134],\n",
       "         ...,\n",
       "         [ 0.5106,  0.2519, -1.1800,  ...,  0.5731,  0.2204,  0.0132],\n",
       "         [ 0.5082,  0.2514, -1.1958,  ...,  0.5717,  0.2193,  0.0134],\n",
       "         [ 0.5074,  0.2516, -1.2210,  ...,  0.5703,  0.2182,  0.0128]]),\n",
       " tensor([[ 0.5410,  0.2495, -1.3908,  ...,  0.6085,  0.2224,  0.0099],\n",
       "         [ 0.5399,  0.2502, -1.3960,  ...,  0.6066,  0.2219,  0.0111],\n",
       "         [ 0.5390,  0.2511, -1.3861,  ...,  0.6062,  0.2215,  0.0105],\n",
       "         ...,\n",
       "         [ 0.5131,  0.2461, -1.3199,  ...,  0.5857,  0.2158,  0.0118],\n",
       "         [ 0.5137,  0.2459, -1.3370,  ...,  0.5858,  0.2153,  0.0118],\n",
       "         [ 0.5141,  0.2456, -1.3376,  ...,  0.5860,  0.2150,  0.0116]]),\n",
       " tensor([[ 0.4813,  0.2267, -1.2462,  ...,  0.5499,  0.2013,  0.0123],\n",
       "         [ 0.4796,  0.2283, -1.4595,  ...,  0.5479,  0.2026,  0.0127],\n",
       "         [ 0.4786,  0.2300, -1.5108,  ...,  0.5480,  0.2025,  0.0122],\n",
       "         ...,\n",
       "         [ 0.4782,  0.2297, -1.2598,  ...,  0.5464,  0.1965,  0.0150],\n",
       "         [ 0.4781,  0.2300, -1.2686,  ...,  0.5459,  0.1960,  0.0147],\n",
       "         [ 0.4780,  0.2304, -1.2444,  ...,  0.5461,  0.1953,  0.0154]]),\n",
       " tensor([[ 0.5051,  0.2433, -1.3371,  ...,  0.5701,  0.2185,  0.0094],\n",
       "         [ 0.5042,  0.2464, -1.4811,  ...,  0.5696,  0.2193,  0.0109],\n",
       "         [ 0.5039,  0.2479, -1.4928,  ...,  0.5700,  0.2201,  0.0112],\n",
       "         ...,\n",
       "         [ 0.4965,  0.2517, -1.4369,  ...,  0.5645,  0.2217,  0.0130],\n",
       "         [ 0.4956,  0.2516, -1.4394,  ...,  0.5637,  0.2215,  0.0129],\n",
       "         [ 0.4943,  0.2514, -1.4394,  ...,  0.5628,  0.2207,  0.0124]]),\n",
       " tensor([[ 0.5269,  0.2488, -1.5232,  ...,  0.5929,  0.2181,  0.0118],\n",
       "         [ 0.5260,  0.2493, -1.4445,  ...,  0.5918,  0.2172,  0.0111],\n",
       "         [ 0.5250,  0.2497, -1.4588,  ...,  0.5916,  0.2171,  0.0113],\n",
       "         ...,\n",
       "         [ 0.5126,  0.2514, -1.5292,  ...,  0.5827,  0.2212,  0.0138],\n",
       "         [ 0.5125,  0.2512, -1.5273,  ...,  0.5813,  0.2204,  0.0136],\n",
       "         [ 0.5123,  0.2512, -1.5221,  ...,  0.5795,  0.2200,  0.0137]]),\n",
       " tensor([[ 0.5103,  0.2330, -1.4731,  ...,  0.5745,  0.2056,  0.0102],\n",
       "         [ 0.5071,  0.2360, -1.5453,  ...,  0.5731,  0.2055,  0.0103],\n",
       "         [ 0.5052,  0.2381, -1.5650,  ...,  0.5733,  0.2056,  0.0100],\n",
       "         ...,\n",
       "         [ 0.4875,  0.2316, -1.5252,  ...,  0.5606,  0.1993,  0.0132],\n",
       "         [ 0.4872,  0.2318, -1.5227,  ...,  0.5604,  0.1997,  0.0129],\n",
       "         [ 0.4870,  0.2318, -1.5220,  ...,  0.5604,  0.2000,  0.0129]]),\n",
       " tensor([[ 0.4952,  0.2338, -1.3905,  ...,  0.5593,  0.2078,  0.0128],\n",
       "         [ 0.4926,  0.2435, -1.4861,  ...,  0.5600,  0.2072,  0.0117],\n",
       "         [ 0.4916,  0.2483, -1.4905,  ...,  0.5596,  0.2079,  0.0126],\n",
       "         ...,\n",
       "         [ 0.4846,  0.2462, -1.4163,  ...,  0.5649,  0.2045,  0.0151],\n",
       "         [ 0.4852,  0.2450, -1.4038,  ...,  0.5648,  0.2038,  0.0156],\n",
       "         [ 0.4857,  0.2436, -1.3897,  ...,  0.5648,  0.2033,  0.0155]]),\n",
       " tensor([[ 0.4828,  0.2604, -1.2334,  ...,  0.5483,  0.2258,  0.0073],\n",
       "         [ 0.4816,  0.2606, -1.4555,  ...,  0.5474,  0.2267,  0.0087],\n",
       "         [ 0.4809,  0.2608, -1.4653,  ...,  0.5475,  0.2266,  0.0088],\n",
       "         ...,\n",
       "         [ 0.4832,  0.2490, -1.4275,  ...,  0.5557,  0.2156,  0.0123],\n",
       "         [ 0.4845,  0.2491, -1.4277,  ...,  0.5566,  0.2158,  0.0124],\n",
       "         [ 0.4855,  0.2492, -1.4367,  ...,  0.5573,  0.2163,  0.0122]]),\n",
       " tensor([[ 0.5122,  0.2456, -1.3706,  ...,  0.5836,  0.2207,  0.0125],\n",
       "         [ 0.5121,  0.2461, -1.4417,  ...,  0.5818,  0.2208,  0.0118],\n",
       "         [ 0.5121,  0.2465, -1.4591,  ...,  0.5815,  0.2204,  0.0118],\n",
       "         ...,\n",
       "         [ 0.5017,  0.2434, -1.4740,  ...,  0.5812,  0.2105,  0.0155],\n",
       "         [ 0.5022,  0.2433, -1.4619,  ...,  0.5818,  0.2102,  0.0157],\n",
       "         [ 0.5032,  0.2431, -1.4609,  ...,  0.5821,  0.2097,  0.0163]]),\n",
       " tensor([[ 0.4849,  0.2484, -1.3873,  ...,  0.5543,  0.2162,  0.0108],\n",
       "         [ 0.4846,  0.2500, -1.4881,  ...,  0.5539,  0.2157,  0.0111],\n",
       "         [ 0.4844,  0.2507, -1.4935,  ...,  0.5537,  0.2161,  0.0111],\n",
       "         ...,\n",
       "         [ 0.4589,  0.2435, -1.4798,  ...,  0.5299,  0.2058,  0.0120],\n",
       "         [ 0.4586,  0.2436, -1.4788,  ...,  0.5293,  0.2056,  0.0122],\n",
       "         [ 0.4583,  0.2437, -1.4790,  ...,  0.5289,  0.2052,  0.0123]]),\n",
       " tensor([[ 0.5199,  0.2290, -1.2810,  ...,  0.5876,  0.2058,  0.0103],\n",
       "         [ 0.5195,  0.2307, -1.3821,  ...,  0.5873,  0.2059,  0.0106],\n",
       "         [ 0.5193,  0.2327, -1.4100,  ...,  0.5877,  0.2059,  0.0109],\n",
       "         ...,\n",
       "         [ 0.5108,  0.2431, -1.3919,  ...,  0.5781,  0.2077,  0.0105],\n",
       "         [ 0.5093,  0.2422, -1.3894,  ...,  0.5768,  0.2068,  0.0103],\n",
       "         [ 0.5079,  0.2405, -1.3818,  ...,  0.5751,  0.2053,  0.0102]]),\n",
       " tensor([[ 0.5005,  0.2439, -1.3629,  ...,  0.5694,  0.2139,  0.0154],\n",
       "         [ 0.5002,  0.2448, -1.5049,  ...,  0.5688,  0.2128,  0.0146],\n",
       "         [ 0.4999,  0.2457, -1.5287,  ...,  0.5683,  0.2134,  0.0148],\n",
       "         ...,\n",
       "         [ 0.4965,  0.2433, -1.3673,  ...,  0.5670,  0.2114,  0.0169],\n",
       "         [ 0.4960,  0.2433, -1.3676,  ...,  0.5660,  0.2107,  0.0168],\n",
       "         [ 0.4950,  0.2433, -1.3637,  ...,  0.5651,  0.2100,  0.0170]])]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the sequences\n",
    "import torch\n",
    "sequences = load_keypoint_sequences(file_paths)\n",
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40, 160, 1662])\n"
     ]
    }
   ],
   "source": [
    "# Pad the sequences to the same length\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "padded_sequences = pad_sequence(sequences, batch_first=True)\n",
    "pad_sequence\n",
    "print(padded_sequences.shape) # (batch_size, max_sequence_length, num_keypoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['กฎกระทรวง',\n",
       " 'กฎหมายรัฐธรรมนูญ',\n",
       " 'กรมอนามัย',\n",
       " 'กรรม',\n",
       " 'กรรมสิทธิ์',\n",
       " 'กระโดด',\n",
       " 'กล้วยบวชชี',\n",
       " 'กล้วยเชื่อม',\n",
       " 'กังวล',\n",
       " 'กีฬา',\n",
       " 'น้อง',\n",
       " 'เขิน',\n",
       " 'เขื่อนดิน',\n",
       " 'เขื่อนสิริกิติ์',\n",
       " 'เข้าใจผิด',\n",
       " 'เคย',\n",
       " 'เครียด',\n",
       " 'เครื่องปั่นดิน',\n",
       " 'เครื่องหมายการค้า',\n",
       " 'เจอ',\n",
       " 'เจ้าหนี้',\n",
       " 'เช่าซื้อ',\n",
       " 'เช่าทรัพย์',\n",
       " 'เซอร์เบีย',\n",
       " 'เซเนกัล',\n",
       " 'เซ็ง',\n",
       " 'เดิน',\n",
       " 'เดิมพัน',\n",
       " 'เพลีย',\n",
       " 'เมื่อย',\n",
       " 'เม็กซิโก',\n",
       " 'เฮโรอีน',\n",
       " 'แกมเบีย',\n",
       " 'แซมเบีย',\n",
       " 'โกหก',\n",
       " 'โจทก์',\n",
       " 'โชจู',\n",
       " 'ใกล้',\n",
       " 'ไดโนเสาร์',\n",
       " 'ไอซ์']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = [action.split(\".\")[0] for action in actions]\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "       34, 35, 36, 37, 38, 39], dtype=int64)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "labels = le.fit_transform(labels)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Create a custom dataset\n",
    "class KeypointDataset(Dataset):\n",
    "    def __init__(self, file_paths, labels):\n",
    "        self.file_paths = file_paths\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        keypoints = np.load(self.file_paths[idx])\n",
    "        label = self.labels[idx]\n",
    "        return torch.tensor(keypoints, dtype=torch.float32), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset\n",
    "dataset = KeypointDataset(file_paths, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Data for different actions/กฎกระทรวง.mp4/กฎกระทรวง.npy', 'Data for different actions/กฎหมายรัฐธรรมนูญ.mp4/กฎหมายรัฐธรรมนูญ.npy', 'Data for different actions/กรมอนามัย.mp4/กรมอนามัย.npy', 'Data for different actions/กรรม.mp4/กรรม.npy', 'Data for different actions/กรรมสิทธิ์.mp4/กรรมสิทธิ์.npy', 'Data for different actions/กระโดด.mp4/กระโดด.npy', 'Data for different actions/กล้วยบวชชี.mp4/กล้วยบวชชี.npy', 'Data for different actions/กล้วยเชื่อม.mp4/กล้วยเชื่อม.npy', 'Data for different actions/กังวล.mp4/กังวล.npy', 'Data for different actions/กีฬา.mp4/กีฬา.npy', 'Data for different actions/น้อง.mp4/น้อง.npy', 'Data for different actions/เขิน.mp4/เขิน.npy', 'Data for different actions/เขื่อนดิน.mp4/เขื่อนดิน.npy', 'Data for different actions/เขื่อนสิริกิติ์.mp4/เขื่อนสิริกิติ์.npy', 'Data for different actions/เข้าใจผิด.mp4/เข้าใจผิด.npy', 'Data for different actions/เคย.mp4/เคย.npy', 'Data for different actions/เครียด.mp4/เครียด.npy', 'Data for different actions/เครื่องปั่นดิน.mp4/เครื่องปั่นดิน.npy', 'Data for different actions/เครื่องหมายการค้า.mp4/เครื่องหมายการค้า.npy', 'Data for different actions/เจอ.mp4/เจอ.npy', 'Data for different actions/เจ้าหนี้.mp4/เจ้าหนี้.npy', 'Data for different actions/เช่าซื้อ.mp4/เช่าซื้อ.npy', 'Data for different actions/เช่าทรัพย์.mp4/เช่าทรัพย์.npy', 'Data for different actions/เซอร์เบีย.mp4/เซอร์เบีย.npy', 'Data for different actions/เซเนกัล.mp4/เซเนกัล.npy', 'Data for different actions/เซ็ง.mp4/เซ็ง.npy', 'Data for different actions/เดิน.mp4/เดิน.npy', 'Data for different actions/เดิมพัน.mp4/เดิมพัน.npy', 'Data for different actions/เพลีย.mp4/เพลีย.npy', 'Data for different actions/เมื่อย.mp4/เมื่อย.npy', 'Data for different actions/เม็กซิโก.mp4/เม็กซิโก.npy', 'Data for different actions/เฮโรอีน.mp4/เฮโรอีน.npy', 'Data for different actions/แกมเบีย.mp4/แกมเบีย.npy', 'Data for different actions/แซมเบีย.mp4/แซมเบีย.npy', 'Data for different actions/โกหก.mp4/โกหก.npy', 'Data for different actions/โจทก์.mp4/โจทก์.npy', 'Data for different actions/โชจู.mp4/โชจู.npy', 'Data for different actions/ใกล้.mp4/ใกล้.npy', 'Data for different actions/ไดโนเสาร์.mp4/ไดโนเสาร์.npy', 'Data for different actions/ไอซ์.mp4/ไอซ์.npy']\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39]\n"
     ]
    }
   ],
   "source": [
    "print(dataset.file_paths)\n",
    "print(dataset.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collate function for padding\n",
    "def collate_fn(batch):\n",
    "    sequences, labels = zip(*batch)\n",
    "    padded_sequences = pad_sequence(sequences, batch_first=True)\n",
    "    return padded_sequences, torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x1daa5db2f00>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the DataLoader\n",
    "batch_size = 4\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set device (use GPU if available)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        self.attention_weights = nn.Linear(hidden_size, 1, bias=False)\n",
    "\n",
    "    def forward(self, lstm_output):\n",
    "        # lstm_output: (batch_size, sequence_length, hidden_size)\n",
    "        attention_scores = self.attention_weights(lstm_output)  # (batch_size, sequence_length, 1)\n",
    "        attention_weights = torch.softmax(attention_scores, dim=1)  # (batch_size, sequence_length, 1)\n",
    "        weighted_output = torch.sum(lstm_output * attention_weights, dim=1)  # (batch_size, hidden_size)\n",
    "        return weighted_output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes,dropout):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.pool = nn.MaxPool2d(kernel_size=(2,1), stride=1)\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.attention = AttentionLayer(hidden_size)\n",
    "        self.fc1 = nn.Linear(hidden_size, 256)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        # Apply pooling before LSTM\n",
    "        x = self.pool(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Forward propagate the LSTM\n",
    "        lstm_output, _ = self.lstm(x, (h0, c0))\n",
    "\n",
    "        # Apply attention to the LSTM output\n",
    "        attention_output, attention_weights = self.attention(lstm_output)\n",
    "\n",
    "        # Classification based on attention output\n",
    "        out = self.fc1(attention_output)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model, loss function, and optimizer\n",
    "model = LSTMModel(input_size=1662, hidden_size=256, num_layers=2, num_classes=40, dropout=0.5).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMModel(\n",
       "  (pool): MaxPool2d(kernel_size=(2, 1), stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "  (lstm): LSTM(1662, 256, num_layers=2, batch_first=True)\n",
       "  (attention): AttentionLayer(\n",
       "    (attention_weights): Linear(in_features=256, out_features=1, bias=False)\n",
       "  )\n",
       "  (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "  (fc2): Linear(in_features=256, out_features=40, bias=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()  # For multi-class classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load model\n",
    "model.load_state_dict(torch.load('saved_data/attention_lstm.pt'))\n",
    "optimizer.load_state_dict(torch.load('saved_data/optimizer.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how many Parameters our Model has!\n",
    "num_model_params = 0\n",
    "for param in model.parameters():\n",
    "    num_model_params += param.flatten().shape[0]\n",
    "\n",
    "print(\"-This Model Has %d (Approximately %d Million) Parameters!\" % (num_model_params, num_model_params//1e6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "# References : https://saturncloud.io/blog/calculating-the-accuracy-of-pytorch-models-every-epoch/#:~:text=In%20order%20to%20calculate%20the,tensor%20along%20a%20specified%20dimension\n",
    "\n",
    "from sklearn.metrics import f1_score, recall_score\n",
    "\n",
    "num_epochs = 900\n",
    "loss_logger = []\n",
    "accuracy_logger = []\n",
    "f1_logger = []\n",
    "recall_logger = []\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "# n_epochs = []\n",
    "for epoch in range(num_epochs):\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    model.train()\n",
    "    for i, (sequences, labels) in enumerate(data_loader):\n",
    "        # Move data to the device\n",
    "        # labels = labels.type(torch.LongTensor)   # casting to long\n",
    "        sequences = sequences.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(sequences)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        total_correct += (predicted == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "        # Store predictions and labels for calculating metrics\n",
    "        all_preds.extend(predicted.cpu().numpy())  # Store predictions\n",
    "        all_labels.extend(labels.cpu().numpy())    # Store true labels\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # loss_logger.append(loss.item())\n",
    "    loss_logger.append(loss.item())\n",
    "    accuracy = 100 * total_correct /total_samples\n",
    "\n",
    "    # Calculate F1 score and recall\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')  # Weighted average for multi-class\n",
    "    recall = recall_score(all_labels, all_preds, average='weighted')\n",
    "\n",
    "    f1_logger.append(f1)\n",
    "    recall_logger.append(recall)\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f} , Accuracy : {accuracy:.2f}%, F1 Score: {f1:.2f}, Recall: {recall:.2f}')\n",
    "    accuracy_logger.append(accuracy)\n",
    "    # n_epochs.append(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.title(\"Loss Curve\")\n",
    "plt.plot(loss_logger, label='train_loss')\n",
    "# plt.plot(accuracy_logger,label='accuracy')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save and Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.5013,  0.2452, -1.2167,  ...,  0.5663,  0.2188,  0.0098],\n",
       "         [ 0.4997,  0.2482, -1.4690,  ...,  0.5652,  0.2181,  0.0106],\n",
       "         [ 0.4984,  0.2500, -1.4853,  ...,  0.5654,  0.2185,  0.0112],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 0.4922,  0.2382, -1.2850,  ...,  0.5578,  0.2124,  0.0094],\n",
       "         [ 0.4920,  0.2405, -1.4288,  ...,  0.5571,  0.2116,  0.0099],\n",
       "         [ 0.4920,  0.2409, -1.4093,  ...,  0.5567,  0.2122,  0.0098],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 0.5049,  0.2371, -1.2115,  ...,  0.5643,  0.2082,  0.0088],\n",
       "         [ 0.5045,  0.2381, -1.1896,  ...,  0.5643,  0.2081,  0.0085],\n",
       "         [ 0.5041,  0.2385, -1.1915,  ...,  0.5643,  0.2080,  0.0089],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.4849,  0.2484, -1.3873,  ...,  0.5543,  0.2162,  0.0108],\n",
       "         [ 0.4846,  0.2500, -1.4881,  ...,  0.5539,  0.2157,  0.0111],\n",
       "         [ 0.4844,  0.2507, -1.4935,  ...,  0.5537,  0.2161,  0.0111],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 0.5199,  0.2290, -1.2810,  ...,  0.5876,  0.2058,  0.0103],\n",
       "         [ 0.5195,  0.2307, -1.3821,  ...,  0.5873,  0.2059,  0.0106],\n",
       "         [ 0.5193,  0.2327, -1.4100,  ...,  0.5877,  0.2059,  0.0109],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 0.5005,  0.2439, -1.3629,  ...,  0.5694,  0.2139,  0.0154],\n",
       "         [ 0.5002,  0.2448, -1.5049,  ...,  0.5688,  0.2128,  0.0146],\n",
       "         [ 0.4999,  0.2457, -1.5287,  ...,  0.5683,  0.2134,  0.0148],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_sequence(sequences, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 13.4496,  -7.2260, -14.8576,  ..., -19.8868,   6.6438,  -4.2219],\n",
       "        [ -8.7215,  23.3395, -16.4504,  ...,  10.0835, -19.4497, -12.2901],\n",
       "        [ -8.7685,   3.8143,  28.2602,  ...,  -2.3879,  -9.7665,   3.9817],\n",
       "        ...,\n",
       "        [-15.7498,  12.5401, -20.2498,  ...,  32.0454, -24.0659,   8.3257],\n",
       "        [ 11.5046,  -5.8887, -12.1786,  ..., -22.1270,  20.0571,  -9.6304],\n",
       "        [  4.2252, -26.7240,   2.9994,  ...,  -9.7900,   0.6240,  23.9765]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Put the model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# No need to track gradients during inference\n",
    "with torch.no_grad():\n",
    "    # Get the model's output (logits)\n",
    "    outputs = model(padded_sequences.to(device))\n",
    "\n",
    "# outputs = torch.softmax(outputs, dim=1)\n",
    "# outputs = torch.max(outputs,1)\n",
    "\n",
    "outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.5013,  0.2452, -1.2167,  ...,  0.5663,  0.2188,  0.0098],\n",
       "         [ 0.4997,  0.2482, -1.4690,  ...,  0.5652,  0.2181,  0.0106],\n",
       "         [ 0.4984,  0.2500, -1.4853,  ...,  0.5654,  0.2185,  0.0112],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 0.4922,  0.2382, -1.2850,  ...,  0.5578,  0.2124,  0.0094],\n",
       "         [ 0.4920,  0.2405, -1.4288,  ...,  0.5571,  0.2116,  0.0099],\n",
       "         [ 0.4920,  0.2409, -1.4093,  ...,  0.5567,  0.2122,  0.0098],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 0.5049,  0.2371, -1.2115,  ...,  0.5643,  0.2082,  0.0088],\n",
       "         [ 0.5045,  0.2381, -1.1896,  ...,  0.5643,  0.2081,  0.0085],\n",
       "         [ 0.5041,  0.2385, -1.1915,  ...,  0.5643,  0.2080,  0.0089],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.4849,  0.2484, -1.3873,  ...,  0.5543,  0.2162,  0.0108],\n",
       "         [ 0.4846,  0.2500, -1.4881,  ...,  0.5539,  0.2157,  0.0111],\n",
       "         [ 0.4844,  0.2507, -1.4935,  ...,  0.5537,  0.2161,  0.0111],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 0.5199,  0.2290, -1.2810,  ...,  0.5876,  0.2058,  0.0103],\n",
       "         [ 0.5195,  0.2307, -1.3821,  ...,  0.5873,  0.2059,  0.0106],\n",
       "         [ 0.5193,  0.2327, -1.4100,  ...,  0.5877,  0.2059,  0.0109],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 0.5005,  0.2439, -1.3629,  ...,  0.5694,  0.2139,  0.0154],\n",
       "         [ 0.5002,  0.2448, -1.5049,  ...,  0.5688,  0.2128,  0.0146],\n",
       "         [ 0.4999,  0.2457, -1.5287,  ...,  0.5683,  0.2134,  0.0148],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = [\"Data for different actions/เขิน_0.mp4/เขิน_0.npy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.4321,  0.2240, -1.4261,  ...,  0.4724,  0.1833,  0.0108],\n",
       "         [ 0.4325,  0.2250, -1.4727,  ...,  0.4722,  0.1847,  0.0102],\n",
       "         [ 0.4330,  0.2258, -1.4930,  ...,  0.4725,  0.1845,  0.0109],\n",
       "         ...,\n",
       "         [ 0.4396,  0.2238, -1.5889,  ...,  0.4802,  0.1780,  0.0139],\n",
       "         [ 0.4395,  0.2238, -1.5796,  ...,  0.4802,  0.1776,  0.0141],\n",
       "         [ 0.4394,  0.2234, -1.5304,  ...,  0.4799,  0.1771,  0.0144]]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the sequences\n",
    "import torch\n",
    "sequences = load_keypoint_sequences(file_paths)\n",
    "# Change list to numpy array \n",
    "sequences = np.array(sequences)\n",
    "# Change numpy array to tensor\n",
    "sequences = torch.FloatTensor(sequences)\n",
    "sequences = pad_sequence(sequences, batch_first=True)\n",
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  7.5823,   0.3613, -19.9278,  -6.2863, -21.7236,   4.8139, -11.4924,\n",
       "           5.2065,  -9.3429,  -2.8338,  -2.5934,  -1.9949,   6.4344,  -5.0653,\n",
       "          -4.0501,  -7.7373,   5.0221,  -2.0814, -14.2553,   3.0559, -14.9440,\n",
       "         -11.7800,  -9.3077,  -3.2981,   2.3669,  -3.5013,  -1.1654,  -0.4708,\n",
       "          -2.1079,   8.9268, -13.5628,   5.2018,  -5.0090,  -2.0148,  -3.4905,\n",
       "          10.2788,   2.6378,  -7.8567,  -1.1117,  -4.4926]], device='cuda:0',\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model(sequences.to(device))\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['กฎกระทรวง',\n",
       " 'กฎหมายรัฐธรรมนูญ',\n",
       " 'กรมอนามัย',\n",
       " 'กรรม',\n",
       " 'กรรมสิทธิ์',\n",
       " 'กระโดด',\n",
       " 'กล้วยบวชชี',\n",
       " 'กล้วยเชื่อม',\n",
       " 'กังวล',\n",
       " 'กีฬา',\n",
       " 'น้อง',\n",
       " 'เขิน',\n",
       " 'เขื่อนดิน',\n",
       " 'เขื่อนสิริกิติ์',\n",
       " 'เข้าใจผิด',\n",
       " 'เคย',\n",
       " 'เครียด',\n",
       " 'เครื่องปั่นดิน',\n",
       " 'เครื่องหมายการค้า',\n",
       " 'เจอ',\n",
       " 'เจ้าหนี้',\n",
       " 'เช่าซื้อ',\n",
       " 'เช่าทรัพย์',\n",
       " 'เซอร์เบีย',\n",
       " 'เซเนกัล',\n",
       " 'เซ็ง',\n",
       " 'เดิน',\n",
       " 'เดิมพัน',\n",
       " 'เพลีย',\n",
       " 'เมื่อย',\n",
       " 'เม็กซิโก',\n",
       " 'เฮโรอีน',\n",
       " 'แกมเบีย',\n",
       " 'แซมเบีย',\n",
       " 'โกหก',\n",
       " 'โจทก์',\n",
       " 'โชจู',\n",
       " 'ใกล้',\n",
       " 'ไดโนเสาร์',\n",
       " 'ไอซ์']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = [action.split(\".\")[0] for action in actions]\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change from tensor to numpy arrat\n",
    "outputs = outputs.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  7.58235   ,   0.36126336, -19.927765  ,  -6.286277  ,\n",
       "        -21.723566  ,   4.8138638 , -11.492439  ,   5.2064967 ,\n",
       "         -9.34289   ,  -2.8337653 ,  -2.5934296 ,  -1.9948897 ,\n",
       "          6.434372  ,  -5.065332  ,  -4.0501328 ,  -7.7373304 ,\n",
       "          5.0220885 ,  -2.081383  , -14.255286  ,   3.055937  ,\n",
       "        -14.944012  , -11.780033  ,  -9.307734  ,  -3.298115  ,\n",
       "          2.3668766 ,  -3.5013237 ,  -1.1654106 ,  -0.47082248,\n",
       "         -2.1079023 ,   8.926817  , -13.562763  ,   5.2018304 ,\n",
       "         -5.008995  ,  -2.014824  ,  -3.4905324 ,  10.278809  ,\n",
       "          2.6378357 ,  -7.856713  ,  -1.111718  ,  -4.4925623 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  7.58235      0.36126336 -19.927765    -6.286277   -21.723566\n",
      "   4.8138638  -11.492439     5.2064967   -9.34289     -2.8337653\n",
      "  -2.5934296   -1.9948897    6.434372    -5.065332    -4.0501328\n",
      "  -7.7373304    5.0220885   -2.081383   -14.255286     3.055937\n",
      " -14.944012   -11.780033    -9.307734    -3.298115     2.3668766\n",
      "  -3.5013237   -1.1654106   -0.47082248  -2.1079023    8.926817\n",
      " -13.562763     5.2018304   -5.008995    -2.014824    -3.4905324\n",
      "  10.278809     2.6378357   -7.856713    -1.111718    -4.4925623 ]\n"
     ]
    }
   ],
   "source": [
    "for idx, word in enumerate(outputs):\n",
    "    # max_value = torch.max(outputs)\n",
    "    list_outputs = max(outputs)\n",
    "    print(list_outputs)\n",
    "    # print(max_value)\n",
    "    # print(max_value.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_max = max(range(len(list_outputs)), key=list_outputs.__getitem__)\n",
    "index_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "โจทก์\n"
     ]
    }
   ],
   "source": [
    "print(labels[index_max])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : กฎกระทรวง Predicted : กฎกระทรวง\n",
      "Input : กฎหมายรัฐธรรมนูญ Predicted : กฎหมายรัฐธรรมนูญ\n",
      "Input : กรมอนามัย Predicted : กรมอนามัย\n",
      "Input : กรรม Predicted : กังวล\n",
      "Input : กรรมสิทธิ์ Predicted : กรรมสิทธิ์\n",
      "Input : กระโดด Predicted : กระโดด\n",
      "Input : กล้วยบวชชี Predicted : กล้วยบวชชี\n",
      "Input : กล้วยเชื่อม Predicted : กล้วยเชื่อม\n",
      "Input : กังวล Predicted : กังวล\n",
      "Input : กีฬา Predicted : กีฬา\n",
      "Input : น้อง Predicted : น้อง\n",
      "Input : เขิน Predicted : เขิน\n",
      "Input : เขื่อนดิน Predicted : เขื่อนดิน\n",
      "Input : เขื่อนสิริกิติ์ Predicted : เขื่อนสิริกิติ์\n",
      "Input : เข้าใจผิด Predicted : กังวล\n",
      "Input : เคย Predicted : กล้วยเชื่อม\n",
      "Input : เครียด Predicted : เครียด\n",
      "Input : เครื่องปั่นดิน Predicted : เครื่องปั่นดิน\n",
      "Input : เครื่องหมายการค้า Predicted : เครื่องหมายการค้า\n",
      "Input : เจอ Predicted : เจอ\n",
      "Input : เจ้าหนี้ Predicted : เจ้าหนี้\n",
      "Input : เช่าซื้อ Predicted : เช่าซื้อ\n",
      "Input : เช่าทรัพย์ Predicted : เช่าทรัพย์\n",
      "Input : เซอร์เบีย Predicted : เซอร์เบีย\n",
      "Input : เซเนกัล Predicted : เซเนกัล\n",
      "Input : เซ็ง Predicted : โกหก\n",
      "Input : เดิน Predicted : เดิน\n",
      "Input : เดิมพัน Predicted : โจทก์\n",
      "Input : เพลีย Predicted : กฎกระทรวง\n",
      "Input : เมื่อย Predicted : เมื่อย\n",
      "Input : เม็กซิโก Predicted : เม็กซิโก\n",
      "Input : เฮโรอีน Predicted : เฮโรอีน\n",
      "Input : แกมเบีย Predicted : ไดโนเสาร์\n",
      "Input : แซมเบีย Predicted : แซมเบีย\n",
      "Input : โกหก Predicted : โกหก\n",
      "Input : โจทก์ Predicted : เฮโรอีน\n",
      "Input : โชจู Predicted : โชจู\n",
      "Input : ใกล้ Predicted : ใกล้\n",
      "Input : ไดโนเสาร์ Predicted : ไดโนเสาร์\n",
      "Input : ไอซ์ Predicted : เซอร์เบีย\n",
      "Correct Predicted on Training set : 31 Corrct percentage : 77.5%\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "num_clip = 0\n",
    "for clip in labels:\n",
    "    # print(clip)\n",
    "    file_paths = [f\"Data for different actions/{clip}.mp4/{clip}.npy\"]\n",
    "    # print(file_paths)\n",
    "\n",
    "    sequences = load_keypoint_sequences(file_paths)\n",
    "    # Change list to numpy array \n",
    "    sequences = np.array(sequences)\n",
    "    # Change numpy array to tensor\n",
    "    sequences = torch.FloatTensor(sequences)\n",
    "    sequences = pad_sequence(sequences, batch_first=True)\n",
    "\n",
    "    outputs = model(sequences.to(device))\n",
    "\n",
    "    for idx, word in enumerate(outputs):\n",
    "        # max_value = torch.max(outputs)\n",
    "        list_outputs = max(outputs)\n",
    "\n",
    "    index_max = max(range(len(list_outputs)), key=list_outputs.__getitem__)\n",
    "\n",
    "    print(f\"Input : {clip} Predicted : {labels[index_max]}\")\n",
    "\n",
    "    if clip == labels[index_max]:\n",
    "        correct = correct+1\n",
    "    \n",
    "    num_clip = num_clip + 1 \n",
    "\n",
    "print(f\"Correct Predicted on Training set : {correct} Corrct percentage : {correct*100/num_clip}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['กฎกระทรวง', 'กฎหมายรัฐธรรมนูญ', 'กรมอนามัย', 'กรรม', 'กรรมสิทธิ์', 'กระโดด', 'กล้วยบวชชี', 'กล้วยเชื่อม', 'กังวล', 'กีฬา', 'น้อง', 'เขิน', 'เขื่อนดิน', 'เขื่อนสิริกิติ์', 'เข้าใจผิด', 'เคย', 'เครียด', 'เครื่องปั่นดิน', 'เครื่องหมายการค้า', 'เจอ', 'เจ้าหนี้', 'เช่าซื้อ', 'เช่าทรัพย์', 'เซอร์เบีย', 'เซเนกัล', 'เซ็ง', 'เดิน', 'เดิมพัน', 'เพลีย', 'เมื่อย', 'เม็กซิโก', 'เฮโรอีน', 'แกมเบีย', 'แซมเบีย', 'โกหก', 'โจทก์', 'โชจู', 'ใกล้', 'ไดโนเสาร์', 'ไอซ์']\n"
     ]
    }
   ],
   "source": [
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "video_dir = \"C:/Users/araya/Desktop/augments\"\n",
    "video_list = []\n",
    "video_list = os.listdir(video_dir)\n",
    "\n",
    "len(video_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for exported data, numpy arrays\n",
    "Model_Data=os.path.join('../MediaPipe/Test')\n",
    "\n",
    "actions_test = np.array(video_list)\n",
    "\n",
    "no_of_seqs = 1\n",
    "\n",
    "# 30 frames in length\n",
    "seq_length = 160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['กฎกระทรวง_0',\n",
       " 'กฎกระทรวง_1',\n",
       " 'กฎหมายรัฐธรรมนูญ_0',\n",
       " 'กฎหมายรัฐธรรมนูญ_1',\n",
       " 'กรมอนามัย_0',\n",
       " 'กรมอนามัย_1',\n",
       " 'กรรม_0',\n",
       " 'กรรม_1',\n",
       " 'กรรมสิทธิ์_0',\n",
       " 'กรรมสิทธิ์_1',\n",
       " 'กระโดด_0',\n",
       " 'กระโดด_1',\n",
       " 'กล้วยบวชชี_0',\n",
       " 'กล้วยบวชชี_1',\n",
       " 'กล้วยเชื่อม_0',\n",
       " 'กล้วยเชื่อม_1',\n",
       " 'กังวล_0',\n",
       " 'กังวล_1',\n",
       " 'กีฬา_0',\n",
       " 'กีฬา_1',\n",
       " 'น้อง_0',\n",
       " 'น้อง_1',\n",
       " 'เขิน_0',\n",
       " 'เขิน_1',\n",
       " 'เขื่อนดิน_0',\n",
       " 'เขื่อนดิน_1',\n",
       " 'เขื่อนสิริกิติ์_0',\n",
       " 'เขื่อนสิริกิติ์_1',\n",
       " 'เข้าใจผิด_0',\n",
       " 'เข้าใจผิด_1',\n",
       " 'เคย_0',\n",
       " 'เคย_1',\n",
       " 'เครียด_0',\n",
       " 'เครียด_1',\n",
       " 'เครื่องปั่นดิน_0',\n",
       " 'เครื่องปั่นดิน_1',\n",
       " 'เครื่องหมายการค้า_0',\n",
       " 'เครื่องหมายการค้า_1',\n",
       " 'เจอ_0',\n",
       " 'เจอ_1',\n",
       " 'เจ้าหนี้_0',\n",
       " 'เจ้าหนี้_1',\n",
       " 'เช่าซื้อ_0',\n",
       " 'เช่าซื้อ_1',\n",
       " 'เช่าทรัพย์_0',\n",
       " 'เช่าทรัพย์_1',\n",
       " 'เซอร์เบีย_0',\n",
       " 'เซอร์เบีย_1',\n",
       " 'เซเนกัล_0',\n",
       " 'เซเนกัล_1',\n",
       " 'เซ็ง_0',\n",
       " 'เซ็ง_1',\n",
       " 'เดิน_0',\n",
       " 'เดิน_1',\n",
       " 'เดิมพัน_0',\n",
       " 'เดิมพัน_1',\n",
       " 'เพลีย_0',\n",
       " 'เพลีย_1',\n",
       " 'เมื่อย_0',\n",
       " 'เมื่อย_1',\n",
       " 'เม็กซิโก_0',\n",
       " 'เม็กซิโก_1',\n",
       " 'เฮโรอีน_0',\n",
       " 'เฮโรอีน_1',\n",
       " 'แกมเบีย_0',\n",
       " 'แกมเบีย_1',\n",
       " 'แซมเบีย_0',\n",
       " 'แซมเบีย_1',\n",
       " 'โกหก_0',\n",
       " 'โกหก_1',\n",
       " 'โจทก์_0',\n",
       " 'โจทก์_1',\n",
       " 'โชจู_0',\n",
       " 'โชจู_1',\n",
       " 'ใกล้_0',\n",
       " 'ใกล้_1',\n",
       " 'ไดโนเสาร์_0',\n",
       " 'ไดโนเสาร์_1',\n",
       " 'ไอซ์_0',\n",
       " 'ไอซ์_1']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_test = [action.split(\".\")[0] for action in actions_test]\n",
    "labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Test/กฎกระทรวง_0.mp4/กฎกระทรวง_0.npy', 'Test/กฎกระทรวง_1.mp4/กฎกระทรวง_1.npy', 'Test/กฎหมายรัฐธรรมนูญ_0.mp4/กฎหมายรัฐธรรมนูญ_0.npy', 'Test/กฎหมายรัฐธรรมนูญ_1.mp4/กฎหมายรัฐธรรมนูญ_1.npy', 'Test/กรมอนามัย_0.mp4/กรมอนามัย_0.npy', 'Test/กรมอนามัย_1.mp4/กรมอนามัย_1.npy', 'Test/กรรม_0.mp4/กรรม_0.npy', 'Test/กรรม_1.mp4/กรรม_1.npy', 'Test/กรรมสิทธิ์_0.mp4/กรรมสิทธิ์_0.npy', 'Test/กรรมสิทธิ์_1.mp4/กรรมสิทธิ์_1.npy', 'Test/กระโดด_0.mp4/กระโดด_0.npy', 'Test/กระโดด_1.mp4/กระโดด_1.npy', 'Test/กล้วยบวชชี_0.mp4/กล้วยบวชชี_0.npy', 'Test/กล้วยบวชชี_1.mp4/กล้วยบวชชี_1.npy', 'Test/กล้วยเชื่อม_0.mp4/กล้วยเชื่อม_0.npy', 'Test/กล้วยเชื่อม_1.mp4/กล้วยเชื่อม_1.npy', 'Test/กังวล_0.mp4/กังวล_0.npy', 'Test/กังวล_1.mp4/กังวล_1.npy', 'Test/กีฬา_0.mp4/กีฬา_0.npy', 'Test/กีฬา_1.mp4/กีฬา_1.npy', 'Test/น้อง_0.mp4/น้อง_0.npy', 'Test/น้อง_1.mp4/น้อง_1.npy', 'Test/เขิน_0.mp4/เขิน_0.npy', 'Test/เขิน_1.mp4/เขิน_1.npy', 'Test/เขื่อนดิน_0.mp4/เขื่อนดิน_0.npy', 'Test/เขื่อนดิน_1.mp4/เขื่อนดิน_1.npy', 'Test/เขื่อนสิริกิติ์_0.mp4/เขื่อนสิริกิติ์_0.npy', 'Test/เขื่อนสิริกิติ์_1.mp4/เขื่อนสิริกิติ์_1.npy', 'Test/เข้าใจผิด_0.mp4/เข้าใจผิด_0.npy', 'Test/เข้าใจผิด_1.mp4/เข้าใจผิด_1.npy', 'Test/เคย_0.mp4/เคย_0.npy', 'Test/เคย_1.mp4/เคย_1.npy', 'Test/เครียด_0.mp4/เครียด_0.npy', 'Test/เครียด_1.mp4/เครียด_1.npy', 'Test/เครื่องปั่นดิน_0.mp4/เครื่องปั่นดิน_0.npy', 'Test/เครื่องปั่นดิน_1.mp4/เครื่องปั่นดิน_1.npy', 'Test/เครื่องหมายการค้า_0.mp4/เครื่องหมายการค้า_0.npy', 'Test/เครื่องหมายการค้า_1.mp4/เครื่องหมายการค้า_1.npy', 'Test/เจอ_0.mp4/เจอ_0.npy', 'Test/เจอ_1.mp4/เจอ_1.npy', 'Test/เจ้าหนี้_0.mp4/เจ้าหนี้_0.npy', 'Test/เจ้าหนี้_1.mp4/เจ้าหนี้_1.npy', 'Test/เช่าซื้อ_0.mp4/เช่าซื้อ_0.npy', 'Test/เช่าซื้อ_1.mp4/เช่าซื้อ_1.npy', 'Test/เช่าทรัพย์_0.mp4/เช่าทรัพย์_0.npy', 'Test/เช่าทรัพย์_1.mp4/เช่าทรัพย์_1.npy', 'Test/เซอร์เบีย_0.mp4/เซอร์เบีย_0.npy', 'Test/เซอร์เบีย_1.mp4/เซอร์เบีย_1.npy', 'Test/เซเนกัล_0.mp4/เซเนกัล_0.npy', 'Test/เซเนกัล_1.mp4/เซเนกัล_1.npy', 'Test/เซ็ง_0.mp4/เซ็ง_0.npy', 'Test/เซ็ง_1.mp4/เซ็ง_1.npy', 'Test/เดิน_0.mp4/เดิน_0.npy', 'Test/เดิน_1.mp4/เดิน_1.npy', 'Test/เดิมพัน_0.mp4/เดิมพัน_0.npy', 'Test/เดิมพัน_1.mp4/เดิมพัน_1.npy', 'Test/เพลีย_0.mp4/เพลีย_0.npy', 'Test/เพลีย_1.mp4/เพลีย_1.npy', 'Test/เมื่อย_0.mp4/เมื่อย_0.npy', 'Test/เมื่อย_1.mp4/เมื่อย_1.npy', 'Test/เม็กซิโก_0.mp4/เม็กซิโก_0.npy', 'Test/เม็กซิโก_1.mp4/เม็กซิโก_1.npy', 'Test/เฮโรอีน_0.mp4/เฮโรอีน_0.npy', 'Test/เฮโรอีน_1.mp4/เฮโรอีน_1.npy', 'Test/แกมเบีย_0.mp4/แกมเบีย_0.npy', 'Test/แกมเบีย_1.mp4/แกมเบีย_1.npy', 'Test/แซมเบีย_0.mp4/แซมเบีย_0.npy', 'Test/แซมเบีย_1.mp4/แซมเบีย_1.npy', 'Test/โกหก_0.mp4/โกหก_0.npy', 'Test/โกหก_1.mp4/โกหก_1.npy', 'Test/โจทก์_0.mp4/โจทก์_0.npy', 'Test/โจทก์_1.mp4/โจทก์_1.npy', 'Test/โชจู_0.mp4/โชจู_0.npy', 'Test/โชจู_1.mp4/โชจู_1.npy', 'Test/ใกล้_0.mp4/ใกล้_0.npy', 'Test/ใกล้_1.mp4/ใกล้_1.npy', 'Test/ไดโนเสาร์_0.mp4/ไดโนเสาร์_0.npy', 'Test/ไดโนเสาร์_1.mp4/ไดโนเสาร์_1.npy', 'Test/ไอซ์_0.mp4/ไอซ์_0.npy', 'Test/ไอซ์_1.mp4/ไอซ์_1.npy']\n"
     ]
    }
   ],
   "source": [
    "file_paths = []\n",
    "for action in actions_test:\n",
    "    video_path = os.path.join('Test/', action)\n",
    "    # print(video_path)\n",
    "    # print(action)\n",
    "    file_paths.append(video_path + '/' + action.split(\".\")[0] + \".npy\")\n",
    "print(file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : กฎกระทรวง_0 Predicted : กฎกระทรวง\n",
      "Input : กฎกระทรวง_1 Predicted : กฎกระทรวง\n",
      "Input : กฎหมายรัฐธรรมนูญ_0 Predicted : กฎหมายรัฐธรรมนูญ\n",
      "Input : กฎหมายรัฐธรรมนูญ_1 Predicted : กฎหมายรัฐธรรมนูญ\n",
      "Input : กรมอนามัย_0 Predicted : เขื่อนสิริกิติ์\n",
      "Input : กรมอนามัย_1 Predicted : กรมอนามัย\n",
      "Input : กรรม_0 Predicted : แซมเบีย\n",
      "Input : กรรม_1 Predicted : กังวล\n",
      "Input : กรรมสิทธิ์_0 Predicted : กรรมสิทธิ์\n",
      "Input : กรรมสิทธิ์_1 Predicted : กรรมสิทธิ์\n",
      "Input : กระโดด_0 Predicted : โชจู\n",
      "Input : กระโดด_1 Predicted : กระโดด\n",
      "Input : กล้วยบวชชี_0 Predicted : เช่าซื้อ\n",
      "Input : กล้วยบวชชี_1 Predicted : เช่าซื้อ\n",
      "Input : กล้วยเชื่อม_0 Predicted : กล้วยเชื่อม\n",
      "Input : กล้วยเชื่อม_1 Predicted : กล้วยเชื่อม\n",
      "Input : กังวล_0 Predicted : เซเนกัล\n",
      "Input : กังวล_1 Predicted : กังวล\n",
      "Input : กีฬา_0 Predicted : กรมอนามัย\n",
      "Input : กีฬา_1 Predicted : กรมอนามัย\n",
      "Input : น้อง_0 Predicted : เดิน\n",
      "Input : น้อง_1 Predicted : น้อง\n",
      "Input : เขิน_0 Predicted : โจทก์\n",
      "Input : เขิน_1 Predicted : เขิน\n",
      "Input : เขื่อนดิน_0 Predicted : เขื่อนดิน\n",
      "Input : เขื่อนดิน_1 Predicted : เขื่อนดิน\n",
      "Input : เขื่อนสิริกิติ์_0 Predicted : กฎกระทรวง\n",
      "Input : เขื่อนสิริกิติ์_1 Predicted : เขื่อนสิริกิติ์\n",
      "Input : เข้าใจผิด_0 Predicted : เขิน\n",
      "Input : เข้าใจผิด_1 Predicted : กังวล\n",
      "Input : เคย_0 Predicted : เจอ\n",
      "Input : เคย_1 Predicted : กล้วยเชื่อม\n",
      "Input : เครียด_0 Predicted : กฎกระทรวง\n",
      "Input : เครียด_1 Predicted : เครียด\n",
      "Input : เครื่องปั่นดิน_0 Predicted : เครื่องปั่นดิน\n",
      "Input : เครื่องปั่นดิน_1 Predicted : เครื่องปั่นดิน\n",
      "Input : เครื่องหมายการค้า_0 Predicted : เครื่องหมายการค้า\n",
      "Input : เครื่องหมายการค้า_1 Predicted : เครื่องหมายการค้า\n",
      "Input : เจอ_0 Predicted : เจอ\n",
      "Input : เจอ_1 Predicted : โกหก\n",
      "Input : เจ้าหนี้_0 Predicted : โชจู\n",
      "Input : เจ้าหนี้_1 Predicted : เจ้าหนี้\n",
      "Input : เช่าซื้อ_0 Predicted : เช่าซื้อ\n",
      "Input : เช่าซื้อ_1 Predicted : เช่าซื้อ\n",
      "Input : เช่าทรัพย์_0 Predicted : เช่าทรัพย์\n",
      "Input : เช่าทรัพย์_1 Predicted : เช่าทรัพย์\n",
      "Input : เซอร์เบีย_0 Predicted : กฎหมายรัฐธรรมนูญ\n",
      "Input : เซอร์เบีย_1 Predicted : โกหก\n",
      "Input : เซเนกัล_0 Predicted : เซเนกัล\n",
      "Input : เซเนกัล_1 Predicted : เซเนกัล\n",
      "Input : เซ็ง_0 Predicted : เครียด\n",
      "Input : เซ็ง_1 Predicted : โกหก\n",
      "Input : เดิน_0 Predicted : กล้วยเชื่อม\n",
      "Input : เดิน_1 Predicted : เดิน\n",
      "Input : เดิมพัน_0 Predicted : โจทก์\n",
      "Input : เดิมพัน_1 Predicted : โจทก์\n",
      "Input : เพลีย_0 Predicted : กฎกระทรวง\n",
      "Input : เพลีย_1 Predicted : กฎกระทรวง\n",
      "Input : เมื่อย_0 Predicted : แซมเบีย\n",
      "Input : เมื่อย_1 Predicted : เมื่อย\n",
      "Input : เม็กซิโก_0 Predicted : เม็กซิโก\n",
      "Input : เม็กซิโก_1 Predicted : เม็กซิโก\n",
      "Input : เฮโรอีน_0 Predicted : เฮโรอีน\n",
      "Input : เฮโรอีน_1 Predicted : เฮโรอีน\n",
      "Input : แกมเบีย_0 Predicted : ไดโนเสาร์\n",
      "Input : แกมเบีย_1 Predicted : ไดโนเสาร์\n",
      "Input : แซมเบีย_0 Predicted : แซมเบีย\n",
      "Input : แซมเบีย_1 Predicted : แซมเบีย\n",
      "Input : โกหก_0 Predicted : โกหก\n",
      "Input : โกหก_1 Predicted : โกหก\n",
      "Input : โจทก์_0 Predicted : กฎกระทรวง\n",
      "Input : โจทก์_1 Predicted : เฮโรอีน\n",
      "Input : โชจู_0 Predicted : เซเนกัล\n",
      "Input : โชจู_1 Predicted : โชจู\n",
      "Input : ใกล้_0 Predicted : ใกล้\n",
      "Input : ใกล้_1 Predicted : ใกล้\n",
      "Input : ไดโนเสาร์_0 Predicted : เซเนกัล\n",
      "Input : ไดโนเสาร์_1 Predicted : ไดโนเสาร์\n",
      "Input : ไอซ์_0 Predicted : โชจู\n",
      "Input : ไอซ์_1 Predicted : เซอร์เบีย\n",
      "Correct Predicted on Training set : 0 Corrct percentage : 0.0%\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "num_clip = 0\n",
    "for clip in labels_test:\n",
    "    # print(clip)\n",
    "    file_paths = [f\"Test/{clip}.mp4/{clip}.npy\"]\n",
    "    # print(file_paths)\n",
    "\n",
    "    sequences = load_keypoint_sequences(file_paths)\n",
    "    # Change list to numpy array \n",
    "    sequences = np.array(sequences)\n",
    "    # Change numpy array to tensor\n",
    "    sequences = torch.FloatTensor(sequences)\n",
    "    sequences = pad_sequence(sequences, batch_first=True)\n",
    "\n",
    "    outputs = model(sequences.to(device))\n",
    "\n",
    "    for idx, word in enumerate(outputs):\n",
    "        # max_value = torch.max(outputs)\n",
    "        list_outputs = max(outputs)\n",
    "\n",
    "    index_max = max(range(len(list_outputs)), key=list_outputs.__getitem__)\n",
    "\n",
    "    print(f\"Input : {clip} Predicted : {labels[index_max]}\")\n",
    "\n",
    "    if clip == labels[index_max]:\n",
    "        correct = correct+1\n",
    "    \n",
    "    num_clip = num_clip + 1 \n",
    "\n",
    "print(f\"Correct Predicted on Training set : {correct} Corrct percentage : {correct*100/num_clip}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), 'saved_data/attention_lstm.pt')\n",
    "# torch.save(optimizer.state_dict(), 'saved_data/optimizer.pt')\n",
    "# torch.save(scheduler.state_dict(), 'saved_data/scheduler.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real-Time Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holist = mp.solutions.holistic \n",
    "mp_draw = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(img, model):\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img.flags.writeable = False                 \n",
    "    result = model.process(img)                 # Make prediction\n",
    "    img.flags.writeable = True                   \n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR) \n",
    "    return img, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks(img, result):\n",
    "    mp_draw.draw_landmarks(img, result.face_landmarks, mp_holist.FACEMESH_CONTOURS) # Draw face connections\n",
    "    mp_draw.draw_landmarks(img, result.pose_landmarks, mp_holist.POSE_CONNECTIONS) # Draw pose connections\n",
    "    mp_draw.draw_landmarks(img, result.left_hand_landmarks, mp_holist.HAND_CONNECTIONS) # Draw left hand connections\n",
    "    mp_draw.draw_landmarks(img, result.right_hand_landmarks, mp_holist.HAND_CONNECTIONS) # Draw right hand connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_styled_landmarks(img, result):\n",
    "    mp_draw.draw_landmarks(img, result.face_landmarks, mp_holist.FACEMESH_CONTOURS, \n",
    "                             mp_draw.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1), # color the joint \n",
    "                             mp_draw.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1) #color the connection\n",
    "                             ) \n",
    "    # mp_draw.draw_landmarks(img, result.face_landmarks, mp_holist.FACEMESH_CONTOURS, \n",
    "    #                          mp_draw.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1), # color the joint \n",
    "    #                          mp_draw.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1) #color the connection\n",
    "    #                          ) \n",
    "    \n",
    "    mp_draw.draw_landmarks(img, result.pose_landmarks, mp_holist.POSE_CONNECTIONS,\n",
    "                             mp_draw.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4), \n",
    "                             mp_draw.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    mp_draw.draw_landmarks(img, result.left_hand_landmarks, mp_holist.HAND_CONNECTIONS, \n",
    "                             mp_draw.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4), \n",
    "                             mp_draw.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    mp_draw.draw_landmarks(img, result.right_hand_landmarks, mp_holist.HAND_CONNECTIONS, \n",
    "                             mp_draw.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4), \n",
    "                             mp_draw.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                             ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(results):\n",
    "    pose=np.array([[res.x,res.y,res.z,res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    left_hnd=np.array([[res.x,res.y,res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    right_hnd=np.array([[res.x,res.y,res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    face=np.array([[res.x,res.y,res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "    return np.concatenate([pose,left_hnd,right_hnd,face])\n",
    "# concatenating for the model to detect the sign language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize MediaPipe holistic model\n",
    "# mp_holistic = mp.solutions.holistic\n",
    "# holistic = mp_holistic.Holistic()\n",
    "\n",
    "# # Initialize OpenCV video capture\n",
    "# cap = cv2.VideoCapture(0)\n",
    "\n",
    "# sequence = []\n",
    "# sequence_length = 160\n",
    "\n",
    "# while cap.isOpened():\n",
    "#     ret, frame = cap.read()\n",
    "#     if not ret:\n",
    "#         break\n",
    "    \n",
    "#     # Convert the image to RGB (MediaPipe expects RGB images)\n",
    "#     image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "#     # Process the frame with MediaPipe\n",
    "#     results = holistic.process(image_rgb)\n",
    "    \n",
    "#     # Extract keypoints (face, hands, and pose landmarks)\n",
    "#     # Assume extract_keypoints function converts keypoints into a flattened vector of size 1662\n",
    "#     keypoints = extract_keypoints(results) \n",
    "    \n",
    "#     # Append keypoints to the sequence buffer\n",
    "#     sequence.append(keypoints)\n",
    "    \n",
    "#     # Ensure the sequence only keeps the last 160 frames\n",
    "#     if len(sequence) > sequence_length:\n",
    "#         sequence.pop(0)\n",
    "    \n",
    "#     if len(sequence) == sequence_length:\n",
    "#         # Convert sequence to tensor\n",
    "#         input_tensor = torch.tensor([sequence], dtype=torch.float32)\n",
    "        \n",
    "#         # Make predictions\n",
    "#         with torch.no_grad():\n",
    "#             prediction = model(input_tensor.to(device))\n",
    "        \n",
    "#         # Decode and display the prediction\n",
    "#         predicted_class = torch.argmax(prediction, dim=1).item()\n",
    "#         print(f\"Predicted Class: {predicted_class}\")\n",
    "    \n",
    "#     # Display the video frame\n",
    "#     cv2.imshow('Real-time Sign Language Recognition', frame)\n",
    "    \n",
    "#     if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "#         break\n",
    "\n",
    "# cap.release()\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: 33\n",
      "Class : แซมเบีย\n",
      "Predicted Class: 1\n",
      "Class : กฎหมายรัฐธรรมนูญ\n",
      "Predicted Class: 21\n",
      "Class : เช่าซื้อ\n",
      "Predicted Class: 8\n",
      "Class : กังวล\n",
      "Predicted Class: 20\n",
      "Class : เจ้าหนี้\n",
      "Predicted Class: 21\n",
      "Class : เช่าซื้อ\n",
      "Predicted Class: 20\n",
      "Class : เจ้าหนี้\n",
      "Predicted Class: 30\n",
      "Class : เม็กซิโก\n",
      "Predicted Class: 30\n",
      "Class : เม็กซิโก\n",
      "Predicted Class: 13\n",
      "Class : เขื่อนสิริกิติ์\n",
      "Predicted Class: 2\n",
      "Class : กรมอนามัย\n",
      "Predicted Class: 30\n",
      "Class : เม็กซิโก\n",
      "Predicted Class: 1\n",
      "Class : กฎหมายรัฐธรรมนูญ\n",
      "Predicted Class: 20\n",
      "Class : เจ้าหนี้\n",
      "Predicted Class: 20\n",
      "Class : เจ้าหนี้\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "sequence = []\n",
    "sequence_length = 160\n",
    "# Set mediapipe model \n",
    "with mp_holist.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        # print(results)\n",
    "\n",
    "        keypoints = extract_keypoints(results)\n",
    "        sequence.append(keypoints)\n",
    "\n",
    "        if len(sequence) > sequence_length:\n",
    "            sequence = []\n",
    "    \n",
    "        if len(sequence) == sequence_length:\n",
    "            # Convert sequence to tensor\n",
    "            input_tensor = torch.tensor([sequence], dtype=torch.float32)\n",
    "        \n",
    "            # Make predictions\n",
    "            with torch.no_grad():\n",
    "                prediction = model(input_tensor.to(device))\n",
    "        \n",
    "            # Decode and display the prediction\n",
    "            predicted_class = torch.argmax(prediction, dim=1).item()\n",
    "            print(f\"Predicted Class: {predicted_class}\")\n",
    "            print(f\"Class : {labels[predicted_class]}\")\n",
    "        \n",
    "        draw_styled_landmarks(image, results)\n",
    "\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save & Load Model Command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), 'saved_data/attention_lstm.pt')\n",
    "# torch.save(optimizer.state_dict(), 'saved_data/optimizer.pt')\n",
    "# # torch.save(scheduler.state_dict(), 'saved_data/scheduler.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Load model\n",
    "# model.load_state_dict(torch.load('saved_data/model_name.pt'))\n",
    "# optimizer.load_state_dict(torch.load('saved_data/optimizer_namae.pt'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
