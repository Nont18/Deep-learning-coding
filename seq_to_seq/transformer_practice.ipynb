{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch Transformers from Scratch (Attension is all you need)\n",
    "\n",
    "https://www.youtube.com/watch?v=U0s0f995w14 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torchvision\n",
    "from torchtext import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import spacy \n",
    "\n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchtext import data \n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234 \n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    dev = \"cuda:0\"\n",
    "    print(\"gpu up\")\n",
    "else:\n",
    "    dev = \"cpu\"\n",
    "device = torch.device(dev)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\araya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "# Libraries for processing text \n",
    "\n",
    "import nltk \n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "import re \n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English \n",
    "nlp = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\araya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stops = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:4: SyntaxWarning: invalid escape sequence '\\,'\n",
      "<>:4: SyntaxWarning: invalid escape sequence '\\,'\n",
      "C:\\Users\\araya\\AppData\\Local\\Temp\\ipykernel_8948\\1404369135.py:4: SyntaxWarning: invalid escape sequence '\\,'\n",
      "  punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n"
     ]
    }
   ],
   "source": [
    "# Function to remove punctuation\n",
    "\n",
    "def removepunc(my_str):\n",
    "    punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
    "    no_punct = \"\"\n",
    "    for char in my_str:\n",
    "        if char not in punctuations:\n",
    "            no_punct = no_punct + char \n",
    "    return no_punct\n",
    "\n",
    "def hasNumbers(inputString):\n",
    "    return bool(re.search(r'\\d', inputString))\n",
    "snowstem = SnowballStemmer(\"english\")\n",
    "portstem = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata = pd.read_csv(\"/kaggle/input/hate-speech-detection/toxic_train.csv\")\n",
    "test = pd.read_csv(\"/kaggle/input/hate-speech-detection/toxic_test.csv\")\n",
    "traindata.drop(\"Unnamed: 0\",axis=1,inplace=True)\n",
    "test.drop(\"Unnamed: 0\",axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "this function is the tokenizer we are using, it does basic processing also  like ,\n",
    "Lowercase the text\n",
    "removing punctuation, stop words and numbers,\n",
    "it also removes extra spaces and unwanted characters (I use regex for that)\n",
    "\n",
    "\n",
    "before using the tokenizer I was testing it on the train dataframe manually  \n",
    "\"\"\"\n",
    "\n",
    "def myTokenizer(x):\n",
    " return  [snowstem.stem(word.text)for word in \n",
    "          tokenizer(removepunc(re.sub(r\"\\s+\\s+\",\" \",re.sub(r\"[^A-Za-z0-9()!?\\'\\`\\\"\\r+\\n+]\",\" \",x.lower()))).strip()) \n",
    "          if (word.text not in stops and not hasNumbers(word.text)) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(tokenize=myTokenizer,batch_first=True,fix_length=140)\n",
    "LABEL = data.LabelField(dtype=torch.float ,batch_first=True)\n",
    "\n",
    "\n",
    "class DataFrameDataset(data.Dataset):\n",
    "\n",
    "    def __init__(self, df, text_field, label_field, is_test=False, **kwargs):\n",
    "        fields = [('comment_text', text_field), ('toxic', label_field)]\n",
    "        examples = []\n",
    "        for i, row in df.iterrows():\n",
    "            label = row.toxic \n",
    "            text = row.comment_text\n",
    "            examples.append(data.Example.fromlist([text, label], fields))\n",
    "\n",
    "        super().__init__(examples, fields, **kwargs)\n",
    "  \n",
    "\n",
    "torchdataset = DataFrameDataset(traindata, TEXT,LABEL)\n",
    "torchtest = DataFrameDataset(test, TEXT,LABEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data = torchdataset.split(split_ratio=0.8, random_state = random.seed(SEED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train_data,min_freq=3)  \n",
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#No. of unique tokens in text\n",
    "print(\"Size of TEXT vocabulary:\",len(TEXT.vocab))\n",
    "\n",
    "#No. of unique tokens in label\n",
    "print(\"Size of LABEL vocabulary:\",len(LABEL.vocab))\n",
    "\n",
    "#Commonly used words\n",
    "print(TEXT.vocab.freqs.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set batch size\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator,valid_iterator,test_iterator= data.BucketIterator.splits(\n",
    "    (train_data,valid_data,torchtest), \n",
    "    batch_size = BATCH_SIZE,\n",
    "    device = device,\n",
    "    sort =False,\n",
    "shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextTransformer(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(TextTransformer,self).__init__()\n",
    "    self.wordEmbeddings = nn.Embedding(len(TEXT.vocab),140)\n",
    "    self.positionEmbeddings = nn.Embedding(140,20)\n",
    "    self.transformerLayer = nn.TransformerEncoderLayer(160,8) \n",
    "    self.linear1 = nn.Linear(160,  64)\n",
    "    self.linear2 = nn.Linear(64,  1)\n",
    "    self.linear3 = nn.Linear(140,  16)\n",
    "    self.linear4 = nn.Linear(16,  1)\n",
    "  def forward(self,x):\n",
    "    positions = (torch.arange(0,140).reshape(1,140) + torch.zeros(x.shape[0],140)).to(device) \n",
    "    # broadcasting the tensor of positions \n",
    "    sentence = torch.cat((self.wordEmbeddings(x.long()),self.positionEmbeddings(positions.long())),axis=2)\n",
    "    attended = self.transformerLayer(sentence)\n",
    "    linear1 = F.relu(self.linear1(attended))\n",
    "    linear2 = F.relu(self.linear2(linear1))\n",
    "    linear2 = linear2.view(-1,140) # reshaping the layer as the transformer outputs a 2d tensor (or 3d considering the batch size)\n",
    "    linear3 = F.relu(self.linear3(linear2))\n",
    "    out = torch.sigmoid(self.linear4(linear3))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myTransformer = TextTransformer()\n",
    "myTransformer.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateMetrics(ypred,ytrue):\n",
    "  acc  = accuracy_score(ytrue,ypred)\n",
    "  f1  = f1_score(ytrue,ypred)\n",
    "  f1_average  = f1_score(ytrue,ypred,average=\"macro\")\n",
    "  return \" f1 score: \"+str(round(f1,3))+\" f1 average: \"+str(round(f1_average,3))+\" accuracy: \"+str(round(acc,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adagrad(myTransformer.parameters(),lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "  trainpreds = torch.tensor([])\n",
    "  traintrues = torch.tensor([])\n",
    "  for  batch in train_iterator:\n",
    "    X = batch.comment_text\n",
    "    y = batch.toxic\n",
    "    myTransformer.zero_grad()\n",
    "    pred = myTransformer(X).squeeze()\n",
    "    trainpreds = torch.cat((trainpreds,pred.cpu().detach()))\n",
    "    traintrues = torch.cat((traintrues,y.cpu().detach()))\n",
    "    err = F.binary_cross_entropy(pred,y)\n",
    "    err.backward()\n",
    "    optimizer.step()\n",
    "  err = F.binary_cross_entropy(trainpreds,traintrues)\n",
    "  print(\"train BCE loss: \",err.item(),calculateMetrics(torch.round(trainpreds).numpy(),traintrues.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valpreds = torch.tensor([])\n",
    "  valtrues = torch.tensor([])\n",
    "  for batch in valid_iterator:\n",
    "    X = batch.comment_text\n",
    "    y = batch.toxic\n",
    "    valtrues = torch.cat((valtrues,y.cpu().detach()))\n",
    "    pred = myTransformer(X).squeeze().cpu().detach()\n",
    "    # print(valtrues.shape)\n",
    "    valpreds = torch.cat((valpreds,pred))\n",
    "  err = F.binary_cross_entropy(valpreds,valtrues)\n",
    "  print(\"validation BCE loss: \",err.item(),calculateMetrics(torch.round(valpreds).numpy(),valtrues.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testpreds = torch.tensor([])\n",
    "testtrues = torch.tensor([])\n",
    "for batch in test_iterator:\n",
    "    X = batch.comment_text\n",
    "    y = batch.toxic\n",
    "    testtrues = torch.cat((testtrues,y.cpu().detach()))\n",
    "    pred = myTransformer(X).squeeze().cpu().detach()\n",
    "    # print(valtrues.shape)\n",
    "    testpreds = torch.cat((testpreds,pred))\n",
    "err = F.binary_cross_entropy(testpreds,testtrues)\n",
    "print(\"test BCE loss: \",err.item(),calculateMetrics(torch.round(testpreds).numpy(),testtrues.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"predicted\"] = torch.round(testpreds).numpy()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "this shows that the model understands the language well \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "test[test.predicted==1].iloc[32:37]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
